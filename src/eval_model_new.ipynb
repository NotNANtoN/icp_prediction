{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import optuna\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from utils.metrics import apply_all_metrics\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"../results_eval_tune\"\n",
    "study_names = [os.path.join(results_dir, study_name) for study_name in os.listdir(results_dir)] #and 'clinical' in study_name]\n",
    "study_names = [name for name in study_names if 'Jan' in name]\n",
    "study_names.sort(key=lambda x: x.split('_')[1:])\n",
    "\n",
    "for name in study_names:\n",
    "    print(name.split(\"/\")[-1])\n",
    "\n",
    "study_names_other = [ '../results_eval_tune/Dec-17-20:59:35_xgb_5k_freeze_10_5_clinical',\n",
    " '../results_eval_tune/Dec-11-13:58:40_xgb_5k_freeze_10_5_blood',\n",
    " '../results_eval_tune/Dec-11-13:59:07_xgb_5k_freeze_10_5_imaging_pca',\n",
    "  '../results_eval_tune/Dec-18-20:16:19_xgb_5k_freeze_10_5_clinical_blood',\n",
    " '../results_eval_tune/Dec-17-23:54:04_log_5k_freeze_10_5_clinical',\n",
    " '../results_eval_tune/Dec-12-14:32:32_log_0k_freeze_10_5_blood',\n",
    " '../results_eval_tune/Dec-12-14:34:11_log_0k_freeze_10_5_imaging_pca',\n",
    "  '../results_eval_tune/Dec-19-02:16:22_log_5k_freeze_10_5_clinical_blood',\n",
    "  '../results_eval_tune/Dec-19-05:13:04_log_5k_freeze_10_5_clinical_imaging_pca',\n",
    "  '../results_eval_tune/Dec-19-15:59:52_mlp_1k_freeze_10_5_clinical'\n",
    "]\n",
    "study_names_other = ['../results_eval_tune/Dec-22-14:21:47_xgb_5k_freeze_10_5_clinical_blood']\n",
    "\n",
    "study_names_other = [ '../results_eval_tune/Dec-17-20:59:35_xgb_5k_freeze_10_5_clinical',\n",
    " '../results_eval_tune/Dec-11-13:58:40_xgb_5k_freeze_10_5_blood',\n",
    " '../results_eval_tune/Dec-11-13:59:07_xgb_5k_freeze_10_5_imaging_pca',\n",
    " '../results_eval_tune/Dec-22-14:21:47_xgb_5k_freeze_10_5_clinical_blood',\n",
    "               \n",
    " '../results_eval_tune/Dec-17-23:54:04_log_5k_freeze_10_5_clinical',\n",
    " '../results_eval_tune/Dec-12-14:32:32_log_0k_freeze_10_5_blood',\n",
    " '../results_eval_tune/Dec-12-14:34:11_log_0k_freeze_10_5_imaging_pca',\n",
    " '../results_eval_tune/Dec-22-14:24:38_log_0k_freeze_10_5_clinical_blood'\n",
    " ]\n",
    "\n",
    "study_names_other = ['../results_eval_tune/Jan-05-16:04:13_log_5k_freeze_10_5_clinical', '../results_eval_tune/Jan-05-16:33:20_log_5k_freeze_10_5_clinical',\n",
    "               '../results_eval_tune/Jan-05-16:40:03_log_5k_freeze_10_5_clinical', '../results_eval_tune/Jan-05-16:50:59_log_5k_freeze_10_5_clinical',\n",
    "               '../results_eval_tune/Jan-05-20:55:28_xgb_5k_freeze_10_5_clinical',\n",
    "               '../results_eval_tune/Jan-06-01:11:45_svc_5k_freeze_10_5_clinical', '../results_eval_tune/Jan-06-15:40:15_log_5k_freeze_10_5_blood']\n",
    "study_names = ['Jan-07-22:47:10_xgb_5k_freeze_10_5_clinical',\n",
    "         'Jan-08-03:05:05_xgb_5k_freeze_10_5_blood',\n",
    "         'Jan-08-19:10:28_xgb_5k_freeze_10_5_sparse_img',\n",
    "         'Jan-08-22:57:00_xgb_5k_freeze_10_5_clinical_sparse_img',\n",
    "         'Jan-13-05:35:20_xgb_5k_freeze_10_5_clinical_sparse_img',\n",
    "         \n",
    "         \n",
    "         'Jan-08-13:02:26_log_5k_freeze_10_5_clinical',\n",
    "         'Jan-08-17:36:36_log_5k_freeze_10_5_blood',\n",
    "         'Jan-09-07:29:04_log_5k_freeze_10_5_sparse_img',\n",
    "         'Jan-09-11:16:24_log_5k_freeze_10_5_clinical_sparse_img',\n",
    "         'Jan-13-00:28:03_log_5k_freeze_10_5_clinical_sparse_img',\n",
    "         \n",
    "         #'Jan-09-13:32:30_mlp_1k_freeze_10_5_clinical',\n",
    "        ]\n",
    "\n",
    "study_names = ['../results_eval_tune/' + name for name in study_names]",
    "\n",
    "\n",
    "for name in study_names:\n",
    "    print(name.split(\"/\")[-1])\n",
    "\n",
    "study_names = [name for name in study_names if \"Apr\" in name]\n",
    "\n",
    "precips = [\"Apr-05-08:35:47_log_5k_freeze_10_5_blood\",\n",
    "            \"Apr-05-13:00:20_log_5k_freeze_10_5_blood_clinical\",\n",
    "            \"Apr-05-17:47:56_log_5k_freeze_10_5_blood_sparse_img_clinical\",\n",
    "            \"Apr-05-23:06:44_xgb_5k_freeze_10_5_blood\",\n",
    "            \"Apr-06-08:32:37_xgb_5k_freeze_10_5_blood_clinical\",\n",
    "            \"Apr-06-18:11:44_xgb_5k_freeze_10_5_blood_sparse_img_clinical\",\n",
    "            \"Apr-07-04:43:12_mlp_1k_freeze_10_5_clinical_blood_sparse_img\",\n",
    "          \n",
    "            'Apr-12-02:01:17_log_5k_freeze_10_5_clinical',]\n",
    "\n",
    "no_precips =  [name for name in study_names[3:] if not any(precip_name in name for precip_name in precips)]\n",
    "\n",
    "study_names = no_precips[:-2]\n",
    "study_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_of_interest = [\"accuracy\", \"roc_auc\", \"pr_auc\", \"1.0_precision\", \"sensitivity\", \"specificity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_studies(study_list):\n",
    "    all_y_pred_logits = []\n",
    "    all_y_pred_binary = []\n",
    "    all_y_pred_binary_roc = []\n",
    "    all_y_true = []\n",
    "    study_names = []\n",
    "    model_names = []\n",
    "    for study_name in study_list:\n",
    "        names = [\"hyperparams\", \"eval_score\", \"score_dict\", \"y_pred_logits\", \"y_pred_binary\", \"y_true\"]\n",
    "        y_pred_logits = []\n",
    "        y_pred_binary = []\n",
    "        y_pred_binary_roc = []\n",
    "        y_true = []\n",
    "\n",
    "        for file in os.listdir(study_name):\n",
    "            if file.endswith(\".pkl\") and file != \"cfg.pkl\":\n",
    "                study = joblib.load(os.path.join(study_name, file))\n",
    "                y_pred_logits_, y_true_ = study[3], study[5]\n",
    "                y_pred_logits += [y_pred_logits_]\n",
    "                y_true += [y_true_]\n",
    "                y_pred_binary += [binarize_predictions(y_pred_logits_, y_true_, curve_type=\"recall\")]\n",
    "                y_pred_binary_roc += [binarize_predictions(y_pred_logits_, y_true_, curve_type=\"roc\")]\n",
    "\n",
    "        print(f\"Study {study_name} has {len(y_pred_logits)} studies.\")\n",
    "        if len(y_pred_logits) > 0:\n",
    "            all_y_pred_logits += [y_pred_logits]\n",
    "            all_y_pred_binary += [y_pred_binary]\n",
    "            all_y_pred_binary_roc += [y_pred_binary_roc]\n",
    "            all_y_true += [y_true]\n",
    "            study_names += [study_name]\n",
    "            model_names += [\"_\".join(os.path.basename(study_name).split(\"_\")[0:])]# + \"_\" + '_'.join(os.path.basename(study_name).split(\"_\")[6:])]\n",
    "\n",
    "    return all_y_pred_logits, all_y_pred_binary, all_y_pred_binary_roc, all_y_true, study_names, model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def _maximize_curve(y_pred_logits, y_true, curve_type):\n",
    "    if curve_type == \"pr\":\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_pred_logits)\n",
    "        sums = np.sum([precision, recall], axis=0)\n",
    "    elif curve_type == \"roc\":\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_logits)\n",
    "        sums = np.sum([fpr, tpr], axis=0)\n",
    "    elif curve_type == \"recall\":\n",
    "        thresholds = list(np.linspace(0, 0.2, 500)) + list(np.linspace(0.1, 1.0, 100))\n",
    "        goal = 0.95\n",
    "        sums = [1 - abs(goal - recall_score(y_true, y_pred_logits > t)) for t in thresholds]\n",
    "        #sums = []\n",
    "        #for thresh in thresholds:\n",
    "        #    binarized = y_pred_logits > thresh\n",
    "        #    recall = sklearn.metrics.recall_score(y_true, binarized)\n",
    "        #    score = 1 - abs(goal - recall)\n",
    "        #    sums.append(score)\n",
    "            \n",
    "    max_thresh = thresholds[np.argmax(sums)]\n",
    "    return max_thresh\n",
    "\n",
    "def binarize_predictions(y_pred_logits, y_true, curve_type=\"recall\"):\n",
    "    thresh = _maximize_curve(y_pred_logits, y_true, curve_type)\n",
    "    y_pred_binary = y_pred_logits > thresh\n",
    "    return y_pred_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y_pred_logits, all_y_pred_binary, all_y_pred_binary_roc, all_y_true, study_names, model_names = load_studies(study_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_dict(y_true, y_pred_binary, y_pred_logits):\n",
    "    # report_dict = _flatten_dict(classification_report(y_true, y_pred, output_dict=True))\n",
    "    # report_dict['roc_auc'] = roc_auc_score(y_true, y_pred_logits)\n",
    "    # report_dict['pr_auc'] = average_precision_score(y_true, y_pred_logits)\n",
    "    # report_dict['sensitivity'], report_dict['specificity'] = _sensitivity_specificity(y_true, y_pred)\n",
    "    metric_dict = apply_all_metrics(y_true, y_pred_binary, y_pred_logits, shape_is_correct=True)\n",
    "    #metric_dict[\"recall\"] = recall_score(y_true, y_pred_binary)\n",
    "    old_prec = metric_dict[\"1.0_precision\"]\n",
    "    if len(np.unique(y_pred_binary)) == 1:\n",
    "        if int(np.unique(y_pred_binary)[0]) == 0:\n",
    "            metric_dict[\"1.0_precision\"] = 1\n",
    "            print(\"Only one prediction type!\")\n",
    "            print(np.unique(y_pred_binary))\n",
    "            print(\"Overwrite!\")\n",
    "            \n",
    "    if len(np.unique(y_true)) == 1:\n",
    "        print(\"Only one label type!\")\n",
    "        print(np.unique(y_true))\n",
    "    #else:\n",
    "    #    prec = precision_score(y_true, y_pred_binary)\n",
    "    #metric_dict[\"1.0_precision\"] = precision_score(y_true, y_pred_binary) if not (len(np.unique(y_pred_binary)) == 1 and np.unique(y_pred_binary)[0] == 0) else 1\n",
    "    \n",
    "    if old_prec != metric_dict[\"1.0_precision\"]:\n",
    "        print(old_prec, metric_dict[\"1.0_precision\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "    prec = average_precision_score(y_true, y_pred_binary)\n",
    "    #print(prec)\n",
    "    #print(metric_dict[\"1.0_precision\"])\n",
    "    #print()\n",
    "    return metric_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(all_y_pred_logits, all_y_pred_binary, all_y_true, study_names, model_names):\n",
    "    metric_names = [\"0.0_precision\", \"0.0_recall\", \"0.0_f1-score\", \"0.0_support\", \"1.0_precision\", \"1.0_recall\", \"1.0_f1-score\", \"1.0_support\", \"accuracy\", \"macro  avg_precision\", \"macro avg_recall\", \"macro avg_f1-score\", \"macro avg_support\", \"weighted avg_precision\", \"weighted avg_recall\", \"weighted avg_f1-score\", \"weighted avg_support\", \"roc_auc\", \"pr_auc\", \"sensitivity\", \"specificity\"]\n",
    "    report_agg = pd.DataFrame(columns=metric_names+['study_name', 'model_name'])\n",
    "    r = 0\n",
    "    for i in range(len(all_y_pred_logits)):\n",
    "        for j in range(len(all_y_pred_logits[i])):\n",
    "            results_dict = get_metric_dict(all_y_true[i][j], all_y_pred_binary[i][j], all_y_pred_logits[i][j])\n",
    "            report_agg.loc[r, :] = list(results_dict.values()) + [study_names[i]] + [model_names[i]]\n",
    "            r += 1\n",
    "    return report_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(\"binarized with pr:\", all_y_pred_binary, \"\\nbinarized with roc:\", all_y_pred_binary_roc)\n",
    "metric_dfs = compute_metrics(all_y_pred_logits, all_y_pred_binary, all_y_true, study_names, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_dfs_roc = compute_metrics(all_y_pred_logits, all_y_pred_binary_roc, all_y_true, study_names, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_confidence_interval(metrics_df, n, model_name, sign_level='95'):\n",
    "    z_values = {'90': 1.64, '95': 1.96, '98': 2.33, '99': 2.58}\n",
    "    interval = lambda metric, sd :  z_values[sign_level] * (sd / sqrt(n))\n",
    "    intervals = [interval(metrics_df.loc[f\"Mean_{model_name}\", col], metrics_df.loc[f\"SD_{model_name}\", col]) for col in metrics_df.columns]\n",
    "    #print(\"\\nConfidence offsets\", intervals)\n",
    "    lower = metrics_df.loc[f\"Mean_{model_name}\", :] - intervals\n",
    "    upper = metrics_df.loc[f\"Mean_{model_name}\", :] + intervals\n",
    "    metrics_df.loc[f\"CI_{model_name}\", :] = [f\"[{l:.1f}, {u:.1f}]\" for l, u in zip(lower, upper)]\n",
    "    #metrics_df.loc[f\"CI_upper_{model_name}\", :] = metrics_df.loc[f\"Mean_{model_name}\", :] + intervals\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_sd_ci(df):\n",
    "    mean_sd = pd.DataFrame(columns=metrics_of_interest)\n",
    "    for model_name in df.model_name.unique():\n",
    "        mean_sd.loc[f\"Mean_{model_name}\", :] = df.loc[df[\"model_name\"] == model_name, metrics_of_interest].mean().round(3) * 100\n",
    "        mean_sd.loc[f\"SD_{model_name}\", :] = df.loc[df[\"model_name\"] == model_name, metrics_of_interest].std().round(3) * 100\n",
    "        mean_sd = _get_confidence_interval(mean_sd, len(df[df[\"model_name\"] == model_name]), model_name)\n",
    "        mean_sd = mean_sd.drop(f\"SD_{model_name}\")\n",
    "        #print(len(df[df[\"model_name\"] == model_name]))\n",
    "    \n",
    "    dest_path = \"../nested_cv_eval_results\"\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "    mean_sd.to_csv(os.path.join(dest_path, f\"all_studies_cum_metrics.csv\"))\n",
    "    return mean_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs_dfs = compute_mean_sd_ci(metric_dfs)\n",
    "avgs_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = avgs_dfs.copy()\n",
    "paper_df.columns = [\"Accuracy\", \"ROC AUC\", \"AP\", \"Precision\", \"Recall/Sensitivity\", \"Specificity\"]\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "for i in range(0, len(paper_df), 2):\n",
    "    merged_row = paper_df.iloc[i].astype(str).str[0:4] + \" \" + paper_df.iloc[i + 1].astype(str)\n",
    "    index = paper_df.index[i].split(\"_\")\n",
    "    first = index[1]\n",
    "    if first == \"xgb\":\n",
    "        first = \"GBT\"\n",
    "    elif first == \"log\":\n",
    "        first = \"LogReg\"\n",
    "    elif first == \"mlp\":\n",
    "        first = \"MLP\"\n",
    "    end = index[6:]\n",
    "    end = [dt[0].upper() + dt[1:] for dt in end]\n",
    "    end = \" + \".join(end)\n",
    "    end = end.replace(\"Sparse + Img\", \"Sparse Imaging\")\n",
    "    end = end.replace(\" Imaging + Pca\", \"Imaging\")\n",
    "    #end = end[0].upper() + end[1:]\n",
    "    index = first + \" \" + end\n",
    "    \n",
    "    merged_row[\"Model & Data\"] = index\n",
    "    new_df = new_df.append(merged_row, ignore_index=True)\n",
    "\n",
    "new_df = new_df[[\"Model & Data\", \"ROC AUC\", \"AP\", \"Precision\", \"Recall/Sensitivity\", \"Specificity\", \"Accuracy\"]]\n",
    "new_df = new_df.set_index(\"Model & Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_order(index):\n",
    "    if \"LogReg\" in index:\n",
    "        val = 30\n",
    "    elif \"GBT\" in index:\n",
    "        val = 20\n",
    "    else:\n",
    "        val = 10\n",
    "    \n",
    "    if \"Clinical + Blood + Sparse Imaging\" in index:\n",
    "        val += 1\n",
    "    elif \"Clinical + Sparse Imaging\" in index:\n",
    "        val += 2\n",
    "    elif \"Clinical + Blood\" in index:\n",
    "        val += 3\n",
    "    elif \"Sparse Imaging\" in index:\n",
    "        val += 5\n",
    "    elif \"Imaging\" in index:\n",
    "        val += 4\n",
    "    elif \"Blood\" in index:\n",
    "        val += 6\n",
    "    elif \"Clinical\" in index:\n",
    "        val += 7\n",
    "    return val\n",
    "\n",
    "def apply_index_order(index_list):\n",
    "    index_list = [index_order(index_list[i]) for i in range(len(index_list))]\n",
    "    return index_list\n",
    "\n",
    "new_df.sort_index(key=apply_index_order, ascending=False)",
    "no_ci = avgs_dfs.copy()\n",
    "no_ci = no_ci.iloc[[i for i, name in enumerate(no_ci.index) if not name.startswith(\"CI_\")]]\n",
    "no_ci.index = [name.replace(\"Mean_\", \"\").replace(\"freeze_\", \"\").replace(\"10_5_\", \"\").replace(\"5k_\", \"\"). replace(\"1k_\", \"\") for name in no_ci.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table to doc\n",
    "\n",
    "import docx\n",
    "import pandas as pd\n",
    "\n",
    "# open an existing document\n",
    "name = '../docs/report.docx'\n",
    "doc = docx.Document(name)\n",
    "df = new_df\n",
    "cols = list(df.columns)\n",
    "df[\"Model & Data\"] = list(df.index)\n",
    "df = df[[\"Model & Data\"] + cols]\n",
    "# add a table to the end and create a reference variable\n",
    "# extra row is so we can add the header row\n",
    "t = doc.add_table(df.shape[0]+1, df.shape[1])\n",
    "\n",
    "# add the header rows.\n",
    "for j in range(df.shape[-1]):\n",
    "    t.cell(0,j).text = df.columns[j]\n",
    "\n",
    "# add the rest of the data frame\n",
    "for i in range(df.shape[0]):\n",
    "    for j in range(df.shape[-1]):\n",
    "        t.cell(i+1,j).text = str(df.values[i,j])\n",
    "\n",
    "# save the doc\n",
    "doc.save('../docs/test.docx')",
    "no_ci.sort_values(\"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = avgs_dfs.index#\n",
    "#avgs_dfs[~avgs_dfs.index.str.contains('opt|svc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_mean_sd_ci(metric_dfs_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dfs[\"model_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dfs.loc[metric_dfs[\"model_name\"] == \"mlp_clinical\", \"pr_auc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_name = \"xgb_5k_freeze_10_5_clinical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = \"log_5k_freeze_10_5_clinical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dfs.loc[metric_dfs[\"model_name\"] == xgb_name, \"pr_auc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dfs.loc[metric_dfs[\"model_name\"] == log_name, \"pr_auc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_rel(metric_dfs.loc[metric_dfs[\"model_name\"] == xgb_name, \"pr_auc\"], metric_dfs.loc[metric_dfs[\"model_name\"] == log_name, \"pr_auc\"], axis=0, nan_policy='propagate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dfs.loc[metric_dfs[\"model_name\"] == \"xgb_clinical\", \"roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dfs.loc[metric_dfs[\"model_name\"] == \"log_clinical\", \"roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.087/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_rel(metric_dfs.loc[metric_dfs[\"model_name\"] == \"mlp_clinical\", \"pr_auc\"], metric_dfs.loc[metric_dfs[\"model_name\"] == \"log_clinical\", \"pr_auc\"], axis=0, nan_policy='propagate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_names(model_name):\n",
    "    linestyle = '-' if 'xgb' in model_name else '--'\n",
    "    \n",
    "    stylized_name = \"\"\n",
    "    if 'xgb' in model_name:\n",
    "        stylized_name += 'GBT'\n",
    "    if 'svc' in model_name:\n",
    "        stylized_name += 'SVM'\n",
    "    if 'log' in model_name:\n",
    "        stylized_name += 'LogReg'\n",
    "    if 'mlp' in model_name.lower():\n",
    "        stylized_name += 'MLP'\n",
    "            \n",
    "    #if 'clinical' in model_name and 'blood' in model_name:\n",
    "    #    stylized_name += \"Clinical+Blood\"\n",
    "    if 'clinical' in model_name and 'blood' in model_name and 'sparse_img' in model_name:\n",
    "        stylized_name += ' All'\n",
    "    else:\n",
    "        count = 0\n",
    "        if 'clinical' in model_name:\n",
    "            stylized_name += ' Clinical'\n",
    "            count += 1\n",
    "        if 'blood' in model_name:\n",
    "            if count:\n",
    "                stylized_name += \" +\"\n",
    "            stylized_name += ' Blood'\n",
    "            count += 1\n",
    "        if 'sparse_img' in model_name:\n",
    "            if count:\n",
    "                stylized_name += \" +\"\n",
    "            stylized_name += ' Sparse Imaging'\n",
    "            count += 1\n",
    "        if 'imaging' in model_name:\n",
    "            if count:\n",
    "                stylized_name += \" +\"\n",
    "            stylized_name += ' Imaging'\n",
    "    \n",
    "    if 'Jan-13' in model_name:\n",
    "        if \"xgb\" in model_name:\n",
    "            stylized_name = \"GBT Clinical + Sparse Imaging + Precipitals\"\n",
    "        else:\n",
    "            stylized_name = \"LogReg Clinical + Sparse Imaging + Precipitals\"\n",
    "    return linestyle, stylized_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_kfold(y_pred, y, color, model_name, ax):\n",
    "    \"\"\"Create a receiver operating characteristic plot for every k-fold split set on the provided ax.\"\"\"\n",
    "    linestyle, stylized_name = get_plot_names(model_name)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # Individual curves\n",
    "    for idx in range(len(y_pred)):\n",
    "        fpr, tpr, thresholds = roc_curve(y[idx], y_pred[idx])\n",
    "        \n",
    "        df = pd.DataFrame({'x': fpr, 'y': tpr}).sort_values(by='x')\n",
    "        interp_tpr = np.interp(mean_fpr, df['x'], df['y'])\n",
    "        #interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "        \n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(auc(fpr, tpr))\n",
    "    # Mean curve\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr) * 100\n",
    "    std_auc = np.std(aucs) * 100\n",
    "    ax.plot(mean_fpr, mean_tpr, color=color, linestyle=linestyle, lw=2, alpha=1, label=f'{stylized_name} AUC=%0.1f\\u00B1%0.1f' %\n",
    "                                                                 (mean_auc, std_auc))\n",
    "    # Grey confidence intervals\n",
    "    std_tpr = np.std(tprs, axis=0) / sqrt(len(y_pred))\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color=color, alpha=.2)  # label=r'$\\pmodel_type$ 1 std. dev.')\n",
    "\n",
    "    # Title and legend\n",
    "    ax.set(xlim=[0.0, 1.0], ylim=[0.0, 1.05], title=\"Receiver operating characteristic (ROC)\")\n",
    "    ax.set_xlabel('1 - Specificity')\n",
    "    ax.set_ylabel('Sensitivity')\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors = [\"#C52230\", \"#0073EE\", \"#A222C7\", '#178A07'] * 2#, '#FFA500'] * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_outside = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib._color_data as mcd\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "# The diagonal line\n",
    "ax.plot([0, 1], [0, 1], linestyle=':', lw=2, color='black', label='Chance', alpha=.8)\n",
    "\n",
    "\n",
    "for i in range(len(study_names)):\n",
    "    plot_roc_kfold(all_y_pred_logits[i], all_y_true[i], plot_colors[i], f\"{model_names[i]}\", ax)\n",
    "if legend_outside:\n",
    "    lgd = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "else:\n",
    "    lgd = plt.legend()\n",
    "dest_path = os.path.join(\"../nested_cv_eval_results\", f\"ROC_plot_{'legend_out' if legend_outside else 'legend_in'}.jpg\")\n",
    "fig.savefig(dest_path, dpi=300, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def get_prec_rec_thresh(y_true, y_score):\n",
    "    threshs = np.linspace(0, 1, 1000)\n",
    "    recs = [recall_score(y_true, y_score > t) for t in threshs]\n",
    "    precs = []\n",
    "    for t in threshs:\n",
    "        if (len(np.unique(y_score > t)) == 1) and np.unique(y_score > t)[0] == 0:\n",
    "            prec = 1\n",
    "        else:\n",
    "            prec = precision_score(y_true, y_score > t)\n",
    "        precs.append(prec)\n",
    "    #precs = [precision_score(y_true, y_score > t) if not (len(np.unique(y_score > t)) == 1 and np.unique(y_score > t)[0] == 0) else 1 for t in threshs]\n",
    "    #print(\"R\", recs)\n",
    "    #print(\"P\", precs)\n",
    "    return np.array(precs), np.array(recs), threshs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pc_kfold(y_pred, y, color, model_name, ax):\n",
    "    \"\"\"Create a receiver operating characteristic plot for every k-fold split set on the provided ax.\"\"\"\n",
    "    linestyle, stylized_name = get_plot_names(model_name)\n",
    "    print(stylized_name)\n",
    "    \n",
    "    aucs = []\n",
    "    interps = []\n",
    "    interp_points = np.linspace(0, 1, 100)\n",
    "    \n",
    "    use_manual_thresholds = False\n",
    "    precs_at = []\n",
    "    # Individual curves\n",
    "    for idx in range(len(y_pred)):\n",
    "        if use_manual_thresholds:\n",
    "            precision, recall, thresholds = get_prec_rec_thresh(y[idx], y_pred[idx])\n",
    "        else:\n",
    "            precision, recall, thresholds = precision_recall_curve(y[idx], y_pred[idx])\n",
    "            \n",
    "        close_to_95 = np.argmax(1 - abs(0.95 - recall))\n",
    "        prec_at = precision[close_to_95]\n",
    "        precs_at.append(prec_at)\n",
    "        \n",
    "        df = pd.DataFrame({'x': recall, 'y': precision}).sort_values(by='x')\n",
    "        interp_prec = np.interp(interp_points, df['x'], df['y'])\n",
    "        interps.append(interp_prec)\n",
    "        \n",
    "\n",
    "        ap_skl = average_precision_score(y[idx], y_pred[idx])\n",
    "        ap_real = np.mean(interp_prec)\n",
    "        #print(ap_skl, ap_real, np.mean(recall))\n",
    "        aucs.append(ap_skl)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Mean curve    \n",
    "    mean_prec = np.mean(interps, axis=0)\n",
    "    std_prec = np.std(interps, axis=0) / sqrt(len(y_pred)) # standard error\n",
    "    mean_rec = interp_points\n",
    "    \n",
    "    \n",
    "    close_to_95 = np.argmax(1 - abs(0.95 - mean_rec))\n",
    "    print(f\"Precision at {np.round(mean_rec[close_to_95] * 100, 1)}% Recall: \", mean_prec[close_to_95])\n",
    "    print(\"Mean prec at : \", np.mean(precs_at))\n",
    "    \n",
    "    mean_auc = np.mean(aucs) * 100\n",
    "    #print(\"Mean skl auc: \", mean_auc)\n",
    "    #print(\"Mean our auc: \", np.mean(mean_prec))\n",
    "    std_auc = np.std(aucs) * 100\n",
    "    ax.plot(mean_rec, mean_prec, color=color, linestyle=linestyle, lw=2, alpha=1, label=f'{stylized_name} AP=%0.1f\\u00B1%0.1f' %\n",
    "                                                                 (mean_auc, std_auc))\n",
    "    # Grey confidence intervals\n",
    "    tprs_upper = np.minimum(mean_prec + std_prec, 1)\n",
    "    tprs_lower = np.maximum(mean_prec - std_prec, 0)\n",
    "    ax.fill_between(mean_rec, tprs_lower, tprs_upper, color=color, alpha=.2)  # label=r'$\\pmodel_type$ 1 std. dev.')\n",
    "\n",
    "    # Title and legend\n",
    "    ax.set(xlim=[0.0, 1.0], ylim=[0.0, 1.05], title=\"Precision-Recall Curve\")\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "# The diagonal line\n",
    "\n",
    "for i in range(len(study_names)):\n",
    "    plot_pc_kfold(all_y_pred_logits[i], all_y_true[i], plot_colors[i], f\"{model_names[i]}\", ax)\n",
    "\n",
    "if legend_outside:\n",
    "    lgd = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "else:\n",
    "    lgd = plt.legend()\n",
    "dest_path = os.path.join(\"../nested_cv_eval_results\", f\"PR_plot_{'legend_out' if legend_outside else 'legend_in'}.jpg\")\n",
    "fig.savefig(dest_path, dpi=300, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "# The diagonal line\n",
    "\n",
    "for i in range(len(study_names)):\n",
    "    plot_pc_kfold(all_y_pred_logits[i], all_y_true[i], plot_colors[i], f\"{model_names[i]}\", ax)\n",
    "\n",
    "dest_path = os.path.join(\"../nested_cv_eval_results\", f\"PR_plot.jpg\")\n",
    "fig.savefig(dest_path, dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
