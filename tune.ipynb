{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8249040-0da5-4a4c-9120-eb8fa64cc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arg_utils import is_notebook, get_cfg\n",
    "cfg = get_cfg()\n",
    "# choices\n",
    "classical_models = [\"linear\", \"xgb\", \"rf\"]\n",
    "nn_models = [\"mlp\", \"rnn\", \"transformer\", \"clip\", \"gpt\"]\n",
    "# override variables to experiment in notebook\n",
    "if is_notebook():    \n",
    "    cfg[\"target_name\"] = \"ICP_Vital\"   # ICP_Vital\" , long_icp_hypertension_2\n",
    "    cfg[\"db_name\"] = \"UKE\"  # \"UKE\", \"MIMIC\"\n",
    "    cfg[\"minutes\"] = 60\n",
    "    cfg[\"model_type\"] = \"gpt\"\n",
    "    \n",
    "    # do experiments on:fill_type, target_nan_quantile, train_noise_std, \n",
    "    #  min_len(increase from 20 to higher), grad_clip_val (at 1 so far), weight_decay (at 0.2 so far)\n",
    "    \n",
    "    cfg[\"fill_type\"] = \"median\" # \"pat_mean\", \"median\", \"pat_ema\" \"pat_ema_mask\"\n",
    "    cfg[\"norm_method\"] = None # z, or none\n",
    "\n",
    "    \n",
    "    cfg[\"bs\"] = 32 # 8 best for rnn, 32 for GPT\n",
    "    cfg[\"max_len\"] = 128\n",
    "    cfg[\"min_len\"] = 128\n",
    "    cfg[\"target_nan_quantile\"] = 0.9999\n",
    "    cfg[\"block_size\"] = 0\n",
    "\n",
    "    # classical model args\n",
    "    cfg[\"flat_block_size\"] = 8\n",
    "    # general args\n",
    "    cfg[\"max_epochs\"] = 20\n",
    "    cfg[\"use_nan_embed\"] = False\n",
    "    cfg[\"weight_decay\"] = 0.2\n",
    "    \n",
    "    \n",
    "    # rnn params\n",
    "    cfg[\"hidden_size\"] = 2048\n",
    "    cfg[\"rnn_type\"] = \"gru\"\n",
    "    \n",
    "    # transformer params\n",
    "    cfg[\"mode\"] = \"train_mlp_norm\"  # \"adapters\", \"train_mlp_norm\",  \"train_norm\", \"freeze\" (does not train)\n",
    "    \n",
    "    cfg[\"gpu\"] = 1\n",
    "\n",
    "    cfg[\"tune_hebo\"] = False\n",
    "    cfg[\"opt_steps\"] = 5\n",
    "    \n",
    "    \n",
    "# overrides and calculated default vals\n",
    "if cfg[\"lr\"] is None:\n",
    "    model_type = cfg[\"model_type\"]\n",
    "    if model_type == \"clip\":\n",
    "        cfg[\"lr\"] = 0.001\n",
    "    elif model_type == \"gpt\":\n",
    "        # bs 8 and gpt2 take 9.8GB with max seq len of 512\n",
    "        # bs 16 with max seq len of 256\n",
    "        # bs 32 with max seq len 128 only 7.4GB, good performance and fast - 6.9 if mlp_norm\n",
    "        # bs 64 with len 128 and mlp_norm = 10.9GB. 9.4GB for freeze\n",
    "        cfg[\"lr\"] = 0.00005\n",
    "    else:\n",
    "        cfg[\"lr\"] = 0.0001  # 0.01 works kind of for nan_embed\n",
    "        \n",
    "#cfg[\"val_check_interval\"] = int(cfg[\"val_check_interval\"] * (32 / cfg[\"batch_size\"]))\n",
    "    \n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(cfg[\"gpu\"])\n",
    "# disable wandb printing\n",
    "os.environ['WANDB_SILENT'] = \"true\"\n",
    "\n",
    "import torch\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "locals().update(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e611fa-7e8b-4b19-a8ad-e5de8b0d03e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989845a-f790-4c8a-ad2a-a8dcfc39edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import logging\n",
    "import pytorch_lightning\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "pytorch_lightning.utilities.distributed.log.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c73b5-b346-4caa-85ac-ae7d70d940fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "path = f\"data/DB_{cfg['db_name']}_{cfg['minutes']}_final_df.pkl\"\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8067d-fc71-4766-80ed-a6f4d510b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune_hebo import get_hebo_space, tune_hebo\n",
    "\n",
    "# run hebo tuning if wanted\n",
    "if cfg[\"tune_hebo\"]:\n",
    "    space = get_hebo_space(cfg[\"model_type\"])\n",
    "    tune_hebo(space, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c264f0-e741-4004-9f41-5282bdabbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTUNA\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from tune_utils import objective_optuna\n",
    "\n",
    "\n",
    "study = optuna.create_study()  # Create a new study.\n",
    "study.optimize(lambda study: objective_optuna(study, df, cfg), \n",
    "               n_trials=cfg[\"opt_steps\"])  # Invoke optimization of the objective_optuna function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune_utils import make_optuna_foldername, save_study_results\n",
    "\n",
    "# create a folder to save the results\n",
    "folder_name = make_optuna_foldername(cfg)\n",
    "save_study_results(study, folder_name, cfg[\"model_type\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262606b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune_utils import retrain_best_trial\n",
    "\n",
    "val_scores, test_scores, weights = retrain_best_trial(study, df, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da832b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soup_utils import create_and_eval_soup\n",
    "from tune_utils import merge_params_in_cfg, setup_dm\n",
    "\n",
    "soup_cfg = merge_params_in_cfg(cfg, study.best_params)\n",
    "soup_dm = setup_dm(df, soup_cfg)\n",
    "\n",
    "# make soup of different seeds of the best hyperparam set\n",
    "soup_val_score, soup_test_score = create_and_eval_soup(soup_dm, soup_cfg, weights)\n",
    "# save scores of soup model\n",
    "soup_scores_df = pd.DataFrame({\"val_score\": [soup_val_score], \"test_score\": [soup_test_score]})\n",
    "soup_scores_df.to_csv(f\"{folder_name}/best_param_seed_soup_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7cfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get params of some of the best optuna trials to create a soup from\n",
    "from tune_utils import get_best_params, train_multiple\n",
    "\n",
    "top_params, top_vals = get_best_params(study, num_trials=5)\n",
    "top_param_val_scores, top_param_test_scores, top_param_weights = train_multiple(top_params, df, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup of some of the best hyperparameter sets\n",
    "soup_val_score, soup_test_score = create_and_eval_soup(soup_dm, soup_cfg, top_param_weights[:5])\n",
    "# save scores of soup model\n",
    "soup_scores_df = pd.DataFrame({\"val_score\": [soup_val_score], \"test_score\": [soup_test_score]})\n",
    "soup_scores_df.to_csv(f\"{folder_name}/top_param_soup_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f8bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff0243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raw",
   "language": "python",
   "name": "raw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
