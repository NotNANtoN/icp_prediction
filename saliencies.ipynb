{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "import captum\n",
    "import torch\n",
    "import pytorch_lightning\n",
    "from captum.attr import NoiseTunnel, Saliency\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rich.progress import track\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "importlib.reload(sys.modules['utils'])\n",
    "import saliency\n",
    "importlib.reload(saliency)\n",
    "importlib.reload(sys.modules['saliency'])\n",
    "from saliency import get_all_attrs, calc_median\n",
    "\n",
    "\n",
    "sys.path.append(\"../artemis\")\n",
    "from src.models import LitBlockLSTM, LitLSTM\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attrs(model, absolute=False, block_size=24, target_idx=None, desired_target_val=None):\n",
    "    # init\n",
    "    if target_idx is None:\n",
    "        target_idx = 0\n",
    "    model.batch_size = 1\n",
    "    dl = model.val_dataloader()\n",
    "    epochs = 1\n",
    "    model.cuda()\n",
    "    sal_model = saliency.get_sal_model(model, idx=target_idx)\n",
    "    # collect\n",
    "    attrs = []\n",
    "    for epoch in tqdm(range(epochs), disable=epochs == 1):\n",
    "        for data, target, idcs, lens in tqdm(dl, disable=epochs != 1): \n",
    "            # Cut padded values out:\n",
    "            data = data[0, :lens[0]].unsqueeze(0)\n",
    "            # To GPU:\n",
    "            data = data.cuda()\n",
    "            # Add requires grad for saliency:\n",
    "            data.requires_grad = True\n",
    "            if len(data[0]) < block_size:\n",
    "                continue\n",
    "            split_target = torch.split(target, block_size, dim=1)\n",
    "            for split_idx, _pat_split in enumerate(torch.split(data, block_size, dim=1)):\n",
    "                if split_idx == (len(data[0]) // block_size) - 1 and len(data[0]) % block_size != 0:\n",
    "                    break\n",
    "                #print(split_target[split_idx][0, -1, target_idx])\n",
    "                if desired_target_val is not None and split_target[split_idx][0, -1, target_idx] != desired_target_val:\n",
    "                    #print(\"SKIP\")\n",
    "                    continue\n",
    "                # Calculate Atrribution:\n",
    "                out = model(_pat_split)\n",
    "                attribution = sal_model.attribute(_pat_split, nt_type=\"smoothgrad\", n_samples=50,\n",
    "                                                  stdevs=0.01, abs=absolute, target=0).cpu()\n",
    "                attrs.append(attribution.squeeze(0))\n",
    "    #attrs = pad_and_mask(attrs)\n",
    "    attrs = torch.stack(attrs).numpy()\n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_mean_feature_attribution(mean_feature_attr, feature_names, title, folder=None):\n",
    "    # Plot:\n",
    "    plt.figure(figsize=(20, 30))\n",
    "    plt.barh(range(len(mean_feature_attr)), mean_feature_attr, tick_label=feature_names)\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.title(title)\n",
    "    if folder is None:\n",
    "        folder = \"\"\n",
    "    else:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    plt.savefig(folder + title + \".pdf\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_sort_mean_attribution(attrs, feature_names, feature_weights=None):\n",
    "    if torch.is_tensor(feature_names):\n",
    "        feature_names = feature_names.tolist()\n",
    "    attr = attrs.sum(0).sum(0)\n",
    "    print(\"Normed attr shape: \", attr.shape)\n",
    "    # Apply weighting:\n",
    "    if feature_weights is not None:\n",
    "        mean_feature_attr *= feature_weights\n",
    "    # Normalize it:\n",
    "    attr /= np.abs(attr).max()\n",
    "    attr = attr.squeeze()\n",
    "    # Sort them:\n",
    "    zipped_sort = sorted(zip(attr, feature_names), key=lambda x: x[0], reverse=True)\n",
    "    sorted_attr, sorted_names = zip(*zipped_sort)\n",
    "    sorted_attr = np.stack(sorted_attr)\n",
    "    # Sort them also for abs value for saliency evaluation:\n",
    "    zipped_sort = sorted(zip(np.abs(attr), feature_names), key=lambda x: x[0], reverse=True)\n",
    "    _, sorted_names_abs_vals = zip(*zipped_sort)\n",
    "    return sorted_attr, sorted_names, sorted_names_abs_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_saliency_map(attribution, feature_names, path_title, all_positive=False, folder=None, outcome=None, pred_outcome=None):\n",
    "    if all_positive:\n",
    "        cmap = None\n",
    "        vmin = 0\n",
    "        vmax = 1\n",
    "    else:\n",
    "        # Palette from orange to blue with maximum contrast\n",
    "        cmap = sns.diverging_palette(15, 240, s=99, l=50, as_cmap=True)\n",
    "        #cmap = \"coolwarm\"\n",
    "        vmin = -1\n",
    "        vmax = 1\n",
    "    # Normalize:\n",
    "    attribution = attribution / np.max(np.abs(attribution))\n",
    "    # Plot:\n",
    "    plt.figure(figsize=(75, 25), dpi=300)\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    ax = sns.heatmap(attribution, yticklabels=feature_names, cbar=True, cmap=cmap,\n",
    "                    vmin=vmin, vmax=vmax)\n",
    "    ax.set(xlabel='Time step', ylabel='Feature')\n",
    "    plot_title = \"Feature Attribution Heatmap for Outcome Prediction\"\n",
    "    if outcome is not None:\n",
    "        plot_title += \" - Outcome: \" + str(int(outcome)) + \" - Pred Outcome: \" + str(round(pred_outcome, 3))\n",
    "    plt.title(plot_title)\n",
    "    if folder is None:\n",
    "        folder = \"\"\n",
    "    else:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    plt.savefig(folder + path_title + \".jpg\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_plots(attrs, name, feature_names, absolute=False):\n",
    "    plot_folder = name + '/'\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "    # store attrs:\n",
    "    print(\"attr shape: \", attrs.shape)\n",
    "    attrs.dump(f'{plot_folder}attrs.npy')\n",
    "    # feature attribution\n",
    "    sorted_attr, sorted_names, sorted_names_abs_vals = normalize_and_sort_mean_attribution(attrs, feature_names)\n",
    "    plot_mean_feature_attribution(sorted_attr, sorted_names, f'Saliency', folder=plot_folder)\n",
    "    # heatmap\n",
    "    mean_map = np.moveaxis(np.sum(attrs, axis=0), 1, 0)\n",
    "    title = f'saliency_heatmap'\n",
    "    plot_saliency_map(mean_map, feature_names, title, all_positive=absolute, folder=plot_folder)\n",
    "    # Importance over time\n",
    "    over_time = np.abs(attrs).sum(0).mean(-1)\n",
    "    over_time /= over_time.sum()\n",
    "    title = f'importance_over_time'\n",
    "    plt.figure(figsize=(7, 10))\n",
    "    plt.plot(over_time)\n",
    "    plt.title('Importance over time')\n",
    "    plt.savefig(plot_folder + title + '.jpg')\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(model_id, name=\"\", subdir=\"1\", perc=None, ig=False):\n",
    "    # read model\n",
    "    exp_dict = utils.load_experiment_data(mlrun_id=model_id, default_id=subdir)\n",
    "    feature_names = exp_dict[\"feature_names\"]\n",
    "    target_names = exp_dict[\"target_names\"]\n",
    "    data = exp_dict[\"data\"]\n",
    "    target = exp_dict[\"target\"]\n",
    "    pred = exp_dict[\"pred\"]\n",
    "    args = exp_dict[\"args\"]\n",
    "    model = exp_dict[\"model\"]\n",
    "    # create plots\n",
    "    if len(target_names) == 1:\n",
    "        # icp preds\n",
    "        # get attrs\n",
    "        attrs = get_all_attrs(model, absolute=False, perc=perc)\n",
    "        create_plots(attrs, f'Saliencies/ICP_raw_{name}', feature_names, absolute=False)\n",
    "        \n",
    "        #attrs_block = get_attrs(model, absolute=False, block_size=12)\n",
    "        #create_plots(attrs_block, f'Saliencies/Block_ICP_raw_{name}', feature_names, absolute=False)\n",
    "        \n",
    "        attrs_abs = get_all_attrs(model, absolute=True, perc=perc)\n",
    "        create_plots(attrs_abs, f'Saliencies/ICP_abs_{name}', feature_names, absolute=True)\n",
    "    else:\n",
    "        # phase preds\n",
    "        for idx, target_name in enumerate(target_names):\n",
    "            print(\"Saliencies for\", target_name, \"with idx\", idx)\n",
    "            # get attrs\n",
    "            #target_val = 1\n",
    "            #save_name = f'Saliencies/{target_name}_target_{target_val}_raw_{name}'\n",
    "            #print(save_name)\n",
    "            #attrs_int = get_attrs(model, block_size=bs, target_idx=idx, desired_target_val=target_val)\n",
    "            #attrs_int = get_all_attrs(model, target_idx=idx, desired_target_val=target_val, perc=perc)\n",
    "            #create_plots(attrs_int, save_name, feature_names)\n",
    "            \n",
    "            #target_val = 0\n",
    "            #save_name = f'Saliencies/{target_name}_target_{target_val}_raw_{name}'\n",
    "            #print(save_name)\n",
    "            #attrs_int = get_all_attrs(model, target_idx=idx, desired_target_val=target_val, perc=perc)\n",
    "            #create_plots(attrs_int, save_name, feature_names)\n",
    "            \n",
    "            save_name = f'Saliencies/{target_name}_target_all_raw_{name}'\n",
    "            print(save_name)\n",
    "            # get attrs\n",
    "            attrs = get_all_attrs(model, target_idx=idx, perc=perc, agg=False, ig=ig)\n",
    "            # save attrs\n",
    "            #print(len(attrs), attrs[0].shape, attrs[1].shape)\n",
    "            #summed_per_pat = [attr.sum(axis=0) for attr in attrs]\n",
    "            #print(\"summed: len: \", len(summed_per_pat), summed_per_pat[0].shape, summed_per_pat[0][0])\n",
    "            #print(\"type: \", type(summed_per_pat))\n",
    "            \n",
    "            # plot attrs\n",
    "            attrs = utils.pad_and_mask(attrs, attr=True)\n",
    "            print(\"Final attrs shape: \", attrs.shape)\n",
    "            create_plots(attrs, save_name, feature_names)\n",
    "            #attrs_abs = get_attrs(model, absolute=True, block_size=bs, idx=idx)\n",
    "            #create_plots(attrs_abs, f'Saliencies/{target_name}_abs_{name}_{bs}', feature_names, absolute=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sal_list():\n",
    "    attrs = get_all_attrs(model, target_idx=idx, perc=perc, agg=False)\n",
    "    summed_per_pat = [attr.sum(axis=0) for attr in attrs]\n",
    "    return summed_per_pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_id = 'e075eaaf10fe48f2aa3d738cc4495fd7'\n",
    "icp_id = 'f0d90ecf970f467fa23204e6f4490bd3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#analyze_model(icp_id, \"ICP\", perc=0.1)\n",
    "#analyze_model(phase_id, \"phase\", perc=1.0, ig=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sal_list(model, idx, perc, agg, ds=None, ig=False, **sal_kwargs):\n",
    "    model.train()\n",
    "    attrs = get_all_attrs(model, target_idx=idx, perc=perc, agg=False, ds=ds, ig=ig, **sal_kwargs)\n",
    "    summed_per_pat = [attr for attr in attrs]\n",
    "    return summed_per_pat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_rank(sal_list, feature_names):\n",
    "    normed_sal_list = [pat / np.abs(pat).sum() for pat in sal_list]\n",
    "    feat_sal_list = [pat.sum(0) for pat in normed_sal_list]\n",
    "    feat_sal = np.sum(feat_sal_list, axis=0)\n",
    "    feat_sal = feat_sal / np.abs(feat_sal).max()\n",
    "\n",
    "    rank_df = pd.DataFrame({\"val\": feat_sal, \"name\": feature_names, \"val_abs\": np.abs(feat_sal)})\n",
    "    rank_df = rank_df.sort_values(by=\"val_abs\", ascending=False)\n",
    "\n",
    "    rank_names = rank_df[\"name\"].tolist()\n",
    "    rank_idcs = [list(feature_names).index(name) for name in rank_names]\n",
    "    rank_scores = rank_df[\"val\"]\n",
    "    return rank_names, rank_idcs, rank_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def eval_model(model, noise_std=1.0, noise_idcs=None, ds=None, dl=None, batch_size=512, median=None):\n",
    "    if ds is None:\n",
    "        ds = model.val_dataloader().dataset\n",
    "        \n",
    "    if dl is None:\n",
    "        dl = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n",
    "    \n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    for pat_data, pat_target, idx, lens in dl:\n",
    "        bs = pat_data.shape[0]\n",
    "        pat_data = pat_data.cuda(non_blocking=True)\n",
    "        # potentially set some feature to median\n",
    "        if median is not None:\n",
    "            pat_data[:, :, noise_idcs] = median[0, noise_idcs].to(\"cuda\")\n",
    "        # potentially add noise\n",
    "        if noise_idcs is not None and noise_std != 0:\n",
    "            noise_tensor = torch.zeros(bs, pat_data.shape[1], len(noise_idcs), device=\"cuda\")\n",
    "            noise_tensor = noise_tensor.normal_(std=noise_std)\n",
    "            pat_data[:, :, noise_idcs] += noise_tensor\n",
    "        # pred\n",
    "        with torch.no_grad():\n",
    "            pat_pred = model(pat_data)\n",
    "        # select phase\n",
    "        pat_target = pat_target[:, :, phase_idx].cpu()\n",
    "        pat_pred = pat_pred[:, :, phase_idx].cpu()\n",
    "        # cut lens\n",
    "        for idx, len_ in enumerate(lens):\n",
    "            cut_pred = pat_pred[idx, :len_].flatten()\n",
    "            cut_target = pat_target[idx, :len_].flatten()\n",
    "            val_preds.append(cut_pred)\n",
    "            val_targets.append(cut_target)\n",
    "    # flatten list to array\n",
    "    val_preds = np.array([pred for pat in val_preds for pred in pat])\n",
    "    val_targets = np.array([target for pat in val_targets for target in pat])\n",
    "    # remove nans\n",
    "    nan_mask = np.isnan(val_targets)\n",
    "    val_targets = val_targets[~nan_mask]\n",
    "    val_preds = val_preds[~nan_mask]\n",
    "    # calc score\n",
    "    score = average_precision_score(val_targets, val_preds)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calc_remove_feat_score(model, used_idcs, noise_lvls=None, median=None, plot=True, ds=None, dl=None):\n",
    "    # calc scores\n",
    "    if median is None:\n",
    "        scores = []\n",
    "        for std in noise_lvls:\n",
    "            score = eval_model(model, noise_idcs=used_idcs, noise_std=std, ds=ds, dl=dl)\n",
    "            scores.append(score)\n",
    "    else:\n",
    "        score = eval_model(model, noise_idcs=used_idcs, median=median, ds=ds, dl=dl, noise_std=0.0)\n",
    "        scores = [score]\n",
    "    # plot\n",
    "    if plot:\n",
    "        plt.plot(noise_lvls, scores)\n",
    "        plt.xlabel(\"Noise std\")\n",
    "        plt.ylabel(\"AP\")\n",
    "        plt.ylim(0.3, 0.7)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def eval_heatmap(model, rank_idcs, remove_top_k, noise_lvls=None, median=None, ds=None, top=True, dl=None):\n",
    "    all_scores = []\n",
    "    for k in track(range(remove_top_k)):\n",
    "        if k == 0:\n",
    "            used_idcs = None\n",
    "        elif top:\n",
    "            used_idcs = rank_idcs[:k]\n",
    "        else:\n",
    "            used_idcs = rank_idcs[-k:]\n",
    "        \n",
    "        scores = calc_remove_feat_score(model, used_idcs, noise_lvls=noise_lvls, median=median, ds=ds, plot=False, dl=dl)\n",
    "        all_scores.append(scores)\n",
    "    all_scores = np.array(all_scores)\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "def plot_heatmap(noise_lvls, all_scores, name, root_folder):\n",
    "    plt.figure()\n",
    "    sns.heatmap(all_scores, vmin=0.4, vmax=0.65)\n",
    "    plt.ylabel(\"Feats removed\")\n",
    "    plt.xlabel(\"Std\")\n",
    "    plt.xticks(ticks=np.arange(len(noise_lvls)) + 0.5, labels=np.round(noise_lvls, 1))\n",
    "    os.makedirs(root_folder, exist_ok=True)\n",
    "    plt.savefig(os.path.join(root_folder, name + \".pdf\"))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dmg_distr(model, num_feats, noise_lvls=None, median=None, ds=None, dl=None):\n",
    "    max_score = eval_model(model, noise_idcs=None, ds=ds, dl=dl)\n",
    "    dmg_scores = []\n",
    "    for feat_idx in tqdm(range(num_feats)):\n",
    "        used_idcs = [feat_idx]\n",
    "        feat_scores = calc_remove_feat_score(model, used_idcs, noise_lvls=noise_lvls, median=median, ds=ds, plot=False, dl=dl)\n",
    "        feat_dmg_scores = np.clip(feat_scores / max_score, 0, 1)\n",
    "        feat_dmg_score = np.mean(feat_dmg_scores)\n",
    "        dmg_scores.append(feat_dmg_score)\n",
    "    dmg_scores = np.array(dmg_scores)\n",
    "    \n",
    "    dmg_scores_inv = 1 / dmg_scores\n",
    "    dmg_scores_norm = (dmg_scores_inv - dmg_scores_inv.min()) / (dmg_scores_inv.max() - dmg_scores_inv.min())\n",
    "    dmg_distr = dmg_scores_norm / dmg_scores_norm.sum()\n",
    "    \n",
    "    return dmg_distr\n",
    "\n",
    "\n",
    "def eval_dmg_distr(dmg_distr, rank_distr, v=0):\n",
    "    assert np.isclose(dmg_distr.sum(), 1), f'{dmg_distr.sum()}'\n",
    "    assert np.isclose(rank_distr.sum(), 1), f'{rank_distr.sum()}'\n",
    "    # Calculate divergence between both distributions\n",
    "    import scipy\n",
    "    w_d = scipy.stats.wasserstein_distance(rank_distr, dmg_distr)\n",
    "    kl_d = np.mean(scipy.special.kl_div(dmg_distr, rank_distr))\n",
    "    mean_abs_d = np.mean(np.abs(rank_distr - dmg_distr))\n",
    "    mean_sq_d = np.mean((rank_distr - dmg_distr) ** 2)\n",
    "    if v:\n",
    "        print(\"Mean sq diff: \", mean_sq_d)\n",
    "        print(\"Wasserstein distance: \", w_d)\n",
    "        print(\"KL divergence: \", kl_d)\n",
    "        print(\"Mean abs diff: \", mean_abs_d)\n",
    "    return w_d, kl_d, mean_abs_d, mean_sq_d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sal_name(ig, rand, noise_tunnel, sg_std, zero_baseline, median_baseline, median_step_baseline, num_baselines):\n",
    "    name = f'{\"ig\" if ig else (\"sg\" if not rand else \"rand\")}'\n",
    "    if noise_tunnel and not rand:\n",
    "        name += \"_nt\"\n",
    "        name += f'_{sg_std}'\n",
    "    if ig:\n",
    "        if zero_baseline:\n",
    "            name += \"_zero\"\n",
    "        elif median_baseline:\n",
    "            name += \"_med\"\n",
    "        elif median_step_baseline:\n",
    "            name += \"_medStep\"\n",
    "        elif num_baselines > 1:\n",
    "            name += \"_\" + str(num_baselines)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sal_all(model, feature_names, ig=False, rand=False, median_baseline=False, \n",
    "                median_step_baseline=False, zero_baseline=False, num_baselines=1, sg_std=0.01, noise_tunnel=True,\n",
    "                ):\n",
    "    name = get_sal_name(ig, rand, noise_tunnel, sg_std, zero_baseline, median_baseline, median_step_baseline, num_baselines)\n",
    "    \n",
    "    if rand:\n",
    "        rank_idcs = np.arange(len(feature_names))\n",
    "        np.random.shuffle(rank_idcs)\n",
    "        rank_names = feature_names\n",
    "        \n",
    "        rank_scores = np.ones(len(feature_names))\n",
    "        #rank_scores = np.linspace(1, 0.001, len(feature_names))\n",
    "        rank_scores /= rank_scores.sum()\n",
    "        sal_list = None\n",
    "    else:\n",
    "        sal_list = get_sal_list(model, 0, phase_idx, False, ig=ig, median_baseline=median_baseline,\n",
    "                                median_step_baseline=median_step_baseline, num_baselines=num_baselines,\n",
    "                                sg_std=sg_std, zero_baseline=zero_baseline, noise_tunnel=noise_tunnel)\n",
    "        rank_names, rank_idcs, rank_scores = get_rank(sal_list, feature_names)\n",
    "    return name, rank_idcs, rank_names, rank_scores, sal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sal_distr(dmg_distr, rank_idcs, rank_scores, name, root_folder):\n",
    "    # calc dmg distribution and calculate divergence to rank_scores\n",
    "    # dmg_distr = calc_dmg_distr(model, rank_idcs, rank_scores, noise_lvls, ds=ds, dl=dl)\n",
    "    model_dmg_distr = dmg_distr[rank_idcs] # Rearrange order according to ranking\n",
    "    rank_scores = rank_scores / np.abs(rank_scores).sum()\n",
    "    # save sal distr\n",
    "    distr_folder = os.path.join(root_folder, \"distrs\")\n",
    "    os.makedirs(distr_folder, exist_ok=True)\n",
    "    np.save(os.path.join(distr_folder, name + \"_imp.npy\"), rank_scores)\n",
    "    # eval distr\n",
    "    rank_distr = np.abs(rank_scores)\n",
    "\n",
    "    eval_dmg_distr(model_dmg_distr, rank_distr)\n",
    "\n",
    "    return rank_distr, model_dmg_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sal_method(model, feature_names, dmg_distr=None, noise_lvls=None, median=None, plot=False, ds=None, dl=None, **sal_kwargs):\n",
    "    name, rank_idcs, rank_names, rank_scores, sal_list = get_sal_all(model, feature_names, **sal_kwargs)\n",
    "    if ds is None:\n",
    "        ds = model.val_dataloader().dataset\n",
    "    if dl is None:\n",
    "        dl = torch.utils.data.DataLoader(ds, batch_size=256, pin_memory=True)\n",
    "    root_folder = \"sal_eval\"\n",
    "    \n",
    "    if dmg_distr is not None:\n",
    "        dmg_distr = np.copy(dmg_distr)\n",
    "        rank_distr, dmg_distr = eval_sal_distr(dmg_distr, rank_idcs, rank_scores, name, root_folder)\n",
    "        return rank_distr, dmg_distr, name\n",
    "    else:\n",
    "        remove_top_k = 20\n",
    "        # remove top and bottom k features\n",
    "        all_scores_top = eval_heatmap(model, rank_idcs, remove_top_k, noise_lvls=noise_lvls, median=median, ds=ds, top=True, dl=dl)\n",
    "        all_scores_bot = eval_heatmap(model, rank_idcs, remove_top_k, noise_lvls=noise_lvls, median=median, ds=ds, top=False, dl=dl)\n",
    "        # save arrays:\n",
    "        eval_array_folder = os.path.join(root_folder, \"eval_arrays\")\n",
    "        os.makedirs(eval_array_folder, exist_ok=True)\n",
    "        np.save(os.path.join(eval_array_folder, name + \"_top.npy\"), all_scores_top)\n",
    "        np.save(os.path.join(eval_array_folder, name + \"_bot.npy\"), all_scores_bot)\n",
    "        # save saliency list:\n",
    "        if sal_list is not None:\n",
    "            sal_folder = os.path.join(root_folder, \"saliencies\")\n",
    "            os.makedirs(sal_folder, exist_ok=True)\n",
    "            np.save(os.path.join(sal_folder, name + \".npy\"), sal_list)\n",
    "        # save plots\n",
    "        if plot:\n",
    "            heatmap_folder = os.path.join(root_folder, \"heatmaps\")\n",
    "            os.makedirs(heatmap_folder, exist_ok=True)\n",
    "            plot_heatmap(noise_lvls, all_scores_top, name + \"_top\", heatmap_folder)\n",
    "            plot_heatmap(noise_lvls, all_scores_bot, name + \"_bot\", heatmap_folder)\n",
    "    \n",
    "        return all_scores_top, all_scores_bot, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "exp_dict = utils.load_experiment_data(mlrun_id=phase_id, default_id=\"1\")\n",
    "feature_names = exp_dict[\"feature_names\"]\n",
    "model = exp_dict[\"model\"]\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "phase = \"long\"\n",
    "phase_idx = 1 if phase == \"long\" else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dmg_distr(model, noise_lvls=None, median=None):\n",
    "    ds = model.val_dataloader().dataset\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=256, pin_memory=True)\n",
    "\n",
    "    num_features = len(feature_names)\n",
    "    dmg_distr = calc_dmg_distr(model, num_features, noise_lvls=noise_lvls, median=median, ds=ds, dl=dl)\n",
    "    return dmg_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distrs(rank_distr, dmg_distr):\n",
    "    x_axis = range(len(rank_distr))\n",
    "    plt.bar(x_axis, rank_distr, label=\"Importance\")\n",
    "    plt.bar(x_axis, dmg_distr, label=\"Damage\", alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, max(dmg_distr + 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('sal_eval', \"feat_names.npy\"), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = (model, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwarg_list=[{'rand': True},\n",
    "\n",
    "                 {'sg_std': 0.01},\n",
    "                 {'sg_std': 0.02},\n",
    "                 {'sg_std': 0.005},\n",
    "                 {'sg_std': 0.1},\n",
    "                 {'sg_std': 1.0},\n",
    "\n",
    "\n",
    "                 {'ig': True, 'noise_tunnel': False, 'median_baseline': True},\n",
    "                 {'ig': True, 'noise_tunnel': True, 'median_baseline': True},\n",
    "                 {'ig': True, 'noise_tunnel': False, 'median_step_baseline': True},\n",
    "                 #{'ig': True, 'noise_tunnel': True, 'median_step_baseline': True},       \n",
    "                 #{'ig': True, 'noise_tunnel': True, 'num_baselines': 5},     \n",
    "                 #{'ig': True, 'noise_tunnel': False, 'num_baselines': 5},        \n",
    "                 #{'ig': True, 'noise_tunnel': True, 'zero_baseline': True},     \n",
    "                 {'ig': True, 'noise_tunnel': False, 'zero_baseline': True},   \n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwarg_list_debug =[{'rand': True},\n",
    "\n",
    "                 {'sg_std': 0.01},\n",
    "                \n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_sort_df(df):\n",
    "    df.index = df[\"name\"]\n",
    "    df = df.drop(columns=[\"name\"])\n",
    "    \n",
    "    minmax_names = []\n",
    "    for col in df.columns:\n",
    "        minmax_name = col + \"_minmax\"\n",
    "        df[minmax_name] = np.round(minmax(df[col]), 3)\n",
    "        minmax_names.append(minmax_name)\n",
    "    df[\"Agg\"] = df[minmax_names].mean(axis=1)\n",
    "    df = df.sort_values(by=\"Agg\", ascending=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_dmg_df(model, dmg_distr, apply_noise, eval_args, eval_kwarg_list):\n",
    "    df = pd.DataFrame()\n",
    "    for eval_kwargs in eval_kwarg_list:\n",
    "        eval_kwargs['dmg_distr'] = dmg_distr\n",
    "        rank_distr, model_dmg_distr, name = eval_sal_method(*eval_args, **eval_kwargs)\n",
    "        del eval_kwargs['dmg_distr']\n",
    "        w_d, kl_d, abs_d, sq_d = eval_dmg_distr(model_dmg_distr, rank_distr, v=0)\n",
    "        df_dict_row = {\"name\": [name], 'Wasserstein': [w_d], 'KL': [kl_d], 'Mean Squared': [sq_d]}\n",
    "        df = df.append(pd.DataFrame(df_dict_row))\n",
    "    df = minmax_sort_df(df)\n",
    "    df.to_csv(f\"sal_eval/dmg_eval_{'noise' if apply_noise else 'med'}.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_rank_df(model, noise_lvls, median_train, apply_noise, eval_args, eval_kwarg_list):\n",
    "    df = pd.DataFrame()\n",
    "    for eval_kwargs in eval_kwarg_list:\n",
    "        eval_kwargs['noise_lvls'] = noise_lvls\n",
    "        eval_kwargs['median'] = median_train\n",
    "        scores_top, scores_bot, name = eval_sal_method(*eval_args, **eval_kwargs)\n",
    "        del eval_kwargs['noise_lvls']\n",
    "        del eval_kwargs['median']\n",
    "\n",
    "        mean_top, mean_bot = np.mean(scores_top), np.mean(scores_bot)\n",
    "        df_dict_row = {\"name\": [name], 'Top': [mean_top], 'Bot': [mean_bot]}\n",
    "        df = df.append(pd.DataFrame(df_dict_row))\n",
    "    df[\"Bot\"] = 1 - df[\"Bot\"]  \n",
    "    df = minmax_sort_df(df)\n",
    "    \n",
    "    df.to_csv(f\"sal_eval/rank_eval_{'noise' if apply_noise else 'med'}.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_settings(model, apply_noise):\n",
    "    noise_lvls = None\n",
    "    median_train = None\n",
    "    \n",
    "    distr_folder = os.path.join(\"sal_eval\", \"distrs\")\n",
    "    os.makedirs(distr_folder, exist_ok=True)\n",
    "    \n",
    "    if apply_noise:\n",
    "        noise_lvls = np.arange(0.05, 2, 0.05)\n",
    "        dmg_distr = get_dmg_distr(model, noise_lvls)\n",
    "        np.save(os.path.join(distr_folder, \"dmg_distr_std.npy\"), dmg_distr)\n",
    "    else:\n",
    "        median_train = calc_median(model.train_dataloader().dataset)\n",
    "        dmg_distr = get_dmg_distr(model, median=median_train)\n",
    "        np.save(os.path.join(distr_folder, \"dmg_distr_med.npy\"), dmg_distr)\n",
    "    plt.bar(range(len(dmg_distr_med)), sorted(dmg_distr, reverse=True))\n",
    "    return noise_lvls, median_train, dmg_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sals(model, eval_args, eval_kwarg_list, apply_noise=False, settings=None):\n",
    "    if settings is None:\n",
    "        settings = get_settings(model, apply_noise)\n",
    "    noise_lvls, median_train, dmg_distr = settings\n",
    "    \n",
    "    # start first evaluation based on single feats\n",
    "    df_dmg = store_dmg_df(model, dmg_distr, apply_noise, eval_args, eval_kwarg_list)\n",
    "    print(df_dmg)\n",
    "    # start second rank-based evaluation:\n",
    "    df_rank = store_rank_df(model, noise_lvls, median_train, apply_noise, eval_args, eval_kwarg_list)\n",
    "    print(df_rank)\n",
    "    \n",
    "    return df_dmg, df_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = get_settings(model, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sals(model, eval_args, eval_kwarg_list, apply_noise=False, settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = get_settings(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sals(model, eval_args, eval_kwarg_list, apply_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_lvls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwargs = {'dmg_distr': dmg_distr,\n",
    "               'rand':True}\n",
    "rank_distr, model_dmg_distr = eval_sal_method(*eval_args, **eval_kwargs)\n",
    "plot_distrs(rank_distr, model_dmg_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwargs = {'dmg_distr': dmg_distr}\n",
    "rank_distr, model_dmg_distr = eval_sal_method(*eval_args, **eval_kwargs)\n",
    "plot_distrs(rank_distr, model_dmg_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwargs = {'dmg_distr': dmg_distr,\n",
    "               'ig': True}\n",
    "rank_distr, model_dmg_distr = eval_sal_method(*eval_args, **eval_kwargs)\n",
    "plot_distrs(rank_distr, model_dmg_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwargs = {'dmg_distr': dmg_distr,\n",
    "               'ig': True,\n",
    "               'zero_baseline': True}\n",
    "rank_distr, model_dmg_distr = eval_sal_method(*eval_args, **eval_kwargs)\n",
    "plot_distrs(rank_distr, model_dmg_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwargs = {'dmg_distr': dmg_distr,\n",
    "               'ig': True,\n",
    "               'median_baseline': True}\n",
    "rank_distr, model_dmg_distr = eval_sal_method(*eval_args, **eval_kwargs)\n",
    "plot_distrs(rank_distr, model_dmg_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_lvls = [0.2, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top_rand, scores_bot_rand, name = eval_sal_method(model, feature_names, rand=True, noise_lvls=noise_lvls)\n",
    "print(scores_top_rand.mean(), scores_bot_rand.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothgrad used so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top_smooth, scores_bot_smooth = eval_sal_method(model, feature_names, rand=False, ig=False)\n",
    "print(scores_top_smooth.mean(), scores_bot_smooth.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated Gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top_ig, scores_bot_ig = eval_sal_method(model, feature_names, rand=False, ig=True)\n",
    "print(scores_top_ig.mean(), scores_bot_ig.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top_ig_med, scores_bot_ig_med = eval_sal_method(model, feature_names, rand=False, ig=True, median_baseline=True)\n",
    "print(scores_top_ig_med.mean(), scores_bot_ig_med.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top_ig_medstep, scores_bot_ig_medstep = eval_sal_method(model, feature_names, rand=False, ig=True, median_step_baseline=True)\n",
    "print(scores_top_ig_medstep.mean(), scores_bot_ig_medstep.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_zero_bl_top, scores_zero_bl_bot = eval_sal_method(model, feature_names, rand=False, ig=True, zero_baseline=True)\n",
    "print(scores_zero_bl_top.mean(), scores_zero_bl_bot.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top_3, scores_bot_3 = eval_sal_method(model, feature_names, rand=False, ig=True, num_baselines=3)\n",
    "print(scores_top_3.mean(), scores_bot_3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top_5, scores_bot_5 = eval_sal_method(model, feature_names, rand=False, ig=True, num_baselines=5)\n",
    "print(scores_top_5.mean(), scores_bot_5.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IG no smoothgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top_ig_med, scores_bot_ig_med = eval_sal_method(model, feature_names, rand=False, ig=True, median_baseline=True, noise_tunnel=False)\n",
    "print(scores_top_ig_med.mean(), scores_bot_ig_med.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top_ig_med, scores_bot_ig_med = eval_sal_method(model, feature_names, rand=False, ig=True, zero_baseline=True, noise_tunnel=False)\n",
    "print(scores_top_ig_med.mean(), scores_bot_ig_med.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Smoothgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_smooth_top, scores_smooth_bot = eval_sal_method(model, feature_names, sg_std=0.1)\n",
    "print(scores_smooth_top.mean(), scores_smooth_bot.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_smooth_top, scores_smooth_bot = eval_sal_method(model, feature_names, sg_std=0.05)\n",
    "print(scores_smooth_top.mean(), scores_smooth_bot.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_smooth_top, scores_smooth_bot = eval_sal_method(model, feature_names, sg_std=0.002)\n",
    "print(scores_smooth_top.mean(), scores_smooth_bot.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_smooth_top, scores_smooth_bot = eval_sal_method(model, feature_names, sg_std=0.0001)\n",
    "print(scores_smooth_top.mean(), scores_smooth_bot.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
