{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e8e28-a868-4406-981b-c89975e08bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm  # for notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe7e2a-69ac-4b5a-8b0e-60e15f02762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_target_period(pat, mins_before_first_icp=60 * 24, target_name=\"ICP_Vital\",\n",
    "                         min_val=-10, max_val=105):\n",
    "    target_steps = pat[pat[\"Maßnahme\"] == target_name]\n",
    "    no_outlier_mask = (target_steps[\"Wert\"] >= min_val) & (target_steps[\"Wert\"] <= max_val)\n",
    "    target_steps = target_steps[no_outlier_mask]\n",
    "    if len(target_steps) == 0:\n",
    "        return pat.iloc[0:0]\n",
    "    min_target_time = min(target_steps[\"rel_time\"])\n",
    "    min_allowed_time = min_target_time - mins_before_first_icp\n",
    "    max_target_time = max(target_steps[\"rel_time\"])\n",
    "    # kick out (basically set to NaN) measurements beyond interesting region\n",
    "    pat = pat[pat[\"rel_time\"] >= min_allowed_time]\n",
    "    pat = pat[pat[\"rel_time\"] <= max_target_time]\n",
    "    # adjust rel_time\n",
    "    pat[\"rel_time\"] -= min(pat[\"rel_time\"])\n",
    "    return pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78185709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "PYTHON_PATH = sys.executable\n",
    "# install pyarrow\n",
    "!{PYTHON_PATH} -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895c3ce-620c-4336-8d42-9c8cadd17b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_icp = True\n",
    "\n",
    "if load_icp:\n",
    "    df_long = pd.read_parquet(\"data/Datenbank_icp.parquet\", engine='pyarrow')\n",
    "    if \"Unnamed: 0\" in df_long:\n",
    "        df_long = df_long.drop(columns=[\"Unnamed: 0\"])\n",
    "else:\n",
    "    if not os.path.exists(\"data/Datenbank_Werte.parquet\"):\n",
    "        df_long = pd.read_csv(\"data/Datenbank_Werte.csv\")\n",
    "        # drop where Pat_ID or Wert is NaN\n",
    "        df_long = df_long.dropna(subset=[\"Pat_ID\", \"Wert\"])\n",
    "        # convert Pat_ID to int by removing \"_\" to save it more efficiently with parquet\n",
    "        df_long[\"Pat_ID\"] = df_long[\"Pat_ID\"].apply(lambda x: int(str(x).replace(\"_\", \"\"))).astype(int)\n",
    "        df_long.to_parquet(\"data/Datenbank_Werte.parquet\")\n",
    "    df_long = pd.read_parquet(\"data/Datenbank_Werte.parquet\", engine='pyarrow')\n",
    "    print(\"Loaded data\")\n",
    "    # drop eICU\n",
    "    #df_long = df_long[df_long[\"DB\"] != \"eICU\"]\n",
    "    \n",
    "    # add type to Maßnahme\n",
    "    df_long[\"Maßnahme\"] += \"_\" + df_long[\"ID\"]\n",
    "    df_long = df_long.drop(columns=\"ID\")\n",
    "\n",
    "    # drop measurements where we have a \"NAN\"\n",
    "    print(\"Drop NaN measurements\")\n",
    "    df_long = df_long.dropna(subset=[\"Wert\"])\n",
    "    \n",
    "    # drop duplicates\n",
    "    print(\"Dropping duplicates...\")\n",
    "    df_long = df_long.drop_duplicates(subset=[\"Pat_ID\", \"Maßnahme\", \"rel_time\", \"Wert\"])\n",
    "    \n",
    "    # fix rel_times to ICP, drop steps after last ICP measurement, filter ICP outliers\n",
    "    # use 0.999 quantile = -10 to 105\n",
    "    print(\"Fixing ICP times...\")\n",
    "    df_long = df_long.groupby(\"Pat_ID\").apply(lambda pat: \n",
    "                                choose_target_period(pat, mins_before_first_icp=60 * 24, \n",
    "                                                     target_name=\"ICP_Vital\",\n",
    "                                                     min_val=-10, max_val=105))\n",
    "    df_long = df_long.reset_index(drop=True)\n",
    "\n",
    "    # merge \"NBD\" (non-invasive ways of measuring blood pressure) with invasive ways as they measure the same thing\n",
    "    print(\"Merging NBD...\")\n",
    "    def rename_nbd(name):\n",
    "        if \"syst\" in name:\n",
    "            name = \"syst_Vital\"\n",
    "        elif \"diast\" in name:\n",
    "            name = \"diast_Vital\"\n",
    "        elif \"mittl\" in name:\n",
    "            name = \"mittl_Vital\"\n",
    "        return name\n",
    "    df_long[\"Maßnahme\"] = df_long[\"Maßnahme\"].apply(rename_nbd)\n",
    "    # save    \n",
    "    df_long.to_parquet(\"data/Datenbank_icp.parquet\", engine='pyarrow')\n",
    "    \n",
    "    # save maßnahme and maßnahme norm for meds\n",
    "    measures_norm_groups = df_long.groupby(\"Maßnahme\")\n",
    "    mapping_dict = {}\n",
    "    for group_name, group in measures_norm_groups:\n",
    "        if \"_Med\" in group_name:\n",
    "            measures = list(group[\"Maßnahme_norm\"].unique())\n",
    "            mapping_dict[group_name] = measures\n",
    "    # save mapping dict\n",
    "    import json\n",
    "    with open(\"data/measure_norm_mapping_dict.json\", \"w\") as f:\n",
    "        json.dump(mapping_dict, f)\n",
    "print(\"Data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72658f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.DB.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77a6f1-bcd3-45c5-9006-b6050f450e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine columns that have few missings - add min, max, std for those\n",
    "val_counts = df_long.groupby(\"DB\").apply(lambda db: db.groupby(\"Maßnahme\").apply(lambda m: len(m))).reset_index(drop=False)\n",
    "val_counts = val_counts.groupby(\"Maßnahme\").mean()[0]\n",
    "val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a5da2-0e95-4a3a-a781-f14f8463782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select high count vitals \n",
    "quant = 200000 # chosen such that ICP and everything more frequent than it stays in #np.quantile(val_counts, 0.85)\n",
    "print(quant)\n",
    "mask = (val_counts > quant).astype(int) + val_counts.index.str.contains(\"_Vital\").astype(int) #+  (~val_counts.index.str.contains(\"ICP\")).astype(int)\n",
    "mask = mask == 2\n",
    "high_counts = val_counts[mask].sort_values(0)\n",
    "print(high_counts)\n",
    "high_counts = list(high_counts.index) + [\"ICP_Vital\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7026d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dupls = False\n",
    "\n",
    "if check_dupls:\n",
    "    for db in df_long.DB.unique():\n",
    "        df_long_db = df_long[df_long[\"DB\"] == db]\n",
    "        first_5_vals = df_long_db.groupby(\"Pat_ID\").apply(lambda pat: pat.sort_values(\"rel_time\")[\"rel_time\"].iloc[:5]).reset_index()\n",
    "        arrays = []\n",
    "        for pat_id in first_5_vals.Pat_ID.unique():\n",
    "            array = first_5_vals[first_5_vals.Pat_ID == pat_id].values\n",
    "            arrays.append(array)\n",
    "            \n",
    "        s = []\n",
    "        for arr in arrays:\n",
    "            s.append(tuple(arr.reshape(-1).astype(float).round(0)))\n",
    "        s = set(s)\n",
    "        \n",
    "        if len(s) == len(arrays):\n",
    "            print(\"No duplicates in \" + db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f14935",
   "metadata": {},
   "outputs": [],
   "source": [
    "icp_vals = df_long[df_long[\"Maßnahme\"] == \"ICP_Vital\"].reset_index()\n",
    "diffs = icp_vals.groupby(\"Pat_ID\").apply(lambda pat: pat.sort_values(\"rel_time\")[\"rel_time\"].diff()).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbf50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_diff = 24 * 60\n",
    "\n",
    "large_diff_ids = icp_vals[diffs > max_diff].Pat_ID\n",
    "len(large_diff_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_diff = 128 * 60\n",
    "\n",
    "large_diff_ids = icp_vals[diffs > max_diff].Pat_ID.unique()\n",
    "for pat_id in large_diff_ids:\n",
    "    pat_vals = icp_vals[icp_vals[\"Pat_ID\"] == pat_id]\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    pat_diffs = pat_vals.rel_time.diff()\n",
    "    large_diffs = pat_diffs[pat_diffs > max_diff].reset_index()[\"rel_time\"]\n",
    "    times_of_diffs = pat_vals[pat_diffs > max_diff].reset_index()[\"rel_time\"]\n",
    "    for large_diff_time, difference in zip(times_of_diffs, large_diffs):\n",
    "        # plot vertical line at time\n",
    "        plt.axvline(large_diff_time, color=\"red\")\n",
    "        num_vals_above = sum(pat_vals[\"rel_time\"] >= large_diff_time)\n",
    "        num_vals_below = sum(pat_vals[\"rel_time\"] < large_diff_time)\n",
    "        print(pat_id, num_vals_above, num_vals_below)\n",
    "    \n",
    "    pat_vals.plot(x=\"rel_time\", y=\"Wert\", kind=\"scatter\", ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13018be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see many regions where no ICP is measured at all. We can remove those and only take into account the regions where ICP is measured (+ 24 hours before)\n",
    "max_time_diff = 24 * 60\n",
    "min_size = 2\n",
    "time_before_icp = 24 * 60\n",
    "\n",
    "num_windows = []\n",
    "\n",
    "def cut_into_windows(pat_data, icp_times, icp_time_diffs):\n",
    "    times_of_large_diffs = icp_times[icp_time_diffs > max_time_diff]\n",
    "    if len(times_of_large_diffs) == 0:\n",
    "        pat_data[\"window_id\"] = 0\n",
    "        return [pat_data]\n",
    "    \n",
    "    # now we have the sorted times of large diffs\n",
    "    # we want to split regions in two: before and after. If a region is too small, we can remove it\n",
    "    last_time = 0\n",
    "    time_windows = []\n",
    "    for time in times_of_large_diffs:\n",
    "        # get the time of the ICP measurement before the large diff\n",
    "        measure_time_before = icp_times[icp_times < time][-1]\n",
    "        # get the region between the last time and the measurement before the large diff\n",
    "        before = icp_times[(icp_times <= measure_time_before) & (icp_times >= last_time)]\n",
    "        if len(before) >= min_size:\n",
    "            time_windows.append((last_time, measure_time_before))\n",
    "        last_time = time - time_before_icp\n",
    "            \n",
    "    final_part = icp_times[icp_times >= last_time]\n",
    "    if len(final_part) >= min_size:\n",
    "        time_windows.append((last_time, final_part[-1]))\n",
    "    \n",
    "    windows = []\n",
    "    for idx, (start, end) in enumerate(time_windows):\n",
    "        pat_window = pat_data[(pat_data[\"rel_time\"] >= start) & (pat_data[\"rel_time\"] <= end)]\n",
    "        pat_window[\"window_id\"] = idx\n",
    "        windows.append(pat_window)\n",
    "    return windows\n",
    "\n",
    "df_long_copy = df_long.copy()\n",
    "pats = []\n",
    "pat_groups = df_long.groupby(\"Pat_ID\")\n",
    "for pat_id, pat_data in tqdm(pat_groups):\n",
    "    # delete pat_mask'ed from df_long_copy to speed up processing for next pat_ids\n",
    "    #df_long_copy = df_long_copy[~pat_mask]\n",
    "    # get icp measurements for pat\n",
    "    icp_measurements = pat_data[pat_data[\"Maßnahme\"] == \"ICP_Vital\"]\n",
    "    icp_times = icp_measurements.rel_time.values\n",
    "    icp_time_diffs = icp_measurements.rel_time.diff()\n",
    "    # cut into windows\n",
    "    windows = cut_into_windows(pat_data, icp_times, icp_time_diffs)\n",
    "    pats.extend(windows)\n",
    "            \n",
    "df_long_windows = pd.concat(pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_long_windows.to_parquet(\"data/df_long_windows.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce45de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare df_long and df_long_windows \n",
    "print(len(df_long) - len(df_long_windows))\n",
    "# compare number of ICP measurements in df_long and df_long_windows\n",
    "print(len(df_long[df_long[\"Maßnahme\"] == \"ICP_Vital\"]) - len(df_long_windows[df_long_windows[\"Maßnahme\"] == \"ICP_Vital\"]))\n",
    "print(df_long_windows.groupby(\"DB\").apply(lambda db: len(db[db[\"Maßnahme\"] == \"ICP_Vital\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee412784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df_long_windows = pd.read_csv(\"data/df_long_windows.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859037af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check windows lengths! longest should be at most 60 (minutes) * 128 (hours) = 7680 minutes\n",
    "df_long_windows.groupby(\"Pat_ID\").apply(lambda x: x.groupby(\"window_id\").apply(lambda win: win[win[\"Maßnahme\"] == \"ICP_Vital\"].rel_time.diff().max())).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df_long_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6454c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_windows.groupby(\"DB\").apply(lambda db: db.value_counts(\"window_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbb9d1-431a-4af2-831a-15742cfac75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distance between icp measurements per DB\n",
    "for db in [\"UKE\", \"MIMIC\", \"eICU\"]:\n",
    "    df_db = df_long[df_long[\"DB\"] == db]\n",
    "    \n",
    "    plt.figure()\n",
    "    diffs_db = df_db.groupby([\"Pat_ID\", \"window_id\"]).apply(lambda win: win[win[\"Maßnahme\"] == \"ICP_Vital\"].sort_values(\"rel_time\").rel_time.diff()).reset_index(drop=True)    \n",
    "    diffs_db = diffs_db[diffs_db < 125]\n",
    "    if db == \"eICU\":\n",
    "        diffs_db = diffs_db[diffs_db < 10]\n",
    "    # make normalized histogram\n",
    "    diffs_db.hist(bins=100, alpha=1.0, label=db, density=True)\n",
    "    plt.xlabel(\"Minutes\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Minutes between ICP measurements for {db}\")\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.legend()\n",
    "    os.makedirs(\"figures\", exist_ok=True)\n",
    "    plt.savefig(f\"figures/{db}_icp_diffs.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1ced2-4aac-4ce3-b00e-18ef44b440b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "#import numba\n",
    "\n",
    "minutes = 5\n",
    "\n",
    "#@numba.jit()\n",
    "def create_dict(values: np.ndarray, rel_time: np.ndarray, measure_name: str, high_counts: Tuple[str], minutes: int):\n",
    "    means = []\n",
    "    stds = []\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    rel_times = []\n",
    "    i = 0\n",
    "    while i < len(values): \n",
    "        # add consequent measurements as long as they fit in time window\n",
    "        # subtract remainder to make sure rel_times are fitted in the right time window\n",
    "        current_rel_time = rel_time[i] - (rel_time[i] % minutes)  \n",
    "        vals = []\n",
    "        while i < len(values) and rel_time[i] < current_rel_time + minutes:\n",
    "            vals.append(values[i])\n",
    "            i += 1\n",
    "        # summarize values\n",
    "        vals = np.array(vals)\n",
    "        means.append(np.mean(vals))\n",
    "        rel_times.append(current_rel_time)\n",
    "        # add detailed summary if it is a high count feature\n",
    "        if measure_name in high_counts:\n",
    "            stds.append(np.std(vals))\n",
    "            mins.append(np.min(vals))\n",
    "            maxs.append(np.max(vals))\n",
    "    return means, stds, mins, maxs, rel_times\n",
    "\n",
    "\n",
    "def summarize_measure(measure, high_counts, minutes):\n",
    "    # get name and relevant columns\n",
    "    measure_name = measure[\"Maßnahme\"].iloc[0]\n",
    "    rel_time = measure[\"rel_time\"].to_numpy().astype(float)\n",
    "    values = measure[\"Wert\"].to_numpy().astype(float)\n",
    "    # get lists\n",
    "    means, stds, mins, maxs, rel_time = create_dict(values, rel_time, measure_name, high_counts, minutes)\n",
    "    # create new, shorter df with same base stats (Pat_ID etc.)\n",
    "    new_df = pd.DataFrame({\"rel_time\": rel_time, \"Wert\": means})\n",
    "    new_df[\"Pat_ID\"] = measure[\"Pat_ID\"].iloc[0]\n",
    "    new_df[\"Maßnahme\"] = measure[\"Maßnahme\"].iloc[0]\n",
    "    new_df[\"Maßnahme_norm\"] = measure[\"Maßnahme_norm\"].iloc[0]\n",
    "    new_df[\"DB\"] = measure[\"DB\"].iloc[0]\n",
    "    new_df[\"window_id\"] = measure[\"window_id\"].iloc[0]\n",
    "    if len(stds) > 0:\n",
    "        # if we have additional stats for this feature, copy the df above, fill in other value and append it\n",
    "        new_measures = {measure_name: means,\n",
    "                        measure_name + \"_std\": stds,\n",
    "                        measure_name + \"_min\": mins,\n",
    "                        measure_name + \"_max\": maxs} \n",
    "        dfs = [new_df.copy() for key in new_measures]\n",
    "        for df, key in zip(dfs, new_measures):\n",
    "            df[\"Maßnahme\"] = key\n",
    "            df[\"Wert\"] = new_measures[key]\n",
    "        new_df = pd.concat(dfs, axis=0)\n",
    "    return new_df\n",
    "            \n",
    "def summarize_patient(pat, high_counts, minutes):\n",
    "    return pat.groupby(\"Maßnahme\").apply(lambda measure: summarize_measure(measure, high_counts, minutes)).reset_index(drop=True)\n",
    "\n",
    "df_long_summarized = df_long.groupby([\"Pat_ID\", \"window_id\"]).progress_apply(lambda pat: summarize_patient(pat.sort_values(\"rel_time\", ascending=True), high_counts, minutes)).reset_index(drop=True)\n",
    "# summarize per N minutes (5 min, 60 min) \n",
    "# take mean, min, max, std per Vital measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4257d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_long.groupby(\"DB\").apply(lambda db: len(db[db[\"Maßnahme\"] == \"ICP_Vital\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c815246",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_long_summarized.groupby(\"DB\").apply(lambda db: len(db[db[\"Maßnahme\"] == \"ICP_Vital\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2708f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for _ in df_long.groupby([\"Pat_ID\", \"window_id\"]):\n",
    "    count += 1\n",
    "print(\"Total windows: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b2fd9-fea4-4bfc-beac-a24d31c90c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm med by maßnahme norm per db\n",
    "meds = [feat for feat in df_long_summarized[\"Maßnahme\"].unique() if \"_Med\" in feat]\n",
    "\n",
    "meds\n",
    "\n",
    "# not done anymore to avoid information leakage from test to train and also to avoid merging meds of different strengths into one\n",
    "# instead we now replace the Maßnahme col by Maßnahme_norm for all meds \n",
    "def replace_measure(measure):\n",
    "    if measure[\"Maßnahme\"].iloc[0] in meds:\n",
    "        measure[\"Maßnahme\"] = measure[\"Maßnahme_norm\"] + \"_Med\"\n",
    "    return measure\n",
    "df_long_meds = df_long_summarized.groupby(\"Maßnahme\").apply(lambda measure: replace_measure(measure)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b058d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_now = [feat for feat in df_long_meds[\"Maßnahme\"].unique() if \"_Med\" in feat]\n",
    "\n",
    "meds, meds_now, len(meds), len(meds_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d1527-0d00-467e-b816-5c72ab5a79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#icp_long = df_long[df_long[\"Maßnahme\"] == \"ICP_Vital\"]\n",
    "#print(icp_long.groupby(\"DB\").apply(len))\n",
    "#icp_long_summ = df_long_summarized[df_long_summarized[\"Maßnahme\"] == \"ICP_Vital\"]\n",
    "#print(icp_long_summ.groupby(\"DB\").apply(lambda x: len(x)))\n",
    "#icp_long_med_normed = df_long_normed_med[df_long_normed_med[\"Maßnahme\"] == \"ICP_Vital\"]\n",
    "#print(icp_long_med_normed.groupby(\"DB\").apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe46c54-eff1-4ad1-9e6c-3dc9374f04f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from long to wide\n",
    "df = df_long_meds.pivot_table(index=[\"rel_time\", \"Pat_ID\", \"DB\", \"window_id\"], columns=\"Maßnahme\", values=\"Wert\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2392cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f533a-ea19-4634-8011-043a61a9a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(f\"data/df_wide_{minutes}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4ed31-e766-4f40-88a9-341fb612dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num ICP measurements per DB\n",
    "print(len(df))\n",
    "print(df.groupby(\"DB\").apply(lambda x: len(x[~x[\"ICP_Vital\"].isna()])))\n",
    "print(df[\"ICP_Vital\"].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a04ca-02dd-42b5-b8ec-3a0eae4d5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in rows with NAN where we have no measurement at all\n",
    "\n",
    "#@numba.jit()\n",
    "def add_nan_rows(arr: np.ndarray, rel_time: np.ndarray, minutes: int):\n",
    "    # check dist between all consecutive rel_times and create NaN array, which is added to list. Merge this list in the end\n",
    "    num_feats = arr.shape[1]\n",
    "    rows = []\n",
    "    old_time = rel_time[0]\n",
    "    for row, t in zip(arr, rel_time):\n",
    "        #print(t)\n",
    "        # check how many rows between old_time and current time have to be added\n",
    "        num_rows = int(np.ceil((t - old_time) / minutes)) - 1\n",
    "        if num_rows > 0:\n",
    "            #print(num_rows)\n",
    "            #num_rows_added.append(num_rows)\n",
    "            new_rows = np.ones((num_rows, num_feats)) * np.nan\n",
    "            rows.append(new_rows)\n",
    "        rows.append(np.expand_dims(row, 0))\n",
    "        old_time = t\n",
    "    return np.concatenate(rows, axis=0)\n",
    "\n",
    "\n",
    "num_rows_added = []\n",
    "def unroll_patient(pat, minutes):\n",
    "    # assumes a patient, sorted by rel_time\n",
    "    # transform to numpy array and move to helper function\n",
    "    rel_time = pat[\"rel_time\"].to_numpy()\n",
    "    db = pat[\"DB\"].iloc[0]\n",
    "    pat_id = pat[\"Pat_ID\"].iloc[0]\n",
    "    # drop DB and pat_id temporarily, as their type is str\n",
    "    pat = pat.drop(columns=[\"DB\", \"Pat_ID\"]).astype(np.float32)\n",
    "    arr = pat.to_numpy()\n",
    "\n",
    "    # get new_array that contains nan_rows where necessary\n",
    "    #print(\"old array shape: \", pat.shape)\n",
    "    new_array = add_nan_rows(arr, rel_time, minutes)\n",
    "    num_rows_added.append(len(new_array) - len(arr))\n",
    "    #print(\"new array shape: \", new_array.shape)\n",
    "    # set Pat_ID, rel_time and col names correctly at end\n",
    "    new_pat = pd.DataFrame(data=new_array, columns=pat.columns, index=np.arange(len(new_array)))\n",
    "    new_pat[\"DB\"] = db \n",
    "    new_pat[\"Pat_ID\"] = pat_id\n",
    "    # fill in rel_times\n",
    "    start_time = pat[\"rel_time\"].iloc[0]\n",
    "    new_pat[\"rel_time\"] = np.arange(start_time, start_time + len(new_pat) * minutes, minutes)\n",
    "    return new_pat\n",
    "    \n",
    "\n",
    "df_filled = df.groupby([\"Pat_ID\", \"window_id\"]).progress_apply(lambda pat: unroll_patient(pat.sort_values(\"rel_time\", ascending=True), minutes)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008fcd0f-c06d-43fc-a0d4-9bdcaaf078eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows added: \", sum(num_rows_added))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0281a0e-29ff-4ff7-92a6-c3d8f1de8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "p = plt.hist(num_rows_added, bins=100)\n",
    "plt.xlabel(\"Num consecutive missing steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eff92a-6fd4-4c75-890f-08a2c1efea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(num_rows_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204120b-00ca-4871-bbc0-62f9b41f5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filled.to_csv(f\"data/df_filled_{minutes}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066aca2-1526-4b7e-83a9-cbb1abc7622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filled = pd.read_csv(\"data/df_filled_60.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a962b9-0074-4fd3-b63d-7594db5389b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c305ac0d-b919-4103-ab1c-c5d10dd161e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill med missing with 0\n",
    "df_med_filled = df_filled.copy()\n",
    "med_col_names = [col for col in df_med_filled.columns if \"_Med\" in col]\n",
    "df_med_filled[med_col_names] = df_med_filled[med_col_names].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a807cfc-75a4-4ce2-aa65-c555537c1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabled for now as it reduces UKE performance a lot. Might be useful for better dataset transfer in the future\n",
    "\"\"\"\n",
    "# show values with highest std of NaNs between DBs\n",
    "max_nan_frac_over_dbs = df_med_filled.groupby(\"DB\").apply(lambda db: db.isna().mean()).std().sort_values().iloc[-10:]\n",
    "max_nan_frac_over_dbs\n",
    "# select highest ranking ones\n",
    "drop_cols = [\"PEEP_Vital\", \"Pmean_Vital\", \"Freq gesamt_Vital\", \"Ppeak_Vital\", \"FiO2_Vital\", \"Freq spontan_Vital\"]\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c036d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med_filled.groupby(\"DB\").apply(lambda db: db.isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a875d8-5fda-43fb-8688-68ddaebb4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove feature that have N% missing spots in at least one database\n",
    "#threshold = 1 - (1 - 0.999) / (60 / minutes)  # make it such that it is 0.99 for 60 minutes and 0.999 for 5 minutes\n",
    "#max_nan_frac_over_dbs = df_med_filled.groupby(\"DB\").apply(lambda db: db.isna().mean()).max().sort_values()\n",
    "#drop_cols = list(max_nan_frac_over_dbs[max_nan_frac_over_dbs > threshold].index)\n",
    "#df_drop_too_missing = df_med_filled.drop(columns=drop_cols)\n",
    "#print(\"Dropped: \", drop_cols, \" - \", len(drop_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa3430-9174-496d-94eb-5b63087e3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get patient data such as gender etc\n",
    "df_static = pd.read_csv(\"data/Datenbank_Pat_ID.csv\")\n",
    "df_static = df_static[[\"Pat_ID\", \"Alter\", \"Diagnose_txt\", \"Geschlecht\", \"Größe\", \"Gewicht\"]]\n",
    "df_static = df_static.rename(columns={\"Diagnose_txt\": \"Diagnose\"})\n",
    "# one-hot encode\n",
    "df_static = pd.get_dummies(df_static, columns=[\"Diagnose\", \"Geschlecht\"])\n",
    "df_static = df_static.drop(columns=[\"Geschlecht_Weiblich\"])\n",
    "df_static = df_static.rename(columns={\"Geschlecht_Männlich\": \"Geschlecht\"})\n",
    "# drop nan Pat_ID\n",
    "df_static = df_static.dropna(subset=[\"Pat_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31334472-d824-4098-b41c-a3dedbcca906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "df_med_filled[\"Pat_ID\"] = df_med_filled[\"Pat_ID\"].astype(int)\n",
    "df_static[\"Pat_ID\"] = df_static[\"Pat_ID\"].astype(int)\n",
    "df_large = pd.merge(df_med_filled, df_static, on=\"Pat_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6f2e4-4558-4f14-bc07-52c92735e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encode DB\n",
    "df_large = pd.get_dummies(df_large, columns=[\"DB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863dd37-dddb-411e-aee1-41df96b2e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = f\"data/df_final_{minutes}.csv\"\n",
    "#df_large.to_csv(path, index=False)\n",
    "#print(\"Saved to: \", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03095c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_large = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d4f9f-6bbf-494a-ad41-e7b5aa80ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_large.isna().mean().sort_values().iloc[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9422494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e172acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure how often meds are non-zero\n",
    "for db in [\"DB_UKE\", \"DB_MIMIC\", \"DB_eICU\"]:\n",
    "    db_df = df_large[df_large[db] == 1]\n",
    "    sorted_meds = (db_df[med_col_names] == 0).mean().sort_values()\n",
    "    print(db)\n",
    "    print(sorted_meds.iloc[:20])\n",
    "    \n",
    "    too_many_zero_cols = list(sorted_meds[sorted_meds >= 0.995].index)\n",
    "    #print(too_many_zero_cols)\n",
    "    # per DB, set med values to NaN if more than 99.9% are zero\n",
    "    df_large.loc[df_large[db] == 1, too_many_zero_cols] = np.nan\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large.isna().mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619543ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large[df_large[\"DB_UKE\"] == 0].isna().mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set feats completely to NaN per DB that now have too many NaNs\n",
    "threshold = 0.995\n",
    "\n",
    "for db in [\"DB_UKE\", \"DB_MIMIC\", \"DB_eICU\"]:\n",
    "    db_df = df_large[df_large[db] == 1]\n",
    "    missing_per_col = db_df.isna().mean().sort_values()\n",
    "    # ignore completely missing ones\n",
    "    missing_per_col = missing_per_col[missing_per_col < 1.0]\n",
    "    print(db)\n",
    "    print(missing_per_col.iloc[-5:])\n",
    "    print()\n",
    "    drop_cols = list(missing_per_col[missing_per_col > threshold].index)\n",
    "    df_large.loc[df_large[db] == 1, drop_cols] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfa2c8-c122-4dba-a3e8-f09fa7b1f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove correlated feats per database. Set dropped values to NaN\n",
    "df_corr = df_large.copy()\n",
    "\n",
    "threshold = 0.90\n",
    "\n",
    "for db in [\"DB_UKE\", \"DB_MIMIC\", \"DB_eICU\"]:\n",
    "    db_df = df_corr[df_corr[db] == 1]\n",
    "    corr_matrix = db_df.corr()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    print(db)\n",
    "    print(to_drop)\n",
    "    print()\n",
    "    df_corr.loc[df_corr[db] == 1, to_drop] = np.nan\n",
    "\n",
    "# drop columns that are now completely NaN\n",
    "# first print them\n",
    "print(df_corr.isna().mean().sort_values().iloc[-10:])\n",
    "# then drop them\n",
    "df_corr = df_corr.dropna(axis=1, how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc4498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e4fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2be43-525d-42f4-ba60-f7c5a04a8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_split_idcs(df, test_size=0.2):\n",
    "    pat_ids = np.array(sorted(df[\"Pat_ID\"].unique()))\n",
    "    seq_list = [df[df[\"Pat_ID\"] == pat_id] for pat_id in pat_ids]\n",
    "    dev_data, test_data, dev_idcs, test_idcs = make_split(seq_list, test_size=test_size, seed=1)\n",
    "    #df[dev_idcs][\"split\"] = \"dev\"\n",
    "    #df.loc[test_idcs, \"split\"] = \"test\"\n",
    "    train_data, val_data, train_idcs, val_idcs = make_split(dev_data, test_size=test_size, seed=1)\n",
    "    mapped_train_idcs = dev_idcs[train_idcs]\n",
    "    mapped_val_idcs = dev_idcs[val_idcs]\n",
    "    \n",
    "    # map idcs to pat_ids\n",
    "    train_pat_ids = set(pat_ids[mapped_train_idcs])\n",
    "    val_pat_ids = set(pat_ids[mapped_val_idcs])\n",
    "    test_pat_ids = set(pat_ids[test_idcs])\n",
    "    #print(test_pat_ids)\n",
    "    \n",
    "    # set column in original df according to idcs\n",
    "    def assign_split_name(pat_id):\n",
    "        if pat_id in train_pat_ids:\n",
    "            return \"train\"\n",
    "        elif pat_id in val_pat_ids:\n",
    "            return \"val\"\n",
    "        else:\n",
    "            return \"test\"\n",
    "    df[\"split\"] = df[\"Pat_ID\"].apply(assign_split_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c49425-700e-45fe-a626-998c122dca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_seq_labels(seq_list, target_name=\"ICP_Vital\"):\n",
    "    median_len = np.median([len(pat) for pat in seq_list])\n",
    "    median_target = np.median([seq[target_name][~seq[target_name].isna()].mean() for seq in seq_list])\n",
    "    #print(\"Mean len: \", mean_len)\n",
    "    #print(\"Mean target: \", mean_target)\n",
    "    labels =  [(len(seq) < median_len).astype(int).astype(str) +\n",
    "               ((seq[target_name][~seq[target_name].isna()].mean() < median_target).astype(int).astype(str))\n",
    "               for seq in seq_list]\n",
    "    return labels\n",
    "\n",
    "def make_split(seq_list, test_size=0.2, seed=1):\n",
    "    indices = np.arange(len(seq_list))\n",
    "    labels = create_seq_labels(seq_list)\n",
    "    train_data, val_data, train_idcs, val_idcs = train_test_split(seq_list, indices, test_size=test_size,\n",
    "                                                                  stratify=labels, shuffle=True,\n",
    "                                                                  random_state=seed)\n",
    "    return train_data, val_data, train_idcs, val_idcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990631ec-0afa-43ad-bc46-eefcd1a4318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#from icp_pred.data_utils import make_split\n",
    "db_cols = [col for col in df_corr.columns if \"DB_\" in col]\n",
    "for db_name in db_cols:\n",
    "    print(db_name)\n",
    "    db_df = df_corr[df_corr[db_name] == 1].drop(columns=db_cols)\n",
    "    # drop columns that are completely NaN\n",
    "    db_df = db_df.dropna(axis=1, how=\"all\")\n",
    "    # drop columns that are completely zero\n",
    "    mean_zeros = (db_df == 0).mean()\n",
    "    db_df = db_df[list(mean_zeros[mean_zeros < 1.0].index)]\n",
    "    print(db_df.shape)\n",
    "    print(\"ICP measurements in DB: \", (~db_df[\"ICP_Vital\"].isna()).sum())\n",
    "    # make splits if 60 minutes else take split from 60 minutes\n",
    "    if minutes == 60:\n",
    "        db_df = add_split_idcs(db_df, test_size=0.2)\n",
    "    else:\n",
    "        ref_df = pd.read_parquet(f\"data/{db_name}_60_final_df.parquet\")\n",
    "        mapping = {pat_id: ref_df[ref_df[\"Pat_ID\"] == pat_id].iloc[0][\"split\"] for pat_id in ref_df[\"Pat_ID\"].unique()}\n",
    "        split = np.array(db_df[\"Pat_ID\"].apply(lambda x: mapping[x]))\n",
    "        db_df[\"split\"] = split\n",
    "    # save it\n",
    "    save_path = f\"data/{db_name}_{minutes}_final_df.pkl\"\n",
    "    os.makedirs(\"data/\", exist_ok=True)\n",
    "    db_df.to_parquet(f\"data/{db_name}_{minutes}_final_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9cb63-9a91-4c4b-88f9-d68c9581ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6ff73-8f13-47da-80d7-a0457739cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(db_df[\"split\"] == \"train\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b6af7-9cee-4f92-bc90-c0c08f399392",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(db_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4dcd9f-2900-4077-9d47-c2412c46486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check icp distributions\n",
    "db_df[db_df[\"split\"] == \"train\"][\"ICP_Vital\"].hist(bins=100, density=True, alpha=0.5)\n",
    "db_df[db_df[\"split\"] == \"val\"][\"ICP_Vital\"].hist(bins=100, density=True, alpha=0.5)\n",
    "db_df[db_df[\"split\"] == \"test\"][\"ICP_Vital\"].hist(bins=100, density=True, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd54b28-44d1-43a2-aa96-bf44b06fa4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = db_df[\"ICP_Vital\"]\n",
    "\n",
    "vals[vals < 0].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals.quantile(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68eb6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b020b1-baa5-47cc-92b3-44d864152b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5609c7d-3010-4d3a-b02f-f77b764d99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distributions\n",
    "p = uke_df.hist(bins=100, figsize=(15, 15))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcce67-d469-4df7-a4b1-51f1b2ffe75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "transform = PowerTransformer(method='yeo-johnson', standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561dc1de-3817-4d29-9e53-1764e34234f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uke_df = df_large[df_large[\"DB_UKE\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e279f-0d3a-4d7c-957c-efdf375c462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uke_arr = uke_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e4340-748b-4708-b0f8-2f3b175f139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.fit(uke_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f46cb8-adaf-41e8-b7d4-41aeef0239ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(transform.lambdas_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0e61b-4ddf-454f-a642-e2bd75818d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_arr = transform.transform(uke_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b638d-a1e5-479e-a9ad-3ddc926ba035",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_ = transform.lambdas_\n",
    "mask = np.abs(lambdas_) > 5\n",
    "print(lambdas_[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24078b-1084-4acb-a944-4332b21f36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = pd.DataFrame(transformed_arr, columns=df_large.columns)#, index=df_large.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be29b31-6a66-472a-9f24-bde2f915a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = \"sO2_BGA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c92b5-a8c1-458f-a987-b99e35a7e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(transformed_df.columns).index(feat)\n",
    "lambdas_[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa9686-c217-4b21-8328-c501a445bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "uke_df[feat].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45cb8a-939e-4133-82ab-1422e65833bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df[feat].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04588faf-a676-42b9-b8a3-dfd91892b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uke_df[feat].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c712ff-6b02-4175-aef7-5afc10b20409",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df[feat].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8c162-f9d5-4ae8-bded-79a629c21e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "uke_df[feat].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716000f6-ca88-4758-8487-b2b5e2fb431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df[feat].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e2459-ca93-4297-908e-55dcbbfe98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "\n",
    "@numba.jit()\n",
    "def ema_fill(pat: np.ndarray, ema_val: float, mean: np.ndarray):\n",
    "    # init ema\n",
    "    ema = np.ones_like(pat[0]) * pat[0]\n",
    "    ema[np.isnan(ema)] = mean[np.isnan(ema)]\n",
    "    # run ema\n",
    "    ema_steps = np.ones_like(pat)\n",
    "    for i, pat_step in enumerate(pat):\n",
    "        pat_step[np.isnan(pat_step)] = 0\n",
    "        ema = ema_val * ema + (1 - ema_val) * pat_step\n",
    "        ema_steps[i] = ema.copy()\n",
    "    return ema_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82554915-3254-4360-bc8d-efee1b8725c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = uke_df.mean().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7943b06-ca6a-40f6-b4a5-ef88731fede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uke_filled = uke_df.groupby(\"Pat_ID\").apply(lambda pat: pd.DataFrame(ema_fill(pat.sort_values(\"rel_time\").to_numpy(), 0.9, mean), columns=pat.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84702d-3c9d-4ce9-a5d0-4395760f1173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4e13e-f07a-46f4-92f6-f75fd4e594e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_parts(df):\n",
    "    leng = len(df)\n",
    "    first = df.iloc[0: leng // 4].mean()\n",
    "    second = df.iloc[leng // 4: leng // 2].mean()\n",
    "    third = df.iloc[leng // 2: leng * 3 // 4].mean()\n",
    "    fourth = df.iloc[leng * 3 // 4:].mean()\n",
    "    all_parts = [first, second, third, fourth]\n",
    "    return pd.DataFrame(all_parts, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677ad01-b730-4046-aec4-c1c8c06acb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f3718-0275-494b-a3e9-122c9fe99b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_arr_red = umap_norm.reset_index(drop=True).groupby(\"Pat_ID\").apply(four_parts).reset_index(drop=True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73360c6-78cc-4ad2-8d25-81f64e8dfb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_arr_dropped = umap_arr_red.drop(columns=[\"DB_MIMIC\", \"DB_UKE\", \"DB_eICU\", \"Pat_ID\", \"ICP_Vital\"]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963c610-9e88-42d0-9c54-aac3bdba701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def apply_yeo(df, thresh=50, lambs=None):\n",
    "    transform = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    drop_cols = [\"DB_MIMIC\", \"DB_UKE\", \"DB_eICU\", \"Pat_ID\", \"ICP_Vital\", \"rel_time\"]\n",
    "    dropped = df.drop(columns=drop_cols)\n",
    "    arr = dropped.to_numpy()\n",
    "    if lambs is None:\n",
    "        # apply yeo\n",
    "        transform.fit(arr)\n",
    "        lambs = transform.lambdas_ \n",
    "        mask = np.abs(lambs) > thresh\n",
    "        print(df[\"DB_MIMIC\"].sum(), df[\"DB_UKE\"].sum(), df[\"DB_eICU\"].sum())\n",
    "        print(mask.sum())\n",
    "        #print(pd.Series(np.round(lambs, 1), index=dropped.columns).sort_values(np.abs(lambs)))\n",
    "        print(np.round(lambs, 1))\n",
    "        print()\n",
    "        lambs[mask] = 1\n",
    "    transform.lambdas_ = lambs\n",
    "    trans_arr = transform.transform(arr)\n",
    "    # merge back\n",
    "    df = pd.concat([pd.DataFrame(trans_arr, columns=dropped.columns), df[drop_cols]], axis=1)\n",
    "    return df, lambs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0a753-ef45-4756-af17-9369489156b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_umap_df(df, yeo=False, lambs=None, mean=None, std=None, thresh=10):\n",
    "    # apply yeo\n",
    "    if yeo:\n",
    "        df, _ = apply_yeo(df, lambs=lambs, thresh=thresh)\n",
    "    # calc median\n",
    "    median = df.median().to_numpy()\n",
    "    # fill using ema\n",
    "    df_filled = df.groupby(\"Pat_ID\").apply(lambda pat: pd.DataFrame(ema_fill(pat.sort_values(\"rel_time\").to_numpy(), 0.9, median), columns=pat.columns)).reset_index(drop=True)\n",
    "    # calc mean\n",
    "    if mean is None:\n",
    "        mean = df_filled.mean()\n",
    "        std = df_filled.std()\n",
    "        mean[std == 0] = 0\n",
    "        std[std == 0] = 1\n",
    "    #norm\n",
    "    df_norm = (df_filled - mean) / std\n",
    "    # average over four regions per Pat\n",
    "    df_red = df_norm.groupby(\"Pat_ID\").apply(four_parts).reset_index(drop=True).dropna()\n",
    "    return df_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236e9d8-cd16-45b0-b13a-01ba050285e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeo = False\n",
    "use_train_stats = False\n",
    "thresh = 10\n",
    "\n",
    "lambs = None\n",
    "mean = None\n",
    "std = None\n",
    "    \n",
    "if use_train_stats:\n",
    "    train_df = df_large[df_large[\"DB_UKE\"] == 1]\n",
    "    if yeo:\n",
    "        # calc lambdas for train dataset\n",
    "        #train_df_filled = train_df.groupby(\"Pat_ID\").apply(lambda pat: pd.DataFrame(ema_fill(pat.astype(float).sort_values(\"rel_time\").to_numpy(), 0.9, train_df.median()), columns=pat.columns)).reset_index(drop=True)\n",
    "        train_df, lambs = apply_yeo(train_df, thresh=thresh)\n",
    "         # calc median\n",
    "        median = train_df.median().to_numpy()\n",
    "        # fill using ema\n",
    "        train_df = train_df.groupby(\"Pat_ID\").apply(lambda pat: pd.DataFrame(ema_fill(pat.sort_values(\"rel_time\").to_numpy(), 0.9, median), columns=pat.columns)).reset_index(drop=True)\n",
    "    \n",
    "    # calc mean\n",
    "    mean = train_df.mean().to_numpy()\n",
    "    std = train_df.std()\n",
    "    mean[std == 0] = 0\n",
    "    std[std == 0] = 1\n",
    "\n",
    "\n",
    "grouper = df_large[\"DB_UKE\"] * 1 + df_large[\"DB_MIMIC\"] * 2 + df_large[\"DB_eICU\"] * 3\n",
    "df_red = df_large.groupby(grouper).apply(lambda db: create_umap_df(db, yeo=yeo, lambs=lambs, mean=mean, std=std, thresh=thresh))\n",
    "#df_red = transformed_df.groupby(grouper).apply(create_umap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67301a13-9f43-4a81-bd6b-ddf8de363e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if std is not None:\n",
    "    std.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629279d1-9c35-452c-a01c-c7f0ac7f1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_values = df_red[\"DB_UKE\"] * 1 + df_red[\"DB_MIMIC\"] * 2 + df_red[\"DB_eICU\"] * 3\n",
    "db_values[db_values == 1] = \"UKE\"\n",
    "db_values[db_values == 2] = \"MIMIC\"\n",
    "db_values[db_values == 3] = \"eICU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe67c32d-86e1-4efe-962f-483897ce36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some cols and to numpy\n",
    "drop_cols = [\"DB_MIMIC\", \"DB_UKE\", \"DB_eICU\", \"Pat_ID\", \"ICP_Vital\"]\n",
    "#drop_cols.extend([col for col in df_red.columns if \"_Med\" in col])\n",
    "#drop_cols.extend([col for col in df_red.columns if \"Diagnose\" in col])\n",
    "#drop_cols.extend([col for col in df_red.columns if \"BGA\" in col])\n",
    "#drop_cols.extend([col for col in df_red.columns if \"Labor\" in col])\n",
    "#drop_cols.extend([col for col in df_red.columns if \"Vital\" in col])\n",
    "\n",
    "#print(drop_cols)\n",
    "dropped = df_red.drop(columns=drop_cols)\n",
    "#print(dropped.columns)\n",
    "arr = dropped.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f7f85-8a60-438e-9b35-f163c2af9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used.groupby(grouper).apply(lambda db: db[\"Cl_BGA\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337004a7-eaf2-49b9-885d-c42fdb4f0c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used.groupby(grouper).apply(lambda db: db[\"Cl_BGA\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae2a74-6a28-46a5-a0c6-6dcc79b4a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped.groupby(db_values).apply(lambda db: db[\"Cl_BGA\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba659f14-ad1d-46ad-8528-8aa867e53eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped.groupby(db_values).apply(lambda db: db[\"Cl_BGA\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe21edb-37a0-41ba-8146-5fda49e90a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped[\"Cl_BGA\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dc995e-407c-43f1-af2d-f62fb904972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import umap.plot\n",
    "\n",
    "umapper = UMAP(n_components=2, n_neighbors=15)\n",
    "umapped_arr = umapper.fit_transform(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12ea5d-ae8d-4296-827b-755a99d1824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "mapper = PCA()\n",
    "\n",
    "pca_arr = mapper.fit_transform(arr)\n",
    "pca_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f7d28-f79e-43ae-9a5b-d348e9112dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = db_values.copy()\n",
    "colors[colors==\"UKE\"] = \"red\"\n",
    "colors[colors==\"MIMIC\"] = \"blue\"\n",
    "colors[colors==\"eICU\"] = \"green\"\n",
    "\n",
    "colors = df_red[\"DB_UKE\"]\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.scatter(pca_arr[:, 0], pca_arr[:, 1], c=colors, s=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e692e24-48ae-4a62-b223-996e4fe5d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mapper.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0391618-f1a9-47e1-865b-cb53f40d78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_idx = 0\n",
    "plt.bar(range(len(mapper.components_[comp_idx])), mapper.components_[comp_idx])\n",
    "print(mapper.components_[comp_idx].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e238682-e5be-4510-aa4d-7e85f245dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_comp_idx = np.argmax(mapper.components_[comp_idx])\n",
    "print(max_comp_idx)\n",
    "print(dropped.columns[max_comp_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5c0ef-7ddc-45ed-9cd9-0b662cae9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped.groupby(db_values).apply(lambda x: x[\"Temp_Vital\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a14afb-1f66-4d75-9148-1936f59b1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77bfb5-5b3c-45eb-8bfc-200cfd000b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#umap.plot.points(umapper, theme=\"fire\", values=df_red[\"Pupille re_Vital\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efba52-7c43-4554-833a-f08dc066aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(umapper, theme=\"fire\", values=df_red[\"Phosphat_Labor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec84a0e-7894-430e-a7fb-a0db45b29386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeo all\n",
    "umap.plot.points(umapper, theme=\"fire\", labels=db_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b43694-4c29-4c30-af55-8c41823592aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(umapper, theme=\"fire\", labels=db_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff11b3a-626e-44fe-b952-86eff7138e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeo all\n",
    "umap.plot.points(umapper, theme=\"fire\", labels=db_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67037bf5-94e2-4ad3-abcd-49451ad9772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all\n",
    "umap.plot.points(umapper, theme=\"fire\", labels=db_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b290316f-571f-4691-8501-9ce391281918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all yeo\n",
    "umap.plot.points(umapper, theme=\"fire\", labels=db_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301fd2c-b402-4bc4-b913-8e1f155ba688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f028ac0-e5e0-4377-bfb2-1cef2f7577d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da3077-2c97-4134-9a45-bd82a226d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only static\n",
    "umap.plot.points(umapper, theme=\"fire\", labels=db_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7df012-3cf9-4e36-aa84-dadea29987b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only vital\n",
    "umap.plot.points(umapper, theme=\"fire\", labels=db_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8331b4-d980-4651-928b-a81d6eba5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no med + no diag\n",
    "umap.plot.points(umapper, theme=\"fire\", labels=db_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230e262-0020-461d-a931-02816726e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no med\n",
    "umap.plot.points(umapper, theme=\"fire\", labels=db_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8904c5-56ad-4f95-a77b-4ea19abd85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(umapper, theme=\"fire\", labels=db_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbba894-fb96-4a60-8c41-e57c601f9e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d3a9c-85cb-46bd-9b20-de0c29066c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818d79c-98b5-4f83-8d95-504ffe362cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(transformed_df.columns).index(\"Na_BGA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127f7aa-582b-40f8-8d39-ec9337e68d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = transformed_df.loc[:, mask].hist(figsize=(13, 13), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d43b3a-0e11-437f-9c7d-29575676eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find stds of 0\n",
    "transformed_df.loc[:, transformed_df.std() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d414724-dcb2-4aeb-ab30-b4812598110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df[\"sO2_BGA\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005af29-b411-4ebb-9185-48afdd00aad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846b786-6e18-45ad-b21e-5121ccb6b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(transformed_df[\"sO2_BGA\"].dropna(), 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f76a68-9428-4192-8157-7c87e1c93f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(df_large[\"sO2_BGA\"].dropna(), 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96f0aa-701e-45aa-a058-d0eb2a00c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large[\"sO2_BGA\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931be10-fc7d-4868-a20c-81b5b663e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.max()[transformed_df.max() > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516ef16-78d4-495d-8d17-ce9430c32562",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.loc[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9de4a-b4b4-48ef-b25e-4d6b8048d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df[\"Na_BGA\"].hist(bins=100)\n",
    "transformed_df[\"Na_BGA\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7613d42-fcb7-4b5d-a02c-23091a9d8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = transformed_df.hist(bins=100, figsize=(15, 15))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3eb13a-8ea9-404c-93e9-d5f6a1a9c2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa4675-e9a1-40c8-b802-ced3797c2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "(~df_large[\"ICP_Vital\"].isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765c6c0-0531-4ac2-bfe9-89c0d43e66a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(~df_large[\"ICP_Vital\"].isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7564c-4a2f-46f0-a152-eec961397cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f1392-7445-4688-b8c5-58c10316a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "(~df_large[df_large[\"DB_MIMIC\"] == 1][\"ICP_Vital\"].isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef042c97-7762-4678-a216-51b460fdb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_large[df_large[\"DB_MIMIC\"] == 1][\"ICP_Vital\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99caa34-7f5d-4ccb-94b8-aaf1451626b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 min: 3093093 ICP_Vital non_nans. 154014 in MIMIC\n",
    "# len total: xxx. MIMIC: 2118712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7bcf7-9924-43d4-9ba6-fcc36f9ba3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60 min: 585353 ICP VItal non_nans in total. 128303 in MIMIC\n",
    "# total steps: 735951. MIMIC: 177102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66087bee-c7f5-4e93-9999-73a86270d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"Diagnose_Tumor\"].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515139c-6a2d-4da5-a131-b650d71a739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"Alter\"].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c08787-1737-4245-9e6a-48246776f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/df_final_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213b91e-0763-4201-bb17-33954cbaff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df[test_df[\"DB_MIMIC\"] == 1][\"ICP_Vital\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0d936-c1e7-4314-9e09-d0cec306e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(~test_df[test_df[\"DB_MIMIC\"] == 1][\"ICP_Vital\"].isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be47f08-ca9c-46e7-b84c-1556a686b41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295ca79-763a-4465-9e17-9c32b42dd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(~df_large[df_large[\"DB_MIMIC\"] == 1][\"ICP_Vital\"].isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d67285-8341-46f9-9fac-220f447562fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large[df_large[\"DB_MIMIC\"] == 1][\"ICP_Vital\"].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5493a2-b132-4590-9a68-a3530950fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_ids = []\n",
    "for db in df_long[\"DB\"].unique():\n",
    "    subset = df_long[df_long[\"DB\"] == db]\n",
    "    ids = subset[\"Pat_ID\"].unique()\n",
    "    pat_ids.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4111b9-43bd-4ec8-92ba-eb7363fb6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dict = {}\n",
    "for id_ in pat_ids[0]:\n",
    "    id_dict[id_] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37dec2-74be-426f-bd11-99f66d6ec799",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_ in pat_ids[1]:\n",
    "    if id_ in id_dict:\n",
    "        print(id_, \"is duplicate!\")\n",
    "        id_dict[id_] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980dcb9-a7cd-4c36-83b8-0b3d144501f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_ in pat_ids[2]:\n",
    "    if id_ in id_dict:\n",
    "        print(id_, \"is duplicate!\")\n",
    "        id_dict[id_] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8522d-e8ae-4f6e-9b5e-64577de40940",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isinf(df_large.to_numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b3f0c-85fd-4d0b-9774-06845712a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check specific data\n",
    "clinic = df_large[df_large[\"DB_UKE\"] == 1]\n",
    "pat_ids = clinic[\"Pat_ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce696bfe-cfab-42eb-953a-f66d7dbfe0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd17aa5-5f9e-4a13-8d46-1f256fa49f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinic[\"ICP_Vital\"].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c85941-acaf-4071-97ac-352d7dc6cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_idx = -500\n",
    "\n",
    "pat_id = pat_ids[pat_idx]\n",
    "pat = clinic[clinic[\"Pat_ID\"] == pat_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2355077-e228-4359-8447-4b40cbc2e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat.plot.scatter(x=\"rel_time\", y=\"ICP_Vital\")\n",
    "pat.plot(x=\"rel_time\", y=\"ICP_Vital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e2ffe-8f36-4848-af58-d42392186b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pat[\"HF_Vital\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f335fe-d686-45dd-bb8c-43d4d51bd257",
   "metadata": {},
   "outputs": [],
   "source": [
    "uke = df_long[df_long[\"DB\"] == \"UKE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a036c-2019-4f8f-98b9-2f789451dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uke[uke[\"Maßnahme_norm\"] == \"FiO2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ec849-64fc-4338-8c70-7adcdb1ee3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_frac_per_db = df_filled.groupby(\"DB\").apply(lambda: db.isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66413853-1f03-428c-8ca9-a7483a0c3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_frac_per_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de2a20-46b9-41f9-b7e7-20090b71cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_filled.columns:\n",
    "    nan_means = df_filled.groupby(\"DB\").apply(lambda db: db[col].isna().mean())\n",
    "    print(col)\n",
    "    print(nan_means)\n",
    "    print()\n",
    "    if max(nan_means) > 0.99:\n",
    "        print(\"Drop: \", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d61a38-e94b-4c82-b38d-f8f0c8858f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled.groupby(\"DB\").apply(lambda db: db[\"FiO2_BGA\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a26ccd-0ffe-42c7-aec7-de6a56dc9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled.groupby(\"DB\").apply(lambda db: db[\"FiO2_Vital\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff3dad-69de-45df-95c5-437243e28013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled.groupby(\"DB\").apply(lambda db: db[\"FiO2_Vital\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee739b8-ab6d-4f68-bb4c-20e38cd0e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large[\"FiO2_BGA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6ef8a-45fd-4de3-9e8e-adfa20db2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FiO2 does not exist in UKE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
