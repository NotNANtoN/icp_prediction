{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as  sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"tunings\"\n",
    "\n",
    "\n",
    "# get all paths\n",
    "all_paths = []\n",
    "for dataset_folder in os.listdir(root_folder):\n",
    "    if not os.path.isdir(os.path.join(root_folder, dataset_folder)):\n",
    "        continue\n",
    "    for tune_folder in os.listdir(os.path.join(root_folder, dataset_folder)):\n",
    "        tune_path = os.path.join(root_folder, dataset_folder, tune_folder)\n",
    "        all_paths.append(tune_path)\n",
    "# get all configs, and best params\n",
    "all_cfgs = []\n",
    "all_best_params = []\n",
    "all_scores = []\n",
    "for tune_path in all_paths:\n",
    "    try:\n",
    "        # load cfg and best params\n",
    "        with open(os.path.join(tune_path, \"cfg.json\"), \"r\") as f:\n",
    "            cfg = json.load(f)\n",
    "        best_params = pd.read_csv(os.path.join(tune_path, \"best_params.csv\"), index_col=0)\n",
    "        scores = pd.read_csv(os.path.join(tune_path, \"scores.csv\"), index_col=0)\n",
    "        \n",
    "        try:\n",
    "            clipped_means = pd.read_csv(os.path.join(tune_path, \"metrics_mean_clipped.csv\"), index_col=0)\n",
    "            clipped_means = clipped_means.T.iloc[0].to_dict()\n",
    "            cfg.update(clipped_means)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Could not all metrics in {tune_path}\")\n",
    "        \n",
    "        for param in best_params:\n",
    "            if param in cfg:\n",
    "                cfg[param] = best_params[param].values[0]\n",
    "                \n",
    "        for key in scores:\n",
    "            cfg[key] = scores[key].values[0]\n",
    "        # store\n",
    "        all_cfgs.append(cfg)\n",
    "        all_best_params.append(best_params)\n",
    "        all_scores.append(scores)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Could not load all file in {tune_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge best param list of dfs in one big df\n",
    "all_best_params_df = pd.concat(all_best_params)#.drop(columns=[\"fill_type\"])\n",
    "# merge cfg list to it\n",
    "df = pd.DataFrame(all_cfgs)\n",
    "\n",
    "df = df[df[\"opt_steps\"] == 5]\n",
    "# merge cfg and best params df, avoiding reindexing error\n",
    "#all_df = pd.concat([all_cfgs_df.reset_index(drop=True), all_best_params_df.reset_index(drop=True)], axis=1)\n",
    "# drop all coumns that contain only one unique value\n",
    "df = df.drop(columns=df.nunique()[df.nunique()<=1].index).sort_values(\"cross_val_score_tune\")\n",
    "# set reduction factor NaNs to 16\n",
    "#all_df.loc[all_df[\"reduction_factor\"] == \"NaN\", \"reduction_factor\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model per db\n",
    "best_model = df.groupby([\"db_name\", \"model_type\"]).apply(lambda x: x.groupby(\"model_size\").mean().sort_values(\"cross_val_score_tune\").iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[\"test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[\"cross_val_score_tune\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"cross_val_score_tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"cross_val_score_tune\"\n",
    "#metric = \"bs\"\n",
    "#metric = \"test_score\"\n",
    "#metric = \"r2\"\n",
    "#metric = \"rmse\"\n",
    "\n",
    "dl_df = df[(df.model_type == \"transformer\") | (df.model_type == \"rnn\")]\n",
    "#dl_df = df\n",
    "\n",
    "# eval model size\n",
    "for db in dl_df.db_name.unique():\n",
    "    print(db)\n",
    "    \n",
    "    df_db = dl_df[dl_df[\"db_name\"] == db]\n",
    "    #df_db = df\n",
    "    \n",
    "    sns.catplot(data=df_db, x=\"model_size\", y=metric,\n",
    "                                order=['xt', 'tiny', \"small\", \"base\", \"large\", \"xl\"],\n",
    "                hue=\"model_type\",\n",
    "                linestyles=[\"-\", \"--\"],\n",
    "                kind=\"point\"\n",
    "               )\n",
    "    plt.title(f\"{db}\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    ## test results - only used for debugging, not model selection!\n",
    "    #sns.catplot(data=df_db, x=\"model_size\", y=\"test_score\",\n",
    "    #                            order=['tiny', \"small\", \"base\", \"large\"],#, \"xl\"]\n",
    "    #            hue=\"model_type\",\n",
    "    #            linestyles=[\"-\", \"--\"],\n",
    "    #            kind=\"point\"\n",
    "    #           )\n",
    "    #plt.title(f\"{db} test\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"db_name\"] == \"UKE\"].sort_values(\"cross_val_score_tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"model_type\"] != \"xgb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstms = df[df[\"model_type\"] == \"rnn\"]\n",
    "lstms = lstms.drop(columns=lstms.nunique()[lstms.nunique()<=1].index)\n",
    "lstms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"cross_val_score_tune\"\n",
    "y = \"test_score\"\n",
    "\n",
    "import seaborn as sns\n",
    "#p = sns.regplot(x=x, y=y, data=all_df)\n",
    "\n",
    "sns.jointplot(x=x, y=y, data=all_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df.drop(columns=[\"tree_method\", \"gpu\"])\n",
    "# fill freeze nan embed nans with 0\n",
    "all_df.loc[all_df[\"freeze_nan_embed\"] == \"NaN\", \"freeze_nan_embed\"] = 0\n",
    "all_df = all_df[all_df[\"freeze_nan_embed\"] != 1].drop(columns=[\"freeze_nan_embed\"])\n",
    "# fill norm nan embed nans with 0\n",
    "all_df.loc[all_df[\"norm_nan_embed\"] == \"NaN\", \"norm_nan_embed\"] = 0\n",
    "all_df = all_df[all_df[\"norm_nan_embed\"] != 0].drop(columns=[\"norm_nan_embed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = all_df[all_df[\"max_epochs\"] == 30]\n",
    "final_df = final_df[final_df[\"opt_steps\"] == 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"tunings/final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_comp = final_df[[\"model_type\", \"db_name\", \"val_score_mean\", \"test_score_mean\",\n",
    "                       \"nan_embed_size\",\n",
    "                      \"pretrained\", \"hidden_size\", \"fill_type\", \"flat_block_size\", \"val_score_std\", \n",
    "                      \"test_score_std\", \"gpt_name\", \"max_len\", \"block_size\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cfgs[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_embeds = all_df.dropna(subset=[\"nan_embed_size\"])\n",
    "nan_embeds = nan_embeds[nan_embeds[\"model_type\"] ==\"mlp\"]\n",
    "nan_embeds = nan_embeds[nan_embeds[\"fill_type\"] == \"none\"]\n",
    "nan_embeds = nan_embeds[nan_embeds[\"norm_nan_embed\"] == 1]\n",
    "\n",
    "reduced = nan_embeds[[\"db_name\", \"fill_type\", \"nan_embed_size\",\n",
    "                     \"norm_nan_embed\", \"freeze_nan_embed\", \"val_score_mean\"]]\n",
    "reduced.sort_values(by=[\"val_score_mean\"], ascending=False, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"UKE\"\n",
    "fill_type = \"ffill\"# \"none\", \"median\"\n",
    "param = \"param_count\"   # hidden_size, num_transformer_blocks, param_count\n",
    "#\"num_transformer_blocks\"\n",
    "\n",
    "\n",
    "val_name = \"cross_val_score_tune\"\n",
    "test_name = \"test_score\"\n",
    "\n",
    "plot_df = all_df[all_df[\"db_name\"] == db].sort_values(\"model_type\").drop(columns=[\"db_name\"])\n",
    "\n",
    "first_models = plot_df[plot_df[\"fill_type\"] == fill_type]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.figure(figsize=(8,5)).gca()\n",
    "\n",
    "\n",
    "\n",
    "# rnn best hidden size\n",
    "rnn_df = first_models[first_models[\"model_type\"] == \"rnn\"].sort_values(\"hidden_size\").copy()\n",
    "rnn_df[\"param_count\"] = rnn_df[\"hidden_size\"] * rnn_df[\"rnn_layers\"]\n",
    "rnn_df = rnn_df.groupby(groupby).mean().reset_index()\n",
    "ax1 = rnn_df.plot(x=param, y=val_name, kind=\"line\", label=\"rnn val\", style=\"-X\", ax=ax)\n",
    "ax = rnn_df.plot(x=param, y=test_name, kind=\"line\", label=\"rnn test\", style=\"-X\", ax=ax)\n",
    "\n",
    "transformer_df = first_models[first_models[\"model_type\"] == \"transformer\"].sort_values(\"hidden_size\").copy()\n",
    "transformer_df[\"param_count\"] = transformer_df[\"hidden_size\"] * transformer_df[\"num_transformer_blocks\"]\n",
    "transformer_df = transformer_df.groupby(groupby).mean().reset_index()\n",
    "ax1 = transformer_df.plot(x=param, y=val_name, kind=\"line\", label=\"trans val\", style=\"-X\", ax=ax)\n",
    "ax = transformer_df.plot(x=param, y=test_name, kind=\"line\", label=\"trans test\", style=\"-X\", ax=ax)\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(param)\n",
    "plt.title(f\"Score changes in dependence of hidden size for {db}\")\n",
    "# x-axis in log2 scale\n",
    "#plt.xscale('log', base=2)\n",
    "# put legend at top\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2),\n",
    "            fancybox=True, shadow=True, ncol=5)\n",
    "#rnn_df[[\"hidden_size\", \"val_score_mean\", \"val_score_std\", \"test_score_mean\", \"test_score_std\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_df.sort_values(\"cross_val_score_tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[all_df[\"db_name\"] == \"UKE\"].sort_values(\"cross_val_score_tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[all_df[\"db_name\"] == \"UKE\"].sort_values(\"cross_val_score_tune\").iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"UKE\"\n",
    "fill_type = \"median\"  # \"none\", \"median\"\n",
    "\n",
    "plot_df = score_comp[score_comp[\"db_name\"] == db].sort_values(\"model_type\").drop(columns=[\"db_name\"])\n",
    "\n",
    "first_models = plot_df[plot_df[\"fill_type\"] == fill_type]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.figure(figsize=(8,5)).gca()\n",
    "# rnn best hidden size\n",
    "rnn_df = first_models[first_models[\"model_type\"] == \"rnn\"].sort_values(\"hidden_size\").copy()\n",
    "ax1 = rnn_df.plot(x=\"hidden_size\", y=\"val_score_mean\", kind=\"line\", label=\"val\", style=\"-X\", ax=ax)\n",
    "ax = rnn_df.plot(x=\"hidden_size\", y=\"test_score_mean\", kind=\"line\", label=\"test\", style=\"-X\", ax=ax)\n",
    "\n",
    "mlp_df = first_models[first_models[\"model_type\"] == \"mlp\"].sort_values(\"hidden_size\").copy()\n",
    "mlp_df.plot(x=\"hidden_size\", y=\"val_score_mean\", kind=\"line\", label=\"val\", style=\"-o\", ax=ax)\n",
    "mlp_df.plot(x=\"hidden_size\", y=\"test_score_mean\", kind=\"line\", label=\"test\", style=\"-o\", ax=ax)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"R2 score\")\n",
    "plt.xlabel(\"Hidden Size\")\n",
    "plt.title(f\"Score changes in dependence of hidden size for {db}\")\n",
    "# x-axis in log2 scale\n",
    "#plt.xscale('log', base=2)\n",
    "# put legend at top\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2),\n",
    "            fancybox=True, shadow=True, ncol=5)\n",
    "#rnn_df[[\"hidden_size\", \"val_score_mean\", \"val_score_std\", \"test_score_mean\", \"test_score_std\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    # drop columsn that only have one unique value\n",
    "    df = df.drop(columns=df.nunique()[df.nunique()<=1].index)\n",
    "    # for each column name that appears twice, only keep one\n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate block size performance\n",
    "rnn_models = all_df[all_df[\"model_type\"] == \"rnn\"].sort_values(\"block_size\")\n",
    "rnn_models = rnn_models[rnn_models[\"db_name\"] == \"MIMIC\"]\n",
    "rnn_models = rnn_models[rnn_models[\"fill_type\"] == \"none\"]\n",
    "\n",
    "rnn_models = clean_df(rnn_models)\n",
    "best = rnn_models.groupby(\"block_size\").apply(lambda x: x.nlargest(1, \"test_score_mean\")).reset_index(drop=True)\n",
    "print(best)\n",
    "print()\n",
    "\n",
    "mlp_models = all_df[all_df[\"model_type\"] == \"mlp\"].sort_values(\"block_size\")\n",
    "mlp_models = mlp_models[mlp_models[\"db_name\"] == \"UKE\"]\n",
    "#rnn_models = rnn_models[rnn_models[\"fill_type\"] == \"none\"]\n",
    "# sort out rows where, if the fill type is none and the nan_embed_size different from 512\n",
    "mlp_models = mlp_models[(mlp_models[\"fill_type\"] == \"median\") | (mlp_models[\"nan_embed_size\"] == 512)]\n",
    "mlp_models = clean_df(mlp_models)\n",
    "best = mlp_models.groupby(\"block_size\").apply(lambda x: x.nlargest(1, \"test_score_mean\")).reset_index(drop=True)\n",
    "print(best)\n",
    "mlp_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.sort_values(\"block_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.sort_values(\"block_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot gpt sizes\n",
    "gpt_models = all_df[all_df[\"model_type\"] == \"gpt\"]\n",
    "gpt_models = gpt_models[gpt_models[\"pretrained\"] == 1]\n",
    "gpt_models = gpt_models[gpt_models[\"db_name\"] == \"UKE\"]\n",
    "gpt_models = gpt_models[gpt_models[\"fill_type\"] == \"median\"]\n",
    "# drop columsn that only have one unique value\n",
    "gpt_models = gpt_models.drop(columns=gpt_models.nunique()[gpt_models.nunique()<=1].index)\n",
    "# for each column name that appears twice, only keep one\n",
    "gpt_models = gpt_models.loc[:,~gpt_models.columns.duplicated()]\n",
    "\n",
    "\n",
    "\n",
    "gpt_sizes = gpt_models[gpt_models[\"reduction_factor\"] == 16]\n",
    "gpt_sizes = gpt_sizes[gpt_sizes[\"max_len\"] == 512].sort_values(\"reduction_factor\")\n",
    "\n",
    "ax = gpt_sizes.plot(x=\"gpt_name\", y=\"test_score_mean\", kind=\"line\", label=\"test\", style=\"-X\")\n",
    "ax = gpt_sizes.plot(x=\"gpt_name\", y=\"val_score_mean\", kind=\"line\", label=\"val\", style=\"-X\", ax=ax)\n",
    "\n",
    "# set correct x-ticks with gpt names\n",
    "plt.xticks(range(len(gpt_sizes)), gpt_sizes[\"gpt_name\"])\n",
    "plt.grid(True)\n",
    "#plt.yticks(np.arange(0.52, 0.55, 0.01))\n",
    "\n",
    "gpt_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_reduction_facs = gpt_models[gpt_models[\"gpt_name\"] == \"gpt2\"]\n",
    "gpt_reduction_facs = gpt_reduction_facs[gpt_reduction_facs[\"max_len\"] == 512].sort_values(\"reduction_factor\")\n",
    "\n",
    "ax = gpt_reduction_facs.plot(x=\"reduction_factor\", y=\"test_score_mean\", kind=\"line\", label=\"val\", style=\"-X\")\n",
    "gpt_reduction_facs.plot(x=\"reduction_factor\", y=\"val_score_mean\", kind=\"line\", label=\"test\", style=\"-X\", ax=ax)\n",
    "\n",
    "# enabled grid \n",
    "plt.grid(True)\n",
    "# make nicer\n",
    "plt.ylabel(\"R2 score\")\n",
    "plt.xlabel(\"Reduction Factor\")\n",
    "plt.title(f\"Score changes in dependence of reduction factor for {db}\")\n",
    "# only show relevant y ticks\n",
    "import numpy as np\n",
    "#plt.yticks(np.arange(0.4, 0.55, 0.01))\n",
    "\n",
    "# gpt_reduction_facs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_reduction_facs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for best model per model type\n",
    "best_models = first_models.copy().sort_values(\"model_type\")\n",
    "#best_models = best_models[best_models[(best_models[\"hidden_size\"] == 2048) & (best_models[\"hidden_size\"] == 2048)]]\n",
    "#best_models = best_models[best_models[\"gpt_name\"] == \"gpt2\"]\n",
    "\n",
    "best_models = best_models[best_models[\"block_size\"] == 128]\n",
    "\n",
    "#best_models = best_models[(best_models[\"pretrained\"] == 0) | (best_models[\"model_type\"] != \"gpt\")]\n",
    "#best_models = best_models.groupby(\"model_type\").mean().reset_index().sort_values(\"test_score_mean\", ascending=False)\n",
    "# capitalize model_type\n",
    "best_models[\"model_type\"] = best_models[\"model_type\"].str.upper()\n",
    "ax1 = best_models.plot(x=\"model_type\", y=\"val_score_mean\", kind=\"line\", label=\"val\", style=\"-X\")\n",
    "ax2 = best_models.plot(x=\"model_type\", y=\"test_score_mean\", kind=\"line\", label=\"test\", ax=ax1, style=\"-X\")\n",
    "plt.ylabel(\"R2 score\")\n",
    "plt.xlabel(\"Model Type\")\n",
    "plt.title(f\"Score changes in dependence of model type for {db}\")\n",
    "# enabled grid\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "#sns.barplot(x=\"model_type\", y=\"val_score_mean\", data=best_models)\n",
    "# plot with std as error bar\n",
    "#sns.barplot(x=\"model_type\", y=\"val_score_mean\", data=best_models, yerr=\"val_score_std\")\n",
    "best_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_type_df = plot_df[plot_df[\"hidden_size\"] == 2048].sort_values(\"model_type\")\n",
    "\n",
    "fill_type_df\n",
    "#best_models = plot_df.groupby(\"model_type\").mean().reset_index().sort_values(\"test_score_mean\", ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for model_type in fill_type_df[\"model_type\"].unique():\n",
    "    model_type_df = fill_type_df[fill_type_df[\"model_type\"] == model_type]\n",
    "    for fill_type in model_type_df[\"fill_type\"].unique():\n",
    "        sub_df = model_type_df[model_type_df[\"fill_type\"] == fill_type]\n",
    "        best = sub_df.nlargest(1, \"val_score_mean\")\n",
    "        rows.append(best)\n",
    "\n",
    "pd.concat(rows).sort_values(\"test_score_mean\", ascending=False)\n",
    "       \n",
    "#fill_type_df.groupby(\"model_type\").apply(lambda model_df: model_df.groupby(\"fill_type\").apply(lambda x: x.nlargest(1, \"val_score_mean\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_type_df.groupby(\"model_type\").mean().reset_index().sort_values(\"test_score_mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
