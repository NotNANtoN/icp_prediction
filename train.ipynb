{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169c4255-7bc4-4ff2-b2fd-0587eabe2b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from arg_utils import is_notebook, get_cfg\n",
    "cfg = get_cfg()\n",
    "# choices\n",
    "classical_models = [\"linear\", \"xgb\", \"rf\"]\n",
    "nn_models = [\"mlp\", \"rnn\", \"clip\", \"gpt\"]\n",
    "# override variables to experiment in notebook\n",
    "if is_notebook():    \n",
    "    cfg[\"target_name\"] = \"ICP_Vital\"   # ICP_Vital\" , long_icp_hypertension_2\n",
    "    cfg[\"db_name\"] = \"UKE\"  # \"UKE\", \"MIMIC\"\n",
    "    cfg[\"minutes\"] = 60\n",
    "    cfg[\"model_type\"] = \"gpt\"\n",
    "    \n",
    "    # do experiments on:  fill_type, target_nan_quantile, train_noise_std, \n",
    "    #  min_len(increase from 20 to higher), grad_clip_val (at 1 so far), weight_decay (at 0.2 so far)\n",
    "    \n",
    "    cfg[\"fill_type\"] = \"none\" # \"pat_mean\", \"median\", \"pat_ema\" \"pat_ema_mask\"\n",
    "    #cfg[\"norm_method\"] = None # z, or none\n",
    "    \n",
    "    cfg[\"bs\"] = 16 # 8 best for rnn, 32 for GPT\n",
    "    length = 128\n",
    "    cfg[\"max_len\"] = length\n",
    "    cfg[\"min_len\"] = length\n",
    "\n",
    "    # classical model args\n",
    "    cfg[\"flat_block_size\"] = 1\n",
    "    # general args\n",
    "    cfg[\"max_epochs\"] = 20\n",
    "    cfg[\"use_nan_embed\"] = True\n",
    "    cfg[\"norm_nan_embed\"] = True\n",
    "    cfg[\"weight_decay\"] = 0.2\n",
    "    cfg[\"grad_clip_val\"] = 1.0\n",
    "    cfg[\"use_huber\"] = 0\n",
    "    \n",
    "    cfg[\"lr\"] = 0.001\n",
    "\n",
    "    # rnn params\n",
    "    cfg[\"hidden_size\"] = 2048\n",
    "    cfg[\"rnn_type\"] = \"gru\"\n",
    "\n",
    "    cfg[\"rnn_layers\"] = 1\n",
    "    \n",
    "    # transformer stats for gpt2\n",
    "    # 4.338-4.8GB with adapters and batch size 16 and 117 secs\n",
    "    # 6.620GB with adapters and batch size 32 and only 100 secs\n",
    "    # 11.074 GB with adapters and batch size 64 and 92 secs\n",
    "    # also 4.864GB with train_mlp_norm and 127 secss\n",
    "    # all get to r2 of around 0.48\n",
    "\n",
    "    # transformer stats for gptneo1.3\n",
    "    # bs8 2.1GB, \n",
    "\n",
    "    # rnn stats for hidden layers size 2048\n",
    "    # bs 64, 5084MB, 40 secs.\n",
    "    # bs 128, 8632MB, 38 secs.\n",
    "\n",
    "    # transformer params\n",
    "    cfg[\"mode\"] = \"adapters\"  # \"adapters\", \"train_mlp_norm\",  \"train_norm\", \"freeze\" (does not train)\n",
    "    cfg[\"gpt_name\"] = \"neo1.3\"  # gpt2, neo1.3, neo2.7\n",
    "    cfg[\"gpu\"] = 1\n",
    "    \n",
    "    cfg[\"seed\"] = 0\n",
    "    cfg[\"subsample\"] = 0.9\n",
    "    cfg[\"colsample_bytree\"] = 0.9\n",
    "    \n",
    "    \n",
    "# overrides and calculated default vals\n",
    "if cfg[\"lr\"] is None:\n",
    "    model_type = cfg[\"model_type\"]\n",
    "    if model_type == \"clip\":\n",
    "        cfg[\"lr\"] = 0.001\n",
    "    elif model_type == \"gpt\":\n",
    "        # bs 8 and gpt2 take 9.8GB with max seq len of 512\n",
    "        # bs 16 with max seq len of 256\n",
    "        # bs 32 with max seq len 128 only 7.4GB, good performance and fast - 6.9 if mlp_norm\n",
    "        # bs 64 with len 128 and mlp_norm = 10.9GB. 9.4GB for freeze\n",
    "        cfg[\"lr\"] = 0.00005\n",
    "    else:\n",
    "        cfg[\"lr\"] = 0.0001  # 0.01 works kind of for nan_embed\n",
    "\n",
    "if cfg[\"fill_type\"] == \"none\":\n",
    "    cfg[\"use_nan_embed\"] = 1\n",
    "        \n",
    "#cfg[\"val_check_interval\"] = int(cfg[\"val_check_interval\"] * (32 / cfg[\"batch_size\"]))\n",
    "    \n",
    "import pytorch_lightning as pl\n",
    "pl.utilities.seed.seed_everything(seed=cfg[\"seed\"], workers=False)\n",
    "locals().update(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4cf0b4-8d17-4dba-9a73-26de0eb9bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(cfg[\"gpu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f4c780-09b5-4421-a595-a1e1d47cb861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "        \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_utils import SeqDataModule\n",
    "\n",
    "import logging\n",
    "import pytorch_lightning\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "pytorch_lightning.utilities.distributed.log.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10ef0e1-e32d-46c5-b1ba-64c282c82091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "path = f'data/DB_{cfg[\"db_name\"]}_{cfg[\"minutes\"]}_final_df.pkl'\n",
    "df = pd.read_pickle(path)\n",
    "\n",
    "if \"Bili_BGA\" in df:\n",
    "    print(\"Drop Bili\")\n",
    "    df = df.drop(columns=[\"Bili_BGA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77787e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1696639/1720448617.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  filled_df = df.copy().fillna(df.median())\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "85da11a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4682e50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "53734e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5a58bc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 86.,  17., 186.,   4.,  35.,  59.,  12.,  77., 143.,  42.,   1.,\n",
       "         18.,   1.,  18., 178.,  38.,   5., 125.,  14.,  62.,  72.,  73.,\n",
       "         21.,  17.,  50.,  92., 110.,  81.,  15.,  76.,  89.,  27.,   6.,\n",
       "          2.,   1.,   8.,   8., 135., 177.,   4.,  13.,   4.,  49.,  42.,\n",
       "        175.,  13.,  76.,  20.,  34.,  52.,  62.,  46.,  21., 138.,   1.,\n",
       "          2.,  89.,   2.,   1.,  23.,   2.,  53., 126.,   1., 155.,   1.,\n",
       "         19.,  32.,  67.,   1.,  27.,   8., 154.,   3.,  27.,   3.,   9.,\n",
       "         97., 140.,  50., 131.,   2.,  81.,   1.,  28.,  59.,   1.,  73.,\n",
       "          1.,   9.,  40.,  38.,  35.,  39.,  17.,  22.,   5.,  88.,  13.,\n",
       "          2.]),\n",
       " array([ 0.  ,  0.99,  1.98,  2.97,  3.96,  4.95,  5.94,  6.93,  7.92,\n",
       "         8.91,  9.9 , 10.89, 11.88, 12.87, 13.86, 14.85, 15.84, 16.83,\n",
       "        17.82, 18.81, 19.8 , 20.79, 21.78, 22.77, 23.76, 24.75, 25.74,\n",
       "        26.73, 27.72, 28.71, 29.7 , 30.69, 31.68, 32.67, 33.66, 34.65,\n",
       "        35.64, 36.63, 37.62, 38.61, 39.6 , 40.59, 41.58, 42.57, 43.56,\n",
       "        44.55, 45.54, 46.53, 47.52, 48.51, 49.5 , 50.49, 51.48, 52.47,\n",
       "        53.46, 54.45, 55.44, 56.43, 57.42, 58.41, 59.4 , 60.39, 61.38,\n",
       "        62.37, 63.36, 64.35, 65.34, 66.33, 67.32, 68.31, 69.3 , 70.29,\n",
       "        71.28, 72.27, 73.26, 74.25, 75.24, 76.23, 77.22, 78.21, 79.2 ,\n",
       "        80.19, 81.18, 82.17, 83.16, 84.15, 85.14, 86.13, 87.12, 88.11,\n",
       "        89.1 , 90.09, 91.08, 92.07, 93.06, 94.05, 95.04, 96.03, 97.02,\n",
       "        98.01, 99.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = plt.hist(labels, bins=100)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f4d86067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [03:26<00:00, 22.93s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_elbow(df):\n",
    "    distortions = []\n",
    "    K = range(50, 500, 50)\n",
    "    for k in tqdm(K):\n",
    "        kmeanModel = KMeans(n_clusters=k).fit(df)\n",
    "        kmeanModel.fit(df)\n",
    "        distortions.append(sum(np.min(cdist(df, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k')\n",
    "    plt.show()\n",
    "\n",
    "find_elbow(cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0508a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027d55bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291704\n",
      "count    243510.000000\n",
      "mean         13.136463\n",
      "std          10.522283\n",
      "min         -23.000000\n",
      "25%           8.000000\n",
      "50%          12.000000\n",
      "75%          16.000000\n",
      "max         298.000000\n",
      "Name: ICP_Vital, dtype: float64\n",
      "0.49113061427296983\n",
      "0.1652154238543181\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(df[\"ICP_Vital\"].describe())\n",
    "print(df.isna().mean().mean())\n",
    "print(df[\"ICP_Vital\"].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95db715a-8709-4a85-bbe1-6a2a6de3287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datamodule with dataloaders\n",
    "from data_utils import SeqDataModule\n",
    "\n",
    "dm = SeqDataModule(df, db_name,\n",
    "                   target_name=cfg[\"target_name\"],\n",
    "                   random_starts=cfg[\"random_starts\"], \n",
    "                   min_len=cfg[\"min_len\"], \n",
    "                   max_len=cfg[\"max_len\"],\n",
    "                   train_noise_std=cfg[\"train_noise_std\"], \n",
    "                   batch_size=cfg[\"bs\"], \n",
    "                   fill_type=cfg[\"fill_type\"], \n",
    "                   flat_block_size=cfg[\"flat_block_size\"],\n",
    "                   target_nan_quantile=cfg[\"target_nan_quantile\"],\n",
    "                   block_size=cfg[\"block_size\"],\n",
    "                   subsample_frac=cfg[\"subsample_frac\"],\n",
    "                   )\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc46f5e-6dc0-47e3-898f-b3a20dbd425f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 88]) tensor(nan) tensor(nan)\n",
      "torch.Size([1, 16, 1])\n",
      "tensor(-7.2778)\n",
      "tensor([16])\n",
      "tensor(16.) tensor(16)\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "out = next(iter(dm.train_dataloader()))\n",
    "inputs, targets, lens = next(iter(dm.train_dataloader()))\n",
    "#print(dm.feature_names)\n",
    "print(inputs.shape, inputs.min(), inputs.max())\n",
    "print(targets.shape)\n",
    "print(targets[~torch.isnan(targets)].mean())\n",
    "print(lens)\n",
    "print(lens.float().mean(), lens.max())\n",
    "# TODO: lens max and mean deviate strongly from each other - smaller ranges, increase min_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f5b4ee0-f3cb-4547-86bd-5c16750fb475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters:  810815489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mantonius\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anton/ICU/icp_prediction/wandb/run-20220323_190316-3udyy25z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/antonius/ICP_Vital/runs/3udyy25z\" target=\"_blank\">fresh-fire-4754</a></strong> to <a href=\"https://wandb.ai/antonius/ICP_Vital\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "/home/anton/.local/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a487d56373bb4c46ae36396f76e63847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e21807ff4e43cf81745c2d586345b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.78 GiB total capacity; 10.22 GiB already allocated; 20.19 MiB free; 10.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1593043/475849911.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ICU/icp_prediction/train_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_type, data_modules, cfg, verbose)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;31m# store model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ICU/icp_prediction/train_utils.py\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(model_type, data_module, cfg, verbose)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;31m# do actual training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m#trainer.logger.unwatch(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    738\u001b[0m             )\n\u001b[1;32m    739\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         self._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    741\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \"\"\"\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_active_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[override]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         result = self._run_optimization(\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# gradient update with accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         lightning_module.optimizer_step(\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \"\"\"\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofiler_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[1;32m    338\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/native_amp.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;34mf\"Native AMP and the LBFGS optimizer are not compatible (optimizer {optimizer_idx}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             )\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mclosure_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;31m# `unscale` after the closure is executed but before the `on_before_optimizer_step` hook.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step_and_backward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ICU/icp_prediction/model.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ICU/icp_prediction/model.py\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(self, inputs, targets, hiddens, lens)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ICU/icp_prediction/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ICU/icp_prediction/model.py\u001b[0m in \u001b[0;36mmake_preds\u001b[0;34m(self, x, lens)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# x = [BS, seq_len, feats]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    622\u001b[0m                 )\n\u001b[1;32m    623\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    625\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.78 GiB total capacity; 10.22 GiB already allocated; 20.19 MiB free; 10.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# train model on datamodule\n",
    "from train_utils import train_model\n",
    "\n",
    "models, trainers = train_model(cfg[\"model_type\"], [dm], cfg, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8cd7d-4d53-464e-a8c8-33093791304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3b97d-e248-40d2-a95b-cfa6e024c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_utils import seq_pad_collate\n",
    "\n",
    "#def traini(bs, num_workers, pin):\n",
    "#    dl = torch.utils.data.DataLoader(data_modules[0].train_ds, batch_size=bs, num_workers=num_workers, pin_memory=pin, collate_fn=seq_pad_collate)\n",
    "#    model = create_model(model_type, data_modules[0], cfg)\n",
    "#    trainer = create_trainer(cfg, verbose=False)\n",
    "#    trainer.fit(model, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61581bf3-7516-4515-8e13-5cada07fac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils import get_all_dfs, print_all_metrics\n",
    "\n",
    "# eval\n",
    "dl_type = \"test\"\n",
    "#dl = external_dls[\"MIMIC\"] # external_dls - MIMIC, eICU, UKE\n",
    "calc_new_norm_stats = False\n",
    "dl = None\n",
    "\n",
    "pred_df = get_all_dfs(models, trainers, cfg[\"model_type\"], dm.regression, dl_type=dl_type, dl=dl, calc_new_norm_stats=calc_new_norm_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad43d8-1c53-4891-90c6-e1cc1e3d4a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4dfef-de2f-410b-a26f-3df8036c5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertension_acc(targets, preds):\n",
    "    thresh = 22\n",
    "    hyper_targets = targets > thresh\n",
    "    hyper_preds = preds > thresh\n",
    "    hyper_acc = (hyper_targets == hyper_preds).astype(float).mean()\n",
    "    return hyper_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25455a66-a68b-45d4-af3b-aeeef8295957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import eval_utils\n",
    "importlib.reload(eval_utils)\n",
    "from eval_utils import get_all_dfs\n",
    "\n",
    "if dm.regression:\n",
    "    #from eval_utils import hypertension_acc\n",
    "    pred_targets = pred_df[\"targets\"].dropna()\n",
    "    preds = pred_df[\"preds\"][~pred_df[\"targets\"].isna()]\n",
    "    print(\"Accuracy for hypertension: \", hypertension_acc(pred_targets, np.zeros((len(pred_targets,)))))\n",
    "    print(\"Accuracy for hypertension: \", hypertension_acc(pred_targets, preds))\n",
    "    print_all_metrics(pred_df)\n",
    "else:\n",
    "    # general metrics\n",
    "    non_na_pred_df = pred_df.dropna(subset=[\"targets\"])\n",
    "    binary_preds = non_na_pred_df[\"preds\"] > 0.5\n",
    "    targets = non_na_pred_df[\"targets\"] \n",
    "    preds = non_na_pred_df[\"preds\"]\n",
    "    \n",
    "    \n",
    "    auc = sklearn.metrics.roc_auc_score(targets, preds)\n",
    "    tpr, fpr, threshs = sklearn.metrics.roc_curve(targets, preds)\n",
    "    plt.plot(tpr, fpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\")\n",
    "    plt.show()\n",
    "    \n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(targets, preds)\n",
    "    plt.plot(precision, recall)\n",
    "    plt.xlabel(\"precision\")\n",
    "    plt.ylabel(\"recall\")\n",
    "    plt.show()\n",
    "    \n",
    "    matrix = sklearn.metrics.confusion_matrix(non_na_pred_df[\"targets\"],\n",
    "                                              binary_preds)\n",
    "    cm_display = sklearn.metrics.ConfusionMatrixDisplay(matrix).plot()\n",
    "    print(matrix)\n",
    "    print(\"AUC: \", auc)\n",
    "    print(\"Mean pred: \", pred_df[\"preds\"].mean())\n",
    "    \n",
    "    #train_pred_df = get_all_dfs(models, trainers, cfg[\"model_type\"], dm.regression, dl_type=\"train\", dl=None, calc_new_norm_stats=False)\n",
    "    #non_na_df = train_pred_df.dropna(subset=[\"targets\"])\n",
    "    #tpr, fpr, threshs = sklearn.metrics.roc_curve(non_na_df[\"targets\"], non_na_df[\"preds\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9175a80-c741-4c32-8cb3-a7dfebf8c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from data_utils import SeqDataModule\n",
    "from eval_utils import get_all_dfs\n",
    "\n",
    "# external performance\n",
    "# read external df\n",
    "if db_name == \"UKE\":\n",
    "    ext_db_name = \"MIMIC\"\n",
    "else:\n",
    "    ext_db_name = \"UKE\"\n",
    "path = f\"data/DB_{ext_db_name}_{minutes}_final_df.pkl\"\n",
    "ext_df = pd.read_pickle(path)\n",
    "if \"Bili_BGA\" in ext_df:\n",
    "    print(\"Drop Bili\")\n",
    "    ext_df = ext_df.drop(columns=[\"Bili_BGA\"])    \n",
    "# create external dataloaders\n",
    "dl_type = \"test\"\n",
    "calc_new_norm_stats = False\n",
    "ext_dm = SeqDataModule(ext_df, \n",
    "                            ext_db_name,\n",
    "                            target_name=cfg[\"target_name\"],\n",
    "                            random_starts=cfg[\"random_starts\"], \n",
    "                            min_len=cfg[\"min_len\"], \n",
    "                            train_noise_std=cfg[\"train_noise_std\"], \n",
    "                            batch_size=cfg[\"bs\"], \n",
    "                            fill_type=cfg[\"fill_type\"], \n",
    "                            flat_block_size=cfg[\"flat_block_size\"],\n",
    "                            target_nan_quantile=cfg[\"target_nan_quantile\"],) \n",
    "dl = ext_dm.val_dataloader()\n",
    "\n",
    "pred_df_ext = get_all_dfs(models, trainers, cfg[\"model_type\"], ext_dm.regression, \n",
    "                          dl_type=dl_type, dl=dl, calc_new_norm_stats=calc_new_norm_stats)\n",
    "\n",
    "print_all_metrics(pred_df_ext)\n",
    "\"\"\"\n",
    "pass\n",
    "# external dataset validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67621cc-d191-4566-bd7c-99272804fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dm.regression:\n",
    "    df_nona = pred_df.dropna(subset=[\"targets\"])\n",
    "    mean_pred_error = (df_nona[\"targets\"] - dm.mean_train_target).dropna() ** 2\n",
    "    ax = sns.jointplot(x=\"targets\", y=\"error\", data=df_nona, kind=\"hist\", bins=100) # data=by_pat\n",
    "    ax.ax_joint.scatter(df_nona[\"targets\"], (np.ones(len(df_nona[\"targets\"])) * dm.mean_train_target - df_nona[\"targets\"]) ** 2, s=2, color=\"orange\")\n",
    "    plt.xlim(-20, 105)\n",
    "    plt.ylim(0, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a2abb-6845-4293-9415-5dd5adc9f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dm.regression:\n",
    "    ylim = (df_nona[\"preds\"].min() - 5, df_nona[\"preds\"].max() + 5)\n",
    "    ax = sns.jointplot(data=df_nona, x=\"targets\", y=\"preds\", kind=\"reg\", ylim=ylim) # data=bypat\n",
    "    min_val = df_nona[\"targets\"].min()\n",
    "    max_val = df_nona[\"targets\"].max()\n",
    "    ax.ax_joint.plot([min_val, max_val], [min_val, max_val], linewidth=2, color=\"black\", label=\"Ideal model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86cce3-b09a-417c-9b1e-df32bd376f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dm.regression:\n",
    "    ax = sns.jointplot(data=df_nona, x=\"targets\", y=\"preds\", kind=\"hist\", bins=100, ylim=ylim)\n",
    "    print(min_val, max_val)\n",
    "    # draw line of perfect correlation\n",
    "    ax.ax_joint.plot([min_val, max_val], [min_val, max_val], linewidth=2)#, color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524c7bd-4af2-40be-8e94-36653d2a39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dm.regression:\n",
    "    #pats = df.groupby(\"ids\").filter(lambda x: x['preds'].max() > 16)\n",
    "    pats = pred_df.groupby(\"ids\").filter(lambda x: x['targets'].mean() > 30)\n",
    "    #pats = df.groupby(\"ids\").filter(lambda x: np.sqrt(x['error'].mean()) > 30)\n",
    "else:\n",
    "    pats = pred_df.groupby(\"ids\").filter(lambda x: x['preds'].mean() > 0.2)\n",
    "pats = pats.groupby(\"ids\").filter(lambda x: len(x[\"targets\"].dropna()) > 2)\n",
    "\n",
    "ids = pats[\"ids\"].unique()\n",
    "print(ids)\n",
    "print(len(ids), \"patients\")\n",
    "#pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539f8f6-a2e1-44a2-9f8f-b59374bc0365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils import mape\n",
    "import sklearn\n",
    "\n",
    "pat_id = 5\n",
    "model_id = 0\n",
    "\n",
    "pat = pred_df[pred_df[\"ids\"] == ids[pat_id]]\n",
    "pat = pat[pat[\"model_id\"] == model_id]\n",
    "plt.scatter(pat[\"step\"], pat[\"targets\"], label=\"targets\", color=\"orange\")\n",
    "plt.plot(pat[\"step\"], pat[\"preds\"], label=\"preds\")\n",
    "mean = dm.mean_train_target\n",
    "print(\"mean average target: \", mean)\n",
    "print(\"mean pat targets: \", pat[\"targets\"].mean())\n",
    "plt.plot([0, max(pat[\"step\"])], [mean, mean], linewidth=1, color=\"black\", label=\"mean train target\", linestyle=\"--\")\n",
    "#plt.legend(location=\"out\")\n",
    "plt.legend(title='Legend', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xlim(0, max(pat[\"step\"]) + 1)\n",
    "pat_nona = pat[~pat[\"targets\"].isna()]\n",
    "print(\"RMSE of model:\", round(np.sqrt(sklearn.metrics.mean_squared_error(pat_nona[\"targets\"], pat_nona[\"preds\"])), 2))\n",
    "print(\"RMSE of mean:\", round(np.sqrt(sklearn.metrics.mean_squared_error(pat_nona[\"targets\"], [mean] * len(pat_nona))), 2))\n",
    "print(\"MAPE of model: \", round(mape(pat_nona[\"targets\"], pat_nona[\"preds\"]), 2))\n",
    "print(\"MAPE of mean: \", round(mape(pat_nona[\"targets\"], [mean] * len(pat_nona)), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ffe0a-eee1-4cce-b1be-49438b10b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(dm.train_ds.targets)):\n",
    "#    d = dm.train_ds.targets[i]\n",
    "#    d = d[~torch.isnan(d)].max()\n",
    "#    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1a9b2-fc96-44fc-ae9a-b36d3ba15adb",
   "metadata": {},
   "source": [
    "## Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92051d2-6364-4ea0-b135-fb1a36742866",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = dm.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1894b92-2011-446f-8a54-c61e63545836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "def classical_saliency(models, trainers, model_type, use_shap=False, verbose=True):\n",
    "    # plot feature importance\n",
    "    all_importances = []\n",
    "    all_inputs = []\n",
    "    \n",
    "    for model, data_module in zip(models, trainers):\n",
    "        if use_shap:\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            inputs = data_module.test_dataloader().dataset.flat_inputs\n",
    "            importances = explainer.shap_values(inputs)\n",
    "            all_inputs.append(inputs)\n",
    "        else:\n",
    "            if hasattr(model, \"feature_importances_\"):\n",
    "                importances = model.feature_importances_\n",
    "            elif hasattr(model, \"coef_\"):\n",
    "                importances = model.coef_\n",
    "        all_importances.append(importances)\n",
    "    if not use_shap and verbose and hasattr(model, \"importance_type\"):\n",
    "        print(\"Importance type: \", model.importance_type)\n",
    "    print(\"importances shape \", importances.shape)\n",
    "    mean_importances = np.mean(np.stack(all_importances), axis=0)\n",
    "    # save\n",
    "    path = f\"outputs/{model_type}\"\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    \n",
    "    block_size = cfg[\"flat_block_size\"]\n",
    "\n",
    "    \n",
    "    if use_shap:\n",
    "        #shap.summary_plot(mean_importances, X_test, plot_type=\"bar\")\n",
    "        #shap.summary_plot(mean_importances, X_test)\n",
    "        #shap.summary_plot(mean_importances)\n",
    "        #input_df = pd.DataFrame(inputs, columns=feature_names)\n",
    "        #shap_df = pd.DataFrame(mean_importances, columns=feature_names)\n",
    "        \n",
    "        #vals= np.abs(shap_values).mean(0)\n",
    "        #feature_importance = pd.DataFrame(list(zip(feature_names, vals)),columns=['col_name','feature_importance_vals'])\n",
    "        #feature_importance = feature_importance.sort_values(by=['feature_importance_vals'], ascending=False)\n",
    "        #feature_importance.head()\n",
    "                \n",
    "        if verbose:\n",
    "            block_feat_names = []\n",
    "            for i in range(block_size):\n",
    "                block_feat_names.extend([f + f\"_{i}\" for f in feature_names])\n",
    "            \n",
    "            shap.summary_plot(mean_importances, features=inputs, \n",
    "                              feature_names=block_feat_names)\n",
    "            plt.savefig(path + \"_shap_importances.jpg\", bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "        # average over block_size/time_steps\n",
    "       \n",
    "        if block_size > 1:\n",
    "            print(mean_importances.shape)\n",
    "            mean_importances = mean_importances.reshape(-1, len(feature_names), block_size).mean(axis=-1)\n",
    "            print(mean_importances.shape)\n",
    "\n",
    "\n",
    "        imp_per_feat = np.abs(mean_importances).mean(axis=0)\n",
    "        feat_df = pd.Series(imp_per_feat, index=feature_names)\n",
    "    else:\n",
    "        # make plot\n",
    "        if block_size > 1:\n",
    "            print(mean_importances.shape)\n",
    "            mean_importances = np.abs(mean_importances.reshape(len(feature_names), block_size)).mean(axis=-1)\n",
    "            print(mean_importances.shape)\n",
    "        \n",
    "        \n",
    "        feat_df = pd.Series(mean_importances, index=feature_names)\n",
    "        if verbose:\n",
    "            p = feat_df.sort_values().plot.barh(figsize=(4, 25))\n",
    "            p.figure.savefig(path + \"_importances.jpg\", bbox_inches='tight')\n",
    "        \n",
    "    feat_df = feat_df.sort_values(ascending=True)\n",
    "    return feat_df\n",
    "\n",
    "\n",
    "def reduce_feat_df(feat_df, threshold=0.2):\n",
    "    # remove everything above 90% explained importance\n",
    "    print(len(feat_df))\n",
    "    feat_df = feat_df / feat_df.sum()\n",
    "    reduced_df = feat_df[feat_df.cumsum() > threshold]\n",
    "    print(len(reduced_df))\n",
    "    print(reduced_df.sum())\n",
    "    print(reduced_df)\n",
    "    reduced_feats = list(reduced_df.index)\n",
    "    return reduced_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d1290-dae4-4e8b-a146-7432f517b1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5cbf78-fa03-46d4-8b25-9a79cebc967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_df = classical_saliency(models, trainers, cfg[\"model_type\"], \n",
    "                            use_shap=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abe9d8-fc90-411e-9b49-8dc55473a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_df_normed = sal_df / sal_df.sum()\n",
    "sal_df_normed.sort_values().iloc[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8796f1-300c-4cea-870b-4701ccbb733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feats = reduce_feat_df(sal_df, threshold=0.15)\n",
    "reduced_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ac8e3-2afe-4cef-94d0-402bf97a63e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796a280-7dea-4b4b-8f0c-652c7bdfe8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from saliency import get_sal_list\n",
    "\n",
    "sal_list = []\n",
    "\n",
    "for model in models:\n",
    "    model_saliency = get_sal_list(model, 0, perc=1.0, agg=True, ds=model.data_module.test_dataloader().dataset, ig=False)\n",
    "    sal_list += model_saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6abc5e-34bc-4e2b-bd36-ee849884ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d8408-dd00-42c3-9276-d87371240738",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_overall = [s.mean(0) for s in sal_list]\n",
    "feat_saliency = np.sum(mean_overall, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146b404-9bba-4c5e-9aa4-4d849d39b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = dm.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2826f-9f8a-4806-813f-66a046c23cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sal_df = pd.DataFrame({\"sal\": feat_saliency}, index = feature_names).sort_values(\"sal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d5fa3-8796-462c-bcb5-b8200f84aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "p = feat_sal_df.plot.barh(figsize = (6, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c47c1-4cbc-4575-83c7-017102979484",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.figure.savefig(\"importances.jpg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7d4bb-3f7f-4b41-b3a2-b4542cfd892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51139e8-1e0d-4967-8899-0c8671e7bf05",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786fb1d-890a-41a6-92bd-143ad740bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune_utils import tune#, make_tune_plots\n",
    "from data_utils import make_split\n",
    "\n",
    "import importlib\n",
    "import tune_utils\n",
    "importlib.reload(tune_utils)\n",
    "from tune_utils import tune\n",
    "\n",
    "import data_utils\n",
    "importlib.reload(data_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f60c1-a0a6-4f25-9391-3b051cd022c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pytorch_lightning\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "pytorch_lightning.utilities.distributed.log.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319e7b2-fde0-4ad7-8414-3e3a8fc2f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"dbs\"] = [\"UKE\"]\n",
    "cfg[\"k_fold\"] = 0\n",
    "cfg[\"num_splits\"] = 3\n",
    "cfg[\"flat_block_size\"] = 0\n",
    "cfg[\"n_trials\"] = 15\n",
    "cfg[\"model_type\"] = \"linear\"\n",
    "cfg[\"fill_type\"] = \"pat_mean\"\n",
    "\n",
    "cfg[\"max_steps\"] = 300\n",
    "\n",
    "opt_flat_block_size = False\n",
    "opt_augs = False\n",
    "opt_fill_type = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193a76c-b789-4845-a4e5-6eaa5eae3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storage = optuna.storages.RDBStorage(url=\"sqlite:///optuna.db\",\n",
    "#                                         engine_kwargs={\"connect_args\": {\"timeout\": 30}})\n",
    "#load_study = optuna.load_study(\"09_17_02_39__linear_UKE_15\", storage=storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d751b3c-3c64-48df-9d5b-ff2b2b09f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dev/test split to test tuning\n",
    "dev_data, test_data, dev_idcs, test_idcs = make_split(seq_list, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5afe9b-7da5-46ec-8f67-98fd4bbce52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune\n",
    "study = tune(dev_data, test_data, args, cfg, verbose=False, n_trials=cfg[\"n_trials\"],\n",
    "             opt_flat_block_size=opt_flat_block_size, opt_augs=opt_augs, opt_fill_type=opt_fill_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59314a06-f83a-4a4b-a50b-c02700a2ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_idcs = np.load(\"test_tune/09_17_02_39__linear_UKE_15/test_idcs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b05d2-0a2b-4599-adb1-321e0bbe31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print key study parts\n",
    "name = study.study_name\n",
    "print(name)\n",
    "print(study.best_trial)\n",
    "best_params = study.best_params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc29766-69c1-4e9f-92ab-7afd708fab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune_utils import store_study_results\n",
    "store_study_results(study, \"./test_tune/\", test_idcs, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe860988-bd2b-4c89-bdce-4955b336eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_tune_plots(study, f\"outputs/tune_results/{cfg['model_type']}_{cfg['n_trials']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99161a6f-167e-4bca-9e0b-7f438fc7fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain on dev/test and check performance\n",
    "    \n",
    "models, trainers = retrain(dev_data, test_data, best_params, args, cfg, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74412f-3003-43f0-ab14-e3caefd6dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"k_fold\"] = 0\n",
    "cfg[\"num_splits\"] = 5\n",
    "models, trainers = retrain(dev_data, test_data, best_params, args, cfg, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f840e-3eb0-4558-8a45-971b7cf3aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"k_fold\"] = 5\n",
    "models, trainers = retrain(dev_data, test_data, best_params, args, cfg, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448055bd-c960-4fd5-90ae-d67cace0f033",
   "metadata": {},
   "source": [
    "## Nested k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9144be8-e705-41ef-8750-b4eb7d799c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_models = [\"xgb\", \"rf\", \"linear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af294171-1ca8-4627-b19a-76355ec58389",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"dbs\"] = [\"UKE\"]\n",
    "cfg[\"k_fold\"] = 0\n",
    "cfg[\"num_splits\"] = 3\n",
    "cfg[\"flat_block_size\"] = 1\n",
    "cfg[\"n_trials\"] = 10\n",
    "\n",
    "cfg[\"n_outer_folds\"] = 3\n",
    "\n",
    "cfg[\"model_type\"] = \"xgb\"\n",
    "cfg[\"fill_type\"] = \"pat_mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058455f-207d-4d20-adf1-10ee74dc84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import make_fold\n",
    "# outer fold\n",
    "dev_data_list, test_data_list, dev_idcs, test_idcs = make_fold(seq_list, k=cfg[\"n_outer_folds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae3167-5c9c-45f3-835b-2a01cb2673b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune and store studies\n",
    "studies = []\n",
    "for dev_data, test_data in zip(dev_data_list, test_data_list):\n",
    "    study = tune(dev_data, test_data, args, cfg, verbose=False, n_trials=cfg[\"n_trials\"])\n",
    "    # print key tune details\n",
    "    name = study.study_name\n",
    "    print(name)\n",
    "    print(study.best_trial)\n",
    "    best_params = study.best_params\n",
    "    print(best_params)\n",
    "    studies.append(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d300d-5dfd-47c4-9e7d-3cbfc2df82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "pred_dfs = []\n",
    "for tune_idx in tqdm(range(len(studies))):\n",
    "    models, trainers = retrain(dev_data_list[tune_idx],\n",
    "                               test_data_list[tune_idx], \n",
    "                               studies[tune_idx].best_params, \n",
    "                               args, cfg, verbose=False)\n",
    "    df = get_all_dfs(models, trainers, cfg[\"model_type\"], dl_type=\"test\")\n",
    "    #loss = print_all_metrics(df)\n",
    "    loss = df.groupby(\"model_id\").apply(lambda model_df: model_df.groupby(\"ids\").mean()).mean()[\"error\"]\n",
    "    metrics.append(loss)\n",
    "    pred_dfs.append(df)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0db61e-746e-4f46-901d-b0694fbd631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(metrics))\n",
    "print(np.std(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafaf53-4484-4f4a-9e53-28744b3327c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for study in studies:\n",
    "    print(study.best_value)\n",
    "    print(study.best_params)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58bb9ad-48e6-4594-9ce6-6857ebab6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "[study.best_value for study in studies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a480c0c7-70f3-4ea3-bae3-08b2b18dc0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60af506-40aa-40f8-9821-ac6ea982ac74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d452de-db1a-4e25-96ca-aac754aa9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(metrics))\n",
    "print(np.std(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79480ff-c1be-4238-9ae2-d5cf3228bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for study in studies:\n",
    "    print(study.best_value)\n",
    "    print(study.best_params)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236125c-e54d-4d13-9fd8-3d763e1b2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6858ffd-9c76-4cb5-bd0c-d8d3298a5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43776d9b-3083-45f3-a58b-5b118f6680e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "studies[0].best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cdda2-cc87-483e-af97-b508505f3d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
