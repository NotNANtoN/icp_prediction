{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'Export_csv/b71e463_UKE_yeojo_60_median_3_mit_Labor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'Export_csv/d860af6_UKE_yeojo_60_median_3_ohne_Labor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_df = pd.read_csv(root + '/saliency_validset_LONG_.csv', index_col=0)\n",
    "sg_df = sg_df.append(pd.read_csv(root + '/saliency_testset_LONG_.csv', index_col=0))\n",
    "sg_df = sg_df.append(pd.read_csv(root + '/MIMIC/saliency_baseset_LONG_.csv', index_col=0))\n",
    "sg_df = sg_df.append(pd.read_csv(root + '/eICU/saliency_baseset_LONG_.csv', index_col=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_df = pd.read_csv(root + '/saliency_validset_LONG_ig.csv', index_col=0)\n",
    "ig_df = ig_df.append(pd.read_csv(root + '/saliency_testset_LONG_ig.csv', index_col=0))\n",
    "ig_df = ig_df.append(pd.read_csv(root + '/MIMIC/saliency_baseset_LONG_ig.csv', index_col=0))\n",
    "ig_df = ig_df.append(pd.read_csv(root + '/eICU/saliency_baseset_LONG_ig.csv', index_col=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv(root + '/validset_targets.csv', index_col=0)\n",
    "targets = targets.append(pd.read_csv(root + '/validset_targets.csv', index_col=0))\n",
    "targets = targets.append(pd.read_csv(root + '/MIMIC/baseset_targets.csv', index_col=0))\n",
    "targets = targets.append(pd.read_csv(root + '/eICU/baseset_targets.csv', index_col=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_df = ig_df.dropna()\n",
    "sg_df = sg_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample points\n",
    "import numpy as np\n",
    "idcs = np.random.choice(range(len(ig_df)), 100000)\n",
    "ig_df_s = ig_df.iloc[idcs]\n",
    "sg_df_s = sg_df.iloc[idcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_df = sg_df_s.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix PatID:\n",
    "ids = umap_df[\"PatID\"].astype(str)\n",
    "ids = [i[7:-2] if i.startswith(\"tensor\") else i for i in ids]\n",
    "umap_df[\"PatID\"] = pd.Series(ids).astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import umap.plot\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "umap_cols = [col for col in umap_df.columns if col != \"PatID\"]\n",
    "embeddings = reducer.fit_transform(umap_df[umap_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top ten feats\n",
    "feat_avg = umap_df[umap_cols].abs().mean().sort_values(ascending=False)\n",
    "feat_avg = feat_avg / sum(feat_avg)\n",
    "\n",
    "top_ten_feats = feat_avg.iloc[:10].index\n",
    "top_ten_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_d_reducer = umap.UMAP(n_components=10,\n",
    "                           n_neighbors=30,\n",
    "                           min_dist=0.0,)\n",
    "high_d_embeddings = high_d_reducer.fit_transform(umap_df[umap_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans_labels = kmeans.fit_predict(high_d_embeddings)\n",
    "umap.plot.points(reducer, labels=kmeans_labels, theme=\"fire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ranking(df, rank_type=\"abs\"):\n",
    "    # take mean of abs vals over all steps\n",
    "    #print(df.iloc[0])\n",
    "    if rank_type == \"abs\":\n",
    "        df = df.abs().mean()\n",
    "    elif rank_type == \"pos\":\n",
    "        df = df[df >= 0].mean()\n",
    "    elif rank_type == \"neg\":\n",
    "        df = df[df <= 0].mean()\n",
    "    #print(df.iloc[0])\n",
    "    # divide by sum of abs vals to have distr\n",
    "    df = df / df.abs().sum()\n",
    "    df = df.transpose()\n",
    "    df = df.sort_values(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_top_vals(df):\n",
    "    df = df.sort_values(ascending=True)\n",
    "    df = df / df.abs().sum()\n",
    "    df = df[df.cumsum() > 0.2]\n",
    "    df = df / df.abs().sum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ranking(df, ax, rank_type=\"abs\"):\n",
    "    if rank_type == \"abs\":\n",
    "        c = \"blue\"\n",
    "    elif rank_type == \"pos\":\n",
    "        c = \"orange\"\n",
    "    elif rank_type == \"neg\":\n",
    "        c = \"green\"\n",
    "    df.index = [\"\".join(idx.split(\"_\")[1:]) if \"_\" in idx else idx for idx in df.index]\n",
    "    df.plot.barh(ax=ax, color=c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ranking(df, title, ax=None):\n",
    "    \n",
    "    'WARNING'\n",
    "    #TODO: !!! Need to balance scale of pos and neg according to how many pos and neg cases there were - cannot just add them in one plot as below!\n",
    "    \n",
    "    # norm by pat\n",
    "    #df = df.groupby(\"PatID\").apply(lambda x: x / x.abs().sum()).drop(columns=[\"PatID\"])\n",
    "    df = df.drop(columns=[\"PatID\"])\n",
    "    \n",
    "    ranking_abs = create_ranking(df, rank_type=\"abs\")\n",
    "    ranking_pos = create_ranking(df, rank_type=\"pos\")\n",
    "    ranking_neg = create_ranking(df, rank_type=\"neg\")\n",
    "\n",
    "    top_vals_abs = take_top_vals(ranking_abs)\n",
    "    ranking_pos = ranking_pos.loc[top_vals_abs.index]\n",
    "    ranking_neg = ranking_neg.loc[top_vals_abs.index]\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(1, 3), dpi=200)\n",
    "        ax = plt.gca()\n",
    "    plot_ranking(ranking_pos, ax, rank_type=\"pos\")\n",
    "    plot_ranking(ranking_neg, ax, rank_type=\"neg\")\n",
    "\n",
    "        \n",
    "    ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "hdbscan = HDBSCAN(min_cluster_size=2000, min_samples=500)\n",
    "hdbscan_labels = hdbscan.fit_predict(high_d_embeddings)\n",
    "print(hdbscan.labels_.max(), (hdbscan.labels_ == -1).mean())\n",
    "umap.plot.points(reducer, labels=hdbscan_labels, theme=\"fire\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "cluster_df = umap_df.copy()\n",
    "cluster_df[\"cluster\"] = hdbscan_labels\n",
    "cluster_df = cluster_df[cluster_df[\"cluster\"] != -1]\n",
    "#cluster_ranking = cluster_df.groupby(\"cluster\").apply(lambda x : create_ranking(x))\n",
    "#plt.close()\n",
    "\n",
    "#plt.figure(figsize=(5, 3), dpi=200)\n",
    "num_clusters = cluster_df[\"cluster\"].max() + 1\n",
    "fig, axs = plt.subplots(1, num_clusters, squeeze=True, figsize=(2 * num_clusters, 4), dpi=25 * num_clusters)\n",
    "axs = axs.squeeze()\n",
    "for i in range(num_clusters):\n",
    "    ax = axs[i]\n",
    "    current_cluster_df = cluster_df[cluster_df[\"cluster\"] == i].drop(columns=[\"cluster\"])\n",
    "    #current = cluster_ranking.iloc[i].drop(columns=\"cluster\")  #cluster_df[cluster_df[\"cluster\"] == i]\n",
    "    title = f\"{i}: Size={len(current_cluster_df)} \"\n",
    "    ax = visualize_ranking(current_cluster_df, title, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "hdbscan = HDBSCAN(min_cluster_size=2000, min_samples=500)\n",
    "hdbscan_labels = hdbscan.fit_predict(high_d_embeddings)\n",
    "print(hdbscan.labels_.max(), (hdbscan.labels_ == -1).mean())\n",
    "umap.plot.points(reducer, labels=hdbscan_labels, theme=\"fire\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "cluster_df = umap_df.copy()\n",
    "cluster_df[\"cluster\"] = hdbscan_labels\n",
    "cluster_df = cluster_df[cluster_df[\"cluster\"] != -1]\n",
    "#cluster_ranking = cluster_df.groupby(\"cluster\").apply(lambda x : create_ranking(x))\n",
    "#plt.close()\n",
    "\n",
    "#plt.figure(figsize=(5, 3), dpi=200)\n",
    "num_clusters = cluster_df[\"cluster\"].max() + 1\n",
    "fig, axs = plt.subplots(1, num_clusters, squeeze=True, figsize=(2 * num_clusters, 4), dpi=25 * num_clusters)\n",
    "axs = axs.squeeze()\n",
    "for i in range(num_clusters):\n",
    "    ax = axs[i]\n",
    "    current_cluster_df = cluster_df[cluster_df[\"cluster\"] == i].drop(columns=[\"cluster\"])\n",
    "    #current = cluster_ranking.iloc[i].drop(columns=\"cluster\")  #cluster_df[cluster_df[\"cluster\"] == i]\n",
    "    title = f\"{i}: Size={len(current_cluster_df)} \"\n",
    "    ax = visualize_ranking(current_cluster_df, title, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = reducer.fit_transform(df[top_ten_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(reducer, theme='fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(reducer, theme='fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
