{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import utils\n",
    "import saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id, name=\"\", subdir=\"1\"):\n",
    "    # read model\n",
    "    exp_dict = utils.load_experiment_data(mlrun_id=model_id, default_id=subdir)\n",
    "    feature_names = exp_dict[\"feature_names\"]\n",
    "    target_names = exp_dict[\"target_names\"]\n",
    "    data = exp_dict[\"data\"]\n",
    "    target = exp_dict[\"target\"]\n",
    "    pred = exp_dict[\"pred\"]\n",
    "    args = exp_dict[\"args\"]\n",
    "    model = exp_dict[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labor_id = 'b71e463e5252431b915954bb91d3d2e4'\n",
    "\n",
    "phase_id = 'e075eaaf10fe48f2aa3d738cc4495fd7'\n",
    "icp_id = 'f0d90ecf970f467fa23204e6f4490bd3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "exp_dict = utils.load_experiment_data(mlrun_id=labor_id, default_id=\"1\")\n",
    "feature_names = exp_dict[\"feature_names\"]\n",
    "model = exp_dict[\"model\"]\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "phase = \"long\"\n",
    "phase_idx = 1 if phase == \"long\" else 0\n",
    "\n",
    "sal_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal = saliency.get_sal_list(model, phase_idx, 1.0, False, ds=None, ig=False, **sal_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_train = saliency.calc_median(model.train_dataloader().dataset)\n",
    "median_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(torch.tensor([0.2, 0.6, 0.15, 0.05]).cumsum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_top_cumsum_idcs(sals, frac, reverse=False):\n",
    "    # returns the sorted saliency values and indices of them that explain in sum at least the given frac of importance\n",
    "    sal_series = pd.Series(sals).sort_values(ascending=True)\n",
    "    sal_series /= sal_series.abs().sum()\n",
    "    if reverse:\n",
    "        mask = sal_series.cumsum() <= frac\n",
    "    else:\n",
    "        mask = sal_series.cumsum() > 1 - frac\n",
    "    chosen_vals = sal_series[mask]\n",
    "    chosen_vals = chosen_vals.sort_values(ascending=False)\n",
    "    sort_vals = chosen_vals.to_numpy()\n",
    "    sort_idcs = chosen_vals.index.to_numpy()\n",
    "    return sort_vals, sort_idcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [0.2, 0.6, 0.15, 0.05]\n",
    "print(sorted(array, reverse=True))\n",
    "print(get_top_cumsum_idcs(array, 0.61))\n",
    "print(get_top_cumsum_idcs(array, 1.0, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "def perturb_batch(batch_data, batch_sal, lens, median, noise_std, top_n, perturb_frac, reverse, average_over_patient=False):\n",
    "    if not top_n and not perturb_frac:\n",
    "        return  batch_data, []\n",
    "    removed_feat_numbers = []\n",
    "    for pat_data, pat_sal, pat_len in zip(batch_data, batch_sal, lens):\n",
    "        # cut to proper len\n",
    "        pat_data = pat_data[:pat_len]\n",
    "        pat_sal = pat_sal[:pat_len]\n",
    "        # determine which to perturb for each patient in batch\n",
    "        # calc mean ranking for patient if averaging\n",
    "        if average_over_patient:\n",
    "            pat_sal = torch.tensor(pat_sal).abs().mean(dim=0)\n",
    "            pat_sal = pat_sal / pat_sal.sum()\n",
    "        \n",
    "        # calc noise idcs\n",
    "        flat_sal = torch.tensor(pat_sal).flatten().abs()\n",
    "        noise_idcs = None\n",
    "        if top_n:\n",
    "            # select top N features\n",
    "            used_top_n = min(top_n, len(flat_sal))\n",
    "            top_k = torch.topk(flat_sal, used_top_n, largest=not reverse)\n",
    "            sal_vals = top_k.values\n",
    "            noise_idcs = top_k.indices\n",
    "        elif perturb_frac:\n",
    "            # select features such that a certain percentage of importance is explained\n",
    "            sal_vals, noise_idcs = get_top_cumsum_idcs(flat_sal, perturb_frac, reverse=reverse)\n",
    "            removed_feat_numbers.append(len(noise_idcs))\n",
    "        # map averaged idcs to whole timeseries\n",
    "        num_steps = pat_sal.shape[0]\n",
    "        num_feats = pat_sal.shape[1]\n",
    "        if average_over_patient:\n",
    "            # init tensor with correct number of elements\n",
    "            new_noise_idcs = torch.ones(num_steps, len(noise_idcs))\n",
    "            # multiply by time component\n",
    "            new_noise_idcs *= torch.arange(num_steps).unsqueeze(1)\n",
    "            # add feature component\n",
    "            new_noise_idcs += noise_idcs\n",
    "            # replace\n",
    "            noise_idcs = new_noise_idcs\n",
    "        # map flat noise idcs to separate time and feat idcs\n",
    "        time_idcs = noise_idcs // num_feats\n",
    "        feat_idcs = noise_idcs % num_feats\n",
    "\n",
    "        # Input perturbation\n",
    "        # potentially set some feature to median\n",
    "        if median is not None:\n",
    "            if median.shape[0] < num_steps:\n",
    "                median = median.repeat(num_steps, 1)\n",
    "            #print(pat_data.shape, median.shape)\n",
    "            #print(time_idcs)\n",
    "            #print(feat_idcs)\n",
    "            #print()\n",
    "            pat_data[time_idcs, feat_idcs] = median[time_idcs, feat_idcs]\n",
    "        # potentially add noise\n",
    "        if noise_idcs is not None and noise_std != 0.0:\n",
    "            noise_tensor = torch.zeros(len(time_idcs)).normal_(std=noise_std)\n",
    "            #print(noise_tensor.shape)\n",
    "            #print(pat_data.shape)\n",
    "            #print(pat_data[time_idcs, feat_idcs].shape)\n",
    "            #print(time_idcs)\n",
    "            #print(feat_idcs)\n",
    "            #print()\n",
    "            pat_data[time_idcs, feat_idcs] += noise_tensor\n",
    "    return batch_data, removed_feat_numbers\n",
    "\n",
    "\n",
    "def eval_model(model, sal, ds=None, dl=None, batch_size=512, median=None, noise_std=0.0, top_n=0, perturb_frac=0.0, reverse=False):\n",
    "    # get dataloader\n",
    "    if ds is None:\n",
    "        ds = model.val_dataloader().dataset\n",
    "    if dl is None:\n",
    "        dl = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n",
    "    \n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    #top_n_num = int(ds[0][0].shape[-1] * perturb_frac)\n",
    "    #if top_n:\n",
    "    #    print(top_n_num)\n",
    "    total_removed_feats = []\n",
    "    batch_size = dl.batch_size\n",
    "    for sal_idx, (pat_data, pat_target, idx, lens) in enumerate(dl):    \n",
    "        # perturb input\n",
    "        batch_sal = sal[sal_idx * batch_size: sal_idx * batch_size + batch_size]\n",
    "        pat_data, removed_feat_numbers = perturb_batch(pat_data, batch_sal, lens, median, noise_std, top_n, perturb_frac, reverse)\n",
    "        total_removed_feats.extend(removed_feat_numbers)\n",
    "        # pred\n",
    "        pat_data = pat_data.cuda(non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            pat_pred = model(pat_data)\n",
    "        # select phase\n",
    "        pat_target = pat_target[:, :, phase_idx].cpu()\n",
    "        pat_pred = pat_pred[:, :, phase_idx].cpu()\n",
    "        # cut lens\n",
    "        for idx, len_ in enumerate(lens):\n",
    "            cut_pred = pat_pred[idx, :len_].flatten()\n",
    "            cut_target = pat_target[idx, :len_].flatten()\n",
    "            val_preds.append(cut_pred)\n",
    "            val_targets.append(cut_target)\n",
    "    if perturb_frac:\n",
    "        print(\"Removed on average: \", np.array(total_removed_feats).mean())\n",
    "    # flatten list to array\n",
    "    val_preds = np.array([pred for pat in val_preds for pred in pat])\n",
    "    val_targets = np.array([target for pat in val_targets for target in pat])\n",
    "    # remove nans\n",
    "    nan_mask = np.isnan(val_targets)\n",
    "    val_targets = val_targets[~nan_mask]\n",
    "    val_preds = val_preds[~nan_mask]\n",
    "    # calc score\n",
    "    val_preds = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
    "    score = average_precision_score(val_targets, val_preds)\n",
    "    return score, val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def calc_progression(model, sal, steps, use_top_n, eval_args, reverse=False, ds=None, dl=None, batch_size=128):\n",
    "    if ds is None:\n",
    "        ds = model.val_dataloader().dataset\n",
    "    if dl is None:\n",
    "        dl = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n",
    "    # calc baseline first\n",
    "    base_score, base_preds = eval_model(model, sal, dl=dl, median=None, noise_std=0.0, top_n=0, perturb_frac=0.0)\n",
    "    # get perturb range\n",
    "    if use_top_n:\n",
    "        perturb_range = list(range(1, steps + 1))\n",
    "    else:\n",
    "        step = 1 / steps\n",
    "        perturb_range = list(np.linspace(step, 1, steps))\n",
    "    # calc progression\n",
    "    perturb_params = [0.0] + perturb_range\n",
    "    score_dict = {\"logit\": [0.0],\n",
    "                  \"abs_logit\": [0.0],\n",
    "                  \"ap_score\": [0.0],\n",
    "                  \"abs_ap_score\": [0.0]}\n",
    "    for val in tqdm(perturb_range):\n",
    "        if use_top_n:\n",
    "            eval_args[\"top_n\"] = val\n",
    "        else:\n",
    "            eval_args[\"perturb_frac\"] = val\n",
    "        # calc perturbation\n",
    "        ap_score, val_preds = eval_model(model, sal, dl=dl, reverse=reverse, **eval_args)\n",
    "        # calc and store scores\n",
    "        score_dict[\"abs_logit\"].append(np.abs(val_preds - base_preds).mean())\n",
    "        score_dict[\"logit\"].append((val_preds - base_preds).mean())\n",
    "        score_dict[\"ap_score\"].append(ap_score - base_score)\n",
    "        score_dict[\"abs_ap_score\"].append(abs(ap_score - base_score))\n",
    "    score_dict = {key: np.array(score_dict[key]) for key in score_dict}\n",
    "    return np.array(perturb_params), score_dict\n",
    "\n",
    "\n",
    "def calc_MoRF(model, sal, steps, use_top_n, eval_args):\n",
    "    return calc_progression(model, sal, steps, use_top_n, eval_args, reverse=False)\n",
    "\n",
    "\n",
    "def calc_LeRF(model, sal, steps, use_top_n, eval_args):\n",
    "    return calc_progression(model, sal, steps, use_top_n, eval_args, reverse=True)\n",
    "\n",
    "\n",
    "def calc_ABPC(model, sal, steps, use_top_n, eval_args):\n",
    "    params, morf_dict = calc_MoRF(model, sal, steps, use_top_n, eval_args)\n",
    "    params, lerf_dict = calc_LeRF(model, sal, steps, use_top_n, eval_args)\n",
    "    \n",
    "    abpc_dict = {key: lerf_dict[key] - morf_dict[key] for key in morf_dict}\n",
    "    \n",
    "    return params, morf_dict, lerf_dict, abpc_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = {\"median\": median_train,\n",
    "             \"noise_std\": 0.0}\n",
    "steps = 5\n",
    "use_top_n = True\n",
    "\n",
    "params, morf_dict, lerf_dict, abpc_dict = calc_ABPC(model, sal\n",
    "                                                    , steps, use_top_n, eval_args)\n",
    "for key in abpc_dict:\n",
    "    print(key, np.round(abpc_dict[key], 3), round(np.mean(abpc_dict[key]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = {\"median\": None,\n",
    "             \"noise_std\": 1.0}\n",
    "steps = 5\n",
    "use_top_n = True\n",
    "\n",
    "params, morf_dict, lerf_dict, abpc_dict = calc_ABPC(model, sal, steps, use_top_n, eval_args)\n",
    "for key in abpc_dict:\n",
    "    print(key, np.round(abpc_dict[key], 3), round(np.mean(abpc_dict[key]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(1, 1).normal_(std=1.0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = {\"median\": None,\n",
    "             \"noise_std\": 3.0}\n",
    "steps = 5\n",
    "use_top_n = True\n",
    "\n",
    "params, morf_dict, lerf_dict, abpc_dict = calc_ABPC(model, sal, steps, use_top_n, eval_args)\n",
    "for key in abpc_dict:\n",
    "    print(key, np.round(abpc_dict[key], 3), round(np.mean(abpc_dict[key]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = {\"median\": median_train,\n",
    "             \"noise_std\": 0.0}\n",
    "steps = 5\n",
    "use_top_n = False\n",
    "\n",
    "params, morf_dict, lerf_dict, abpc_dict = calc_ABPC(model, sal, steps, use_top_n, eval_args)\n",
    "for key in abpc_dict:\n",
    "    print(key, np.round(abpc_dict[key], 3), round(np.mean(abpc_dict[key]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = {\"median\": None,\n",
    "             \"noise_std\": 3.0}\n",
    "steps = 5\n",
    "use_top_n = False\n",
    "\n",
    "params, morf_dict, lerf_dict, abpc_dict = calc_ABPC(model, sal, steps, use_top_n, eval_args)\n",
    "for key in abpc_dict:\n",
    "    print(key, np.round(abpc_dict[key], 3), round(np.mean(abpc_dict[key]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = {\"median\": median_train,\n",
    "             \"noise_std\": 0.0}\n",
    "steps = 5\n",
    "use_top_n = True\n",
    "\n",
    "print(\"MoRF\")\n",
    "params, score_dict = calc_MoRF(model, sal, steps, use_top_n, eval_args)\n",
    "for key in score_dict:\n",
    "    print(key, np.round(score_dict[key], 3))\n",
    "print(\"LeRF\")\n",
    "params, score_dict = calc_LeRF(model, sal, steps, use_top_n, eval_args)\n",
    "for key in score_dict:\n",
    "    print(key, np.round(score_dict[key], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = {\"median\": median_train,\n",
    "             \"noise_std\": 0.0}\n",
    "steps = 5\n",
    "use_top_n = False\n",
    "\n",
    "print(\"MoRF\")\n",
    "params, score_dict = calc_MoRF(model, sal, steps, use_top_n, eval_args)\n",
    "for key in score_dict:\n",
    "    print(key, np.round(score_dict[key], 3))\n",
    "print(\"LeRF\")\n",
    "params, score_dict = calc_LeRF(model, sal, steps, use_top_n, eval_args)\n",
    "for key in score_dict:\n",
    "    print(key, np.round(score_dict[key], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL by adding noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std = 1.0\n",
    "top_n = 0\n",
    "score_baseline, preds_baseline = eval_model(model, sal, noise_std=noise_std, batch_size=128, median=None, top_n=top_n, perturb_frac=0.0)\n",
    "eval_score, eval_preds = eval_model(model, sal, noise_std=noise_std, batch_size=128, median=None, top_n=top_n, perturb_frac=0.02)\n",
    "fully_random, fully_random_preds = eval_model(model, sal, noise_std=noise_std, batch_size=128, median=None, top_n=top_n, perturb_frac=1.0)\n",
    "print(score_baseline)\n",
    "print(eval_score)#, abs(eval_preds - preds_baseline).mean())\n",
    "print(fully_random)#, abs(fully_random_preds - preds_baseline).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "p = plt.hist(preds_baseline, bins=1000)\n",
    "p = plt.hist(eval_preds, bins=1000, color=\"red\", alpha=0.8)\n",
    "p = plt.hist(fully_random_preds, bins=1000, color=\"orange\", alpha=0.5)\n",
    "print(preds_baseline.mean())\n",
    "print(eval_preds.mean())\n",
    "print(fully_random_preds.mean())\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "#p = plt.hist(preds_baseline, bins=1000)\n",
    "p = plt.hist(eval_preds - preds_baseline, bins=1000, color=\"red\")\n",
    "p = plt.hist(fully_random_preds - preds_baseline, bins=1000, color=\"orange\")\n",
    "print(np.abs(eval_preds - preds_baseline).mean())\n",
    "print(np.abs(fully_random_preds - preds_baseline).mean())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL BY MEDIAN\n",
    "score_baseline, preds_baseline = eval_model(model, sal, batch_size=128, median=median_train, top_n=0, perturb_frac=0.0)\n",
    "eval_score, eval_preds = eval_model(model, sal, batch_size=128, median=median_train, top_n=0, perturb_frac=0.5)\n",
    "fully_random, fully_random_preds = eval_model(model, sal, batch_size=128, median=median_train, top_n=0, perturb_frac=1.0)\n",
    "print(score_baseline)\n",
    "print(eval_score)#, abs(eval_preds - preds_baseline).mean())\n",
    "print(fully_random)#, abs(fully_random_preds - preds_baseline).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "p = plt.hist(preds_baseline, bins=1000)\n",
    "p = plt.hist(eval_preds, bins=1000, color=\"red\", alpha=0.8)\n",
    "p = plt.hist(fully_random_preds, bins=1000, color=\"orange\", alpha=0.5)\n",
    "print(preds_baseline.mean())\n",
    "print(eval_preds.mean())\n",
    "print(fully_random_preds.mean())\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "#p = plt.hist(preds_baseline, bins=1000)\n",
    "p = plt.hist(eval_preds - preds_baseline, bins=1000, color=\"red\")\n",
    "p = plt.hist(fully_random_preds - preds_baseline, bins=1000, color=\"orange\")\n",
    "print(np.abs(eval_preds - preds_baseline).mean())\n",
    "print(np.abs(fully_random_preds - preds_baseline).mean())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same sal for all pats\n",
    "uniform_sal = torch.stack([np.abs(torch.tensor(pat_sal)).mean(dim=0) for pat_sal in sal])\n",
    "uniform_sal /= uniform_sal.sum(dim=1, keepdim=True)\n",
    "uniform_sal = uniform_sal.mean(dim=0)\n",
    "uniform_sal /= uniform_sal.sum(dim=0, keepdim=True)\n",
    "uniform_sal = uniform_sal.numpy()\n",
    "# bring into original shape\n",
    "uniform_sal_list = [np.stack([uniform_sal] * len(pat_sal)) for i, pat_sal in enumerate(sal)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL BY MEDIAN\n",
    "score_baseline, preds_baseline = eval_model(model, uniform_sal_list, batch_size=128, median=median_train, top_n=True, perturb_frac=0.0)\n",
    "eval_score, eval_preds = eval_model(model, uniform_sal_list, batch_size=128, median=median_train, top_n=True, perturb_frac=0.02)\n",
    "fully_random, fully_random_preds = eval_model(model, uniform_sal_list, batch_size=128, median=median_train, top_n=True, perturb_frac=1.0)\n",
    "print(score_baseline)\n",
    "print(eval_score)#, abs(eval_preds - preds_baseline).mean())\n",
    "print(fully_random)#, abs(fully_random_preds - preds_baseline).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "p = plt.hist(preds_baseline, bins=1000)\n",
    "p = plt.hist(eval_preds, bins=1000, color=\"red\", alpha=0.8)\n",
    "p = plt.hist(fully_random_preds, bins=1000, color=\"orange\", alpha=0.5)\n",
    "print(preds_baseline.mean())\n",
    "print(eval_preds.mean())\n",
    "print(fully_random_preds.mean())\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "#p = plt.hist(preds_baseline, bins=1000)\n",
    "p = plt.hist(eval_preds - preds_baseline, bins=1000, color=\"red\")\n",
    "p = plt.hist(fully_random_preds - preds_baseline, bins=1000, color=\"orange\")\n",
    "print(np.abs(eval_preds - preds_baseline).mean())\n",
    "print(np.abs(fully_random_preds - preds_baseline).mean())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL BY noise\n",
    "noise_std = 1.0\n",
    "score_baseline, preds_baseline = eval_model(model, uniform_sal_list, noise_std=noise_std, batch_size=128, median=None, top_n=True, perturb_frac=0.0)\n",
    "eval_score, eval_preds = eval_model(model, uniform_sal_list, noise_std=noise_std, batch_size=128, median=None, top_n=True, perturb_frac=0.02)\n",
    "fully_random, fully_random_preds = eval_model(model, uniform_sal_list, noise_std=noise_std, batch_size=128, median=None, top_n=True, perturb_frac=1.0)\n",
    "print(score_baseline)\n",
    "print(eval_score)#, abs(eval_preds - preds_baseline).mean())\n",
    "print(fully_random)#, abs(fully_random_preds - preds_baseline).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "p = plt.hist(preds_baseline, bins=1000)\n",
    "p = plt.hist(eval_preds, bins=1000, color=\"red\", alpha=0.8)\n",
    "p = plt.hist(fully_random_preds, bins=1000, color=\"orange\", alpha=0.5)\n",
    "print(preds_baseline.mean())\n",
    "print(eval_preds.mean())\n",
    "print(fully_random_preds.mean())\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "#p = plt.hist(preds_baseline, bins=1000)\n",
    "p = plt.hist(eval_preds - preds_baseline, bins=1000, color=\"red\")\n",
    "p = plt.hist(fully_random_preds - preds_baseline, bins=1000, color=\"orange\")\n",
    "print(np.abs(eval_preds - preds_baseline).mean())\n",
    "print(np.abs(fully_random_preds - preds_baseline).mean())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
